"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[455],{2146:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module4-vla/ch1-voice-to-action","title":"Voice-to-Action: Speech Integration for Humanoid Control","description":"Learn to integrate speech recognition using Whisper, build natural language command parsers, and map voice commands to ROS 2 actions for intuitive humanoid robot control.","source":"@site/docs/module4-vla/ch1-voice-to-action.md","sourceDirName":"module4-vla","slug":"/module4-vla/ch1-voice-to-action","permalink":"/physical-ai-textbook/docs/module4-vla/ch1-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/salmansiddiqui-99/physical-ai-textbook/tree/main/docs/module4-vla/ch1-voice-to-action.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"id":"ch1-voice-to-action","title":"Voice-to-Action: Speech Integration for Humanoid Control","sidebar_label":"Voice-to-Action","sidebar_position":10,"description":"Learn to integrate speech recognition using Whisper, build natural language command parsers, and map voice commands to ROS 2 actions for intuitive humanoid robot control.","keywords":["Whisper","speech recognition","voice control","natural language processing","intent recognition","ROS 2 actions","voice commands","STT"],"prerequisites":["Completion of Modules 1-3 (ROS 2, Simulation, Isaac Platform)","Python 3.10+ with transformers library","Basic understanding of natural language processing concepts"],"learning_objectives":["Integrate OpenAI Whisper for real-time speech-to-text transcription","Implement natural language command parsing and intent recognition","Create intent-to-action mappers that trigger ROS 2 action servers","Build complete voice-controlled humanoid robot systems"],"estimated_time":"90 minutes"},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Navigation","permalink":"/physical-ai-textbook/docs/module3-isaac/ch3-navigation-humanoids"},"next":{"title":"Cognitive Planning","permalink":"/physical-ai-textbook/docs/module4-vla/ch2-cognitive-planning"}}');var o=t(4848),r=t(8453);const s={id:"ch1-voice-to-action",title:"Voice-to-Action: Speech Integration for Humanoid Control",sidebar_label:"Voice-to-Action",sidebar_position:10,description:"Learn to integrate speech recognition using Whisper, build natural language command parsers, and map voice commands to ROS 2 actions for intuitive humanoid robot control.",keywords:["Whisper","speech recognition","voice control","natural language processing","intent recognition","ROS 2 actions","voice commands","STT"],prerequisites:["Completion of Modules 1-3 (ROS 2, Simulation, Isaac Platform)","Python 3.10+ with transformers library","Basic understanding of natural language processing concepts"],learning_objectives:["Integrate OpenAI Whisper for real-time speech-to-text transcription","Implement natural language command parsing and intent recognition","Create intent-to-action mappers that trigger ROS 2 action servers","Build complete voice-controlled humanoid robot systems"],estimated_time:"90 minutes"},a="Voice-to-Action: Speech Integration for Humanoid Control",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Section 1: Speech Recognition with Whisper",id:"section-1-speech-recognition-with-whisper",level:2},{value:"Subsection 1.1: Why Whisper for Robotics?",id:"subsection-11-why-whisper-for-robotics",level:3},{value:"Subsection 1.2: Installing Whisper",id:"subsection-12-installing-whisper",level:3},{value:"Subsection 1.3: Basic Whisper Usage",id:"subsection-13-basic-whisper-usage",level:3},{value:"Subsection 1.4: Real-Time Microphone Transcription",id:"subsection-14-real-time-microphone-transcription",level:3},{value:"Section 2: Natural Language Command Parsing",id:"section-2-natural-language-command-parsing",level:2},{value:"Subsection 2.1: Intent Recognition Basics",id:"subsection-21-intent-recognition-basics",level:3},{value:"Subsection 2.2: Rule-Based Command Parser",id:"subsection-22-rule-based-command-parser",level:3},{value:"Subsection 2.3: LLM-Based Intent Extraction (Advanced)",id:"subsection-23-llm-based-intent-extraction-advanced",level:3},{value:"Section 3: Intent-to-Action Mapping",id:"section-3-intent-to-action-mapping",level:2},{value:"Subsection 3.1: ROS 2 Action Integration",id:"subsection-31-ros-2-action-integration",level:3},{value:"Section 4: Complete Voice Control System",id:"section-4-complete-voice-control-system",level:2},{value:"Subsection 4.1: Integrated Voice Control Node",id:"subsection-41-integrated-voice-control-node",level:3},{value:"Hands-On Project: Voice-Controlled Navigation",id:"hands-on-project-voice-controlled-navigation",level:2},{value:"Step 1: Install Dependencies",id:"step-1-install-dependencies",level:3},{value:"Step 2: Test Whisper",id:"step-2-test-whisper",level:3},{value:"Step 3: Implement Voice Node",id:"step-3-implement-voice-node",level:3},{value:"Step 4: Define Locations",id:"step-4-define-locations",level:3},{value:"Step 5: Test Voice Commands",id:"step-5-test-voice-commands",level:3},{value:"Challenge: Test Your Understanding",id:"challenge-test-your-understanding",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"voice-to-action-speech-integration-for-humanoid-control",children:"Voice-to-Action: Speech Integration for Humanoid Control"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Integrate OpenAI Whisper for accurate real-time speech recognition"}),"\n",(0,o.jsx)(e.li,{children:"Parse natural language commands and extract actionable intents"}),"\n",(0,o.jsx)(e.li,{children:"Map voice commands to ROS 2 action calls for humanoid control"}),"\n",(0,o.jsx)(e.li,{children:"Build complete voice-controlled navigation and manipulation pipelines"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(e.p,{children:"Before starting this chapter, you should:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["Have completed ",(0,o.jsx)(e.a,{href:"/physical-ai-textbook/docs/module3-isaac/ch1-isaac-sim",children:"Module 3: Isaac Platform"})]}),"\n",(0,o.jsxs)(e.li,{children:["Understand ROS 2 actions and services from ",(0,o.jsx)(e.a,{href:"/physical-ai-textbook/docs/module1-ros2/ch2-ros2-humanoids",children:"Module 1"})]}),"\n",(0,o.jsx)(e.li,{children:"Have Python 3.10+ installed with pip"}),"\n",(0,o.jsx)(e.li,{children:"Have basic familiarity with speech recognition concepts"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:'Voice control represents the most natural human-robot interface. Instead of joysticks, keyboards, or mobile apps, users simply speak: "Go to the kitchen", "Pick up the red mug", "Follow me". For humanoid robots operating in homes and offices, voice commands enable intuitive interaction with non-expert users\u2014elderly care recipients, office workers, or children.'}),"\n",(0,o.jsxs)(e.p,{children:["Modern speech recognition has reached human-level accuracy thanks to deep learning models like ",(0,o.jsx)(e.strong,{children:"OpenAI Whisper"}),". Whisper achieves 95%+ word accuracy across 99 languages, handles noisy environments, and runs efficiently on edge devices like NVIDIA Jetson. Combined with large language models (LLMs) for intent parsing, we can build robust voice control systems that understand context, handle ambiguity, and gracefully recover from errors."]}),"\n",(0,o.jsxs)(e.p,{children:['This chapter covers the complete voice-to-action pipeline: capturing audio, transcribing speech with Whisper, parsing commands to extract intents ("navigate to kitchen" \u2192 ',(0,o.jsx)(e.code,{children:"NavigateToPose"}),' action), and executing actions via ROS 2. You\'ll build a system where saying "Turn around" triggers a 180\xb0 rotation, and "Bring me a bottle" initiates grasping and navigation.']}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"section-1-speech-recognition-with-whisper",children:"Section 1: Speech Recognition with Whisper"}),"\n",(0,o.jsx)(e.h3,{id:"subsection-11-why-whisper-for-robotics",children:"Subsection 1.1: Why Whisper for Robotics?"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"OpenAI Whisper"})," is a transformer-based automatic speech recognition (ASR) model trained on 680,000 hours of multilingual data."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Key Advantages for Humanoid Robots"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness"}),": Handles background noise, accents, and mispronunciations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multilingual"}),": Supports 99 languages (critical for global deployment)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Low Latency"}),": Real-time transcription on RTX GPUs or Jetson Xavier"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Open Source"}),": BSD-3 license, runs offline (privacy for home robots)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"No Fine-Tuning"}),": Works out-of-box for robotics commands"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Model Sizes"}),":"]}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Model"}),(0,o.jsx)(e.th,{children:"Parameters"}),(0,o.jsx)(e.th,{children:"Memory"}),(0,o.jsx)(e.th,{children:"Speed (RTX 3060)"}),(0,o.jsx)(e.th,{children:"Use Case"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Tiny"}),(0,o.jsx)(e.td,{children:"39M"}),(0,o.jsx)(e.td,{children:"1GB"}),(0,o.jsx)(e.td,{children:"32x realtime"}),(0,o.jsx)(e.td,{children:"Edge devices (Jetson Nano)"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Base"}),(0,o.jsx)(e.td,{children:"74M"}),(0,o.jsx)(e.td,{children:"1GB"}),(0,o.jsx)(e.td,{children:"16x realtime"}),(0,o.jsx)(e.td,{children:"Jetson Xavier/Orin"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Small"}),(0,o.jsx)(e.td,{children:"244M"}),(0,o.jsx)(e.td,{children:"2GB"}),(0,o.jsx)(e.td,{children:"6x realtime"}),(0,o.jsx)(e.td,{children:"Desktop/laptop"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Medium"}),(0,o.jsx)(e.td,{children:"769M"}),(0,o.jsx)(e.td,{children:"5GB"}),(0,o.jsx)(e.td,{children:"2x realtime"}),(0,o.jsx)(e.td,{children:"High accuracy"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Large"}),(0,o.jsx)(e.td,{children:"1550M"}),(0,o.jsx)(e.td,{children:"10GB"}),(0,o.jsx)(e.td,{children:"1x realtime"}),(0,o.jsx)(e.td,{children:"Production (best accuracy)"})]})]})]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"For humanoid robots"}),": Use ",(0,o.jsx)(e.code,{children:"small"})," or ",(0,o.jsx)(e.code,{children:"base"})," for real-time performance."]}),"\n",(0,o.jsx)(e.h3,{id:"subsection-12-installing-whisper",children:"Subsection 1.2: Installing Whisper"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Install Whisper via pip\npip install openai-whisper\n\n# Install PyTorch with CUDA support (for GPU acceleration)\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install audio libraries\nsudo apt-get install ffmpeg portaudio19-dev\n\n# Install Python audio interface\npip install pyaudio sounddevice\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Verify Installation"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"python -c \"import whisper; print('Whisper version:', whisper.__version__)\"\n"})}),"\n",(0,o.jsx)(e.h3,{id:"subsection-13-basic-whisper-usage",children:"Subsection 1.3: Basic Whisper Usage"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example: Transcribe Audio File"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import whisper\n\n# Load model (downloads on first run, ~1GB for \'base\')\nmodel = whisper.load_model("base")\n\n# Transcribe audio file\nresult = model.transcribe("audio.mp3")\n\nprint("Transcription:", result["text"])\nprint("Language detected:", result["language"])\n\n# Access word-level timestamps\nfor segment in result["segments"]:\n    print(f"[{segment[\'start\']:.2f}s - {segment[\'end\']:.2f}s] {segment[\'text\']}")\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Output"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Transcription: Go to the kitchen and bring me a bottle of water.\nLanguage detected: en\n[0.00s - 1.50s] Go to the kitchen\n[1.50s - 3.20s] and bring me a bottle of water.\n"})}),"\n",(0,o.jsx)(e.h3,{id:"subsection-14-real-time-microphone-transcription",children:"Subsection 1.4: Real-Time Microphone Transcription"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example: Continuous Voice Recognition"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nReal-time Whisper speech recognition\n\nPurpose: Capture audio from microphone and transcribe in real-time\nEnvironment: Python 3.10+, Whisper, PyAudio\n"""\n\nimport whisper\nimport numpy as np\nimport sounddevice as sd\nimport queue\nimport threading\n\nclass WhisperMicrophone:\n    """Real-time speech recognition using Whisper"""\n\n    def __init__(self, model_size="base", language="en"):\n        # Load Whisper model\n        print(f"Loading Whisper model: {model_size}")\n        self.model = whisper.load_model(model_size)\n        self.language = language\n\n        # Audio parameters\n        self.sample_rate = 16000  # Whisper expects 16kHz\n        self.chunk_duration = 3.0  # Process 3-second chunks\n        self.chunk_samples = int(self.sample_rate * self.chunk_duration)\n\n        # Audio buffer\n        self.audio_queue = queue.Queue()\n        self.is_recording = False\n\n    def audio_callback(self, indata, frames, time, status):\n        """Called by sounddevice for each audio chunk"""\n        if status:\n            print(f"Audio status: {status}")\n        # Add audio to queue\n        self.audio_queue.put(indata.copy())\n\n    def start_recording(self):\n        """Start continuous microphone recording"""\n        self.is_recording = True\n\n        # Start audio stream\n        self.stream = sd.InputStream(\n            samplerate=self.sample_rate,\n            channels=1,  # Mono\n            dtype=\'float32\',\n            callback=self.audio_callback,\n            blocksize=int(self.sample_rate * 0.1)  # 100ms blocks\n        )\n        self.stream.start()\n        print("Recording started. Speak into microphone...")\n\n    def stop_recording(self):\n        """Stop microphone recording"""\n        self.is_recording = False\n        if hasattr(self, \'stream\'):\n            self.stream.stop()\n            self.stream.close()\n        print("Recording stopped.")\n\n    def transcribe_stream(self):\n        """Transcribe audio chunks as they arrive"""\n        audio_buffer = []\n\n        while self.is_recording:\n            try:\n                # Get audio chunk (non-blocking, 100ms timeout)\n                chunk = self.audio_queue.get(timeout=0.1)\n                audio_buffer.append(chunk)\n\n                # Accumulate enough audio (3 seconds)\n                if len(audio_buffer) >= self.chunk_samples / len(chunk):\n                    # Concatenate buffer\n                    audio_data = np.concatenate(audio_buffer, axis=0)\n                    audio_data = audio_data.flatten()\n\n                    # Transcribe with Whisper\n                    result = self.model.transcribe(\n                        audio_data,\n                        language=self.language,\n                        fp16=True  # Use FP16 for speed (requires GPU)\n                    )\n\n                    transcription = result["text"].strip()\n                    if transcription:\n                        yield transcription\n\n                    # Clear buffer (keep last 0.5s for overlap)\n                    overlap_samples = int(0.5 * self.sample_rate)\n                    audio_buffer = [audio_data[-overlap_samples:]]\n\n            except queue.Empty:\n                continue\n\n\n# Usage example\nif __name__ == \'__main__\':\n    mic = WhisperMicrophone(model_size="base")\n    mic.start_recording()\n\n    try:\n        for transcription in mic.transcribe_stream():\n            print(f"[Transcription] {transcription}")\n            # Process command here (next section)\n\n    except KeyboardInterrupt:\n        print("\\nStopping...")\n    finally:\n        mic.stop_recording()\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Running"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"python whisper_microphone.py\n\n# Output:\n# Loading Whisper model: base\n# Recording started. Speak into microphone...\n# [Transcription] Go to the kitchen\n# [Transcription] Turn around\n# [Transcription] Pick up the red mug\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"section-2-natural-language-command-parsing",children:"Section 2: Natural Language Command Parsing"}),"\n",(0,o.jsx)(e.h3,{id:"subsection-21-intent-recognition-basics",children:"Subsection 2.1: Intent Recognition Basics"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Intent"}),": The action the user wants the robot to perform."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example Mappings"}),":"]}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Voice Command"}),(0,o.jsx)(e.th,{children:"Intent"}),(0,o.jsx)(e.th,{children:"Parameters"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:'"Go to the kitchen"'}),(0,o.jsx)(e.td,{children:"NAVIGATE"}),(0,o.jsx)(e.td,{children:'location="kitchen"'})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:'"Turn left"'}),(0,o.jsx)(e.td,{children:"ROTATE"}),(0,o.jsx)(e.td,{children:'direction="left", angle=90'})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:'"Pick up the mug"'}),(0,o.jsx)(e.td,{children:"GRASP"}),(0,o.jsx)(e.td,{children:'object="mug"'})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:'"Follow me"'}),(0,o.jsx)(e.td,{children:"FOLLOW_PERSON"}),(0,o.jsx)(e.td,{children:'target="user"'})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:'"Stop"'}),(0,o.jsx)(e.td,{children:"STOP"}),(0,o.jsx)(e.td,{children:"None"})]})]})]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Approaches"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Rule-Based"}),": Keyword matching (fast, limited)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ML-Based"}),": Intent classification with BERT/RoBERTa (accurate, requires training)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"LLM-Based"}),": Few-shot prompting with GPT-4/Claude (flexible, requires API)"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"subsection-22-rule-based-command-parser",children:"Subsection 2.2: Rule-Based Command Parser"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example: Simple Keyword Matcher"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nRule-based command parser for humanoid robot\n\nPurpose: Extract intents from voice commands using keyword matching\nEnvironment: Python 3.10+\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\n\n\n@dataclass\nclass Intent:\n    \"\"\"Parsed intent from voice command\"\"\"\n    action: str  # NAVIGATE, ROTATE, GRASP, etc.\n    confidence: float  # 0.0-1.0\n    parameters: Dict[str, Any]\n    raw_command: str\n\n\nclass CommandParser:\n    \"\"\"Parse natural language commands into actionable intents\"\"\"\n\n    def __init__(self):\n        # Define command patterns\n        self.patterns = {\n            'NAVIGATE': [\n                (r'go to (?:the )?(\\w+)', lambda m: {'location': m.group(1)}),\n                (r'navigate to (?:the )?(\\w+)', lambda m: {'location': m.group(1)}),\n                (r'move to (?:the )?(\\w+)', lambda m: {'location': m.group(1)}),\n            ],\n            'ROTATE': [\n                (r'turn (left|right)', lambda m: {'direction': m.group(1), 'angle': 90}),\n                (r'rotate (\\d+) degrees', lambda m: {'angle': int(m.group(1))}),\n                (r'turn around', lambda m: {'angle': 180}),\n            ],\n            'GRASP': [\n                (r'pick up (?:the )?(\\w+)', lambda m: {'object': m.group(1)}),\n                (r'grab (?:the )?(\\w+)', lambda m: {'object': m.group(1)}),\n                (r'grasp (?:the )?(\\w+)', lambda m: {'object': m.group(1)}),\n            ],\n            'FOLLOW': [\n                (r'follow me', lambda m: {'target': 'user'}),\n                (r'follow (?:the )?(\\w+)', lambda m: {'target': m.group(1)}),\n            ],\n            'STOP': [\n                (r'stop', lambda m: {}),\n                (r'halt', lambda m: {}),\n                (r'freeze', lambda m: {}),\n            ],\n        }\n\n    def parse(self, command: str) -> Optional[Intent]:\n        \"\"\"\n        Parse voice command into intent\n\n        Args:\n            command: Transcribed voice command (lowercase)\n\n        Returns:\n            Intent object or None if no match\n        \"\"\"\n        command = command.lower().strip()\n\n        # Try each intent pattern\n        for action, patterns in self.patterns.items():\n            for pattern, extractor in patterns:\n                match = re.search(pattern, command)\n                if match:\n                    parameters = extractor(match)\n                    return Intent(\n                        action=action,\n                        confidence=1.0,  # Rule-based is deterministic\n                        parameters=parameters,\n                        raw_command=command\n                    )\n\n        # No match found\n        return None\n\n\n# Usage example\nif __name__ == '__main__':\n    parser = CommandParser()\n\n    commands = [\n        \"Go to the kitchen\",\n        \"Turn left\",\n        \"Pick up the red mug\",\n        \"Turn around\",\n        \"Stop\",\n        \"Make me a sandwich\"  # No match\n    ]\n\n    for cmd in commands:\n        intent = parser.parse(cmd)\n        if intent:\n            print(f\"Command: '{cmd}'\")\n            print(f\"  Intent: {intent.action}\")\n            print(f\"  Parameters: {intent.parameters}\")\n        else:\n            print(f\"Command: '{cmd}' - NO MATCH\")\n        print()\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Output"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Command: 'Go to the kitchen'\n  Intent: NAVIGATE\n  Parameters: {'location': 'kitchen'}\n\nCommand: 'Turn left'\n  Intent: ROTATE\n  Parameters: {'direction': 'left', 'angle': 90}\n\nCommand: 'Pick up the red mug'\n  Intent: GRASP\n  Parameters: {'object': 'red'}\n\nCommand: 'Make me a sandwich' - NO MATCH\n"})}),"\n",(0,o.jsx)(e.h3,{id:"subsection-23-llm-based-intent-extraction-advanced",children:"Subsection 2.3: LLM-Based Intent Extraction (Advanced)"}),"\n",(0,o.jsx)(e.p,{children:"For complex commands, use an LLM like GPT-4 or Claude."}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example: Using OpenAI API"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import openai\nimport json\n\ndef parse_with_llm(command: str) -> Intent:\n    """Parse command using GPT-4 with structured output"""\n\n    prompt = f"""\nYou are a parser for humanoid robot voice commands.\nExtract the intent and parameters from the following command.\n\nCommand: "{command}"\n\nRespond in JSON format:\n{{\n    "action": "NAVIGATE|ROTATE|GRASP|FOLLOW|STOP",\n    "parameters": {{}},\n    "confidence": 0.0-1.0\n}}\n\nExamples:\n- "Go to the kitchen" \u2192 {{"action": "NAVIGATE", "parameters": {{"location": "kitchen"}}, "confidence": 0.95}}\n- "Turn around" \u2192 {{"action": "ROTATE", "parameters": {{"angle": 180}}, "confidence": 1.0}}\n- "Pick up the red mug" \u2192 {{"action": "GRASP", "parameters": {{"object": "mug", "color": "red"}}, "confidence": 0.9}}\n"""\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": prompt}],\n        temperature=0.0  # Deterministic\n    )\n\n    result = json.loads(response.choices[0].message.content)\n\n    return Intent(\n        action=result[\'action\'],\n        confidence=result[\'confidence\'],\n        parameters=result[\'parameters\'],\n        raw_command=command\n    )\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"section-3-intent-to-action-mapping",children:"Section 3: Intent-to-Action Mapping"}),"\n",(0,o.jsx)(e.h3,{id:"subsection-31-ros-2-action-integration",children:"Subsection 3.1: ROS 2 Action Integration"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Map intents to ROS 2 action calls"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nIntent-to-Action mapper for humanoid robot\n\nPurpose: Execute ROS 2 actions based on parsed voice intents\nEnvironment: ROS 2 Humble, action servers from Module 1\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav2_msgs.action import NavigateToPose\nfrom control_msgs.action import GripperCommand\nimport math\n\n\nclass IntentActionMapper(Node):\n    \"\"\"Map voice intents to ROS 2 actions\"\"\"\n\n    def __init__(self):\n        super().__init__('intent_action_mapper')\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        # self.grasp_client = ActionClient(self, GripperCommand, 'gripper_action')\n\n        # Velocity publisher (for rotation)\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Location database (hardcoded for demo)\n        self.locations = {\n            'kitchen': {'x': 5.0, 'y': 2.0, 'yaw': 0.0},\n            'living room': {'x': -3.0, 'y': 1.0, 'yaw': 1.57},\n            'bedroom': {'x': 0.0, 'y': -4.0, 'yaw': 3.14},\n        }\n\n        self.get_logger().info('Intent-Action mapper ready')\n\n    def execute_intent(self, intent):\n        \"\"\"Execute action based on intent\"\"\"\n        self.get_logger().info(f\"Executing: {intent.action} with {intent.parameters}\")\n\n        if intent.action == 'NAVIGATE':\n            self.execute_navigate(intent.parameters)\n        elif intent.action == 'ROTATE':\n            self.execute_rotate(intent.parameters)\n        elif intent.action == 'GRASP':\n            self.execute_grasp(intent.parameters)\n        elif intent.action == 'STOP':\n            self.execute_stop()\n        else:\n            self.get_logger().warn(f\"Unknown action: {intent.action}\")\n\n    def execute_navigate(self, params):\n        \"\"\"Navigate to named location\"\"\"\n        location_name = params.get('location')\n\n        if location_name not in self.locations:\n            self.get_logger().error(f\"Unknown location: {location_name}\")\n            return\n\n        # Get coordinates\n        loc = self.locations[location_name]\n\n        # Create Nav2 goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose.position.x = loc['x']\n        goal_msg.pose.pose.position.y = loc['y']\n\n        # Convert yaw to quaternion\n        qz = math.sin(loc['yaw'] / 2.0)\n        qw = math.cos(loc['yaw'] / 2.0)\n        goal_msg.pose.pose.orientation.z = qz\n        goal_msg.pose.pose.orientation.w = qw\n\n        # Send goal\n        self.get_logger().info(f\"Navigating to {location_name}: ({loc['x']}, {loc['y']})\")\n        self.nav_client.wait_for_server()\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        send_goal_future.add_done_callback(self.nav_goal_response_callback)\n\n    def nav_goal_response_callback(self, future):\n        \"\"\"Handle navigation goal response\"\"\"\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().error('Navigation goal rejected')\n            return\n\n        self.get_logger().info('Navigation goal accepted')\n        result_future = goal_handle.get_result_async()\n        result_future.add_done_callback(self.nav_result_callback)\n\n    def nav_result_callback(self, future):\n        \"\"\"Handle navigation result\"\"\"\n        result = future.result().result\n        self.get_logger().info('Navigation completed!')\n\n    def execute_rotate(self, params):\n        \"\"\"Rotate in place by specified angle\"\"\"\n        angle = params.get('angle', 90)  # Default 90 degrees\n        direction = params.get('direction', 'left')\n\n        # Convert to radians\n        angle_rad = math.radians(angle)\n        if direction == 'right':\n            angle_rad = -angle_rad\n\n        # Calculate rotation duration (assuming 30 deg/s)\n        angular_velocity = 0.5  # rad/s\n        duration = abs(angle_rad / angular_velocity)\n\n        # Publish rotation command\n        twist = Twist()\n        twist.angular.z = angular_velocity if angle_rad > 0 else -angular_velocity\n\n        self.get_logger().info(f\"Rotating {angle}\xb0 {direction}\")\n\n        # Publish for calculated duration\n        rate = self.create_rate(10)  # 10 Hz\n        start_time = self.get_clock().now()\n\n        while (self.get_clock().now() - start_time).nanoseconds / 1e9 < duration:\n            self.cmd_vel_pub.publish(twist)\n            rate.sleep()\n\n        # Stop rotation\n        self.cmd_vel_pub.publish(Twist())\n        self.get_logger().info(\"Rotation complete\")\n\n    def execute_grasp(self, params):\n        \"\"\"Execute grasping action\"\"\"\n        object_name = params.get('object', 'unknown')\n        self.get_logger().info(f\"Grasping {object_name} (placeholder)\")\n\n        # TODO: Integrate with grasp planning system\n        # 1. Detect object with vision\n        # 2. Plan grasp pose\n        # 3. Execute grasp with gripper action\n\n    def execute_stop(self):\n        \"\"\"Emergency stop\"\"\"\n        self.get_logger().warn(\"STOP command received - halting all motion\")\n\n        # Stop velocity\n        self.cmd_vel_pub.publish(Twist())\n\n        # Cancel all navigation goals\n        # self.nav_client.cancel_all_goals()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    mapper = IntentActionMapper()\n\n    # Example: Execute intents from parser\n    from command_parser import CommandParser, Intent\n\n    parser = CommandParser()\n\n    # Simulate voice commands\n    commands = [\n        \"Go to the kitchen\",\n        \"Turn around\",\n        \"Stop\"\n    ]\n\n    for cmd in commands:\n        intent = parser.parse(cmd)\n        if intent:\n            mapper.execute_intent(intent)\n            rclpy.spin_once(mapper, timeout_sec=2.0)\n\n    mapper.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"section-4-complete-voice-control-system",children:"Section 4: Complete Voice Control System"}),"\n",(0,o.jsx)(e.h3,{id:"subsection-41-integrated-voice-control-node",children:"Subsection 4.1: Integrated Voice Control Node"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example: Full Pipeline"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nComplete voice control system for humanoid robot\n\nPurpose: Whisper \u2192 Parser \u2192 Action execution pipeline\nEnvironment: ROS 2 Humble, Whisper, GPU recommended\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nimport threading\nfrom whisper_microphone import WhisperMicrophone\nfrom command_parser import CommandParser\nfrom intent_action_mapper import IntentActionMapper\n\n\nclass VoiceControlNode(Node):\n    """Complete voice-to-action control system"""\n\n    def __init__(self):\n        super().__init__(\'voice_control_node\')\n\n        # Initialize components\n        self.whisper = WhisperMicrophone(model_size="base")\n        self.parser = CommandParser()\n        self.action_mapper = IntentActionMapper()\n\n        # Voice activation\n        self.declare_parameter(\'wake_word\', \'robot\')  # "Robot, go to kitchen"\n        self.wake_word = self.get_parameter(\'wake_word\').value.lower()\n        self.listening = False\n\n        self.get_logger().info(f\'Voice control ready. Wake word: "{self.wake_word}"\')\n\n    def start(self):\n        """Start voice recognition loop"""\n        self.whisper.start_recording()\n\n        # Run transcription in separate thread\n        transcription_thread = threading.Thread(target=self.transcription_loop)\n        transcription_thread.daemon = True\n        transcription_thread.start()\n\n    def transcription_loop(self):\n        """Process transcriptions continuously"""\n        for transcription in self.whisper.transcribe_stream():\n            self.process_transcription(transcription)\n\n    def process_transcription(self, text: str):\n        """Process transcribed text"""\n        text_lower = text.lower()\n\n        # Check for wake word\n        if self.wake_word in text_lower:\n            self.listening = True\n            self.get_logger().info(f"Wake word detected: \'{self.wake_word}\'")\n\n            # Extract command after wake word\n            command = text_lower.split(self.wake_word, 1)[1].strip()\n            if command:\n                self.execute_command(command)\n            return\n\n        # If already listening, process command\n        if self.listening:\n            self.execute_command(text_lower)\n            self.listening = False  # Single-shot mode\n\n    def execute_command(self, command: str):\n        """Parse and execute voice command"""\n        self.get_logger().info(f"Command received: \'{command}\'")\n\n        # Parse intent\n        intent = self.parser.parse(command)\n\n        if intent:\n            self.get_logger().info(\n                f"Intent: {intent.action} (confidence: {intent.confidence:.2f})"\n            )\n            # Execute action\n            self.action_mapper.execute_intent(intent)\n        else:\n            self.get_logger().warn(f"Could not parse command: \'{command}\'")\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_control = VoiceControlNode()\n\n    voice_control.start()\n\n    try:\n        rclpy.spin(voice_control)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_control.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Running the Complete System"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Terminal 1: Launch robot simulation (Isaac Sim or Gazebo)\nros2 launch humanoid_bringup simulation.launch.py\n\n# Terminal 2: Launch Nav2 navigation\nros2 launch nav2_bringup bringup_launch.py\n\n# Terminal 3: Start voice control\nros2 run voice_control voice_control_node\n\n# Output:\n# [INFO] Voice control ready. Wake word: \"robot\"\n# [INFO] Recording started. Speak into microphone...\n# [INFO] Wake word detected: 'robot'\n# [INFO] Command received: 'go to the kitchen'\n# [INFO] Intent: NAVIGATE (confidence: 1.00)\n# [INFO] Navigating to kitchen: (5.0, 2.0)\n# [INFO] Navigation completed!\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"hands-on-project-voice-controlled-navigation",children:"Hands-On Project: Voice-Controlled Navigation"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Goal"}),": Build a voice-controlled humanoid that navigates to locations on command."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Duration"}),": 45 minutes"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Set up Whisper for real-time transcription"}),"\n",(0,o.jsx)(e.li,{children:"Parse navigation commands"}),"\n",(0,o.jsx)(e.li,{children:"Execute Nav2 goals from voice input"}),"\n",(0,o.jsx)(e.li,{children:"Handle errors gracefully"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-1-install-dependencies",children:"Step 1: Install Dependencies"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip install openai-whisper sounddevice pyaudio\n"})}),"\n",(0,o.jsx)(e.h3,{id:"step-2-test-whisper",children:"Step 2: Test Whisper"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"python -c \"import whisper; model = whisper.load_model('base'); print('Whisper ready')\"\n"})}),"\n",(0,o.jsx)(e.h3,{id:"step-3-implement-voice-node",children:"Step 3: Implement Voice Node"}),"\n",(0,o.jsx)(e.p,{children:"(Use complete code from Section 4.1)"}),"\n",(0,o.jsx)(e.h3,{id:"step-4-define-locations",children:"Step 4: Define Locations"}),"\n",(0,o.jsxs)(e.p,{children:["Edit location database in ",(0,o.jsx)(e.code,{children:"intent_action_mapper.py"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"self.locations = {\n    'kitchen': {'x': 5.0, 'y': 2.0, 'yaw': 0.0},\n    'entrance': {'x': 0.0, 'y': 0.0, 'yaw': 0.0},\n    'office': {'x': -2.0, 'y': 3.0, 'yaw': 1.57},\n}\n"})}),"\n",(0,o.jsx)(e.h3,{id:"step-5-test-voice-commands",children:"Step 5: Test Voice Commands"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:'ros2 run voice_control voice_control_node\n\n# Say: "Robot, go to the kitchen"\n# Say: "Robot, turn around"\n# Say: "Robot, stop"\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"challenge-test-your-understanding",children:"Challenge: Test Your Understanding"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Basic"}),': Add support for "move forward 2 meters" command']}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.em,{children:"Hint"}),": Parse distance, publish ",(0,o.jsx)(e.code,{children:"Twist"})," message for calculated duration"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Intermediate"}),": Implement voice-controlled object grasping with vision"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.em,{children:"Hint"}),": Use YOLO from Module 3, find closest object matching voice description"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Advanced"}),": Add conversation context (multi-turn dialogue)"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.em,{children:"Hint"}),': Maintain state machine, "go there" refers to last mentioned location']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Whisper integration"}),": Real-time speech recognition with 95%+ accuracy"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Intent parsing"}),": Rule-based and LLM approaches for command understanding"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action mapping"}),": Connecting voice intents to ROS 2 actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Complete pipeline"}),": End-to-end voice control system"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Key Concepts"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Wake word detection"}),": Activate robot before command"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Confidence scoring"}),": Handle ambiguous commands gracefully"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action clients"}),": Asynchronous execution of navigation/manipulation tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error handling"}),": Fallback for unrecognized commands"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsx)(e.p,{children:"Official Documentation:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper GitHub"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://docs.ros.org/en/humble/Tutorials/Actions.html",children:"ROS 2 Action Servers"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://people.csail.mit.edu/hubert/pyaudio/",children:"PyAudio Documentation"})}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Tutorials:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/speech-to-text",children:"Building Voice Assistants with Whisper"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://huggingface.co/tasks/text-classification",children:"Intent Recognition with Transformers"})}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Research Papers:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Robust Speech Recognition via Large-Scale Weak Supervision" (Whisper paper, 2022)'}),"\n",(0,o.jsx)(e.li,{children:'"Natural Language Commands for Robot Navigation" (Tellex et al., 2011)'}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Next Chapter"}),": ",(0,o.jsx)(e.a,{href:"/physical-ai-textbook/docs/module4-vla/ch2-cognitive-planning",children:"Chapter 4.2: Cognitive Planning with LLMs"})]}),"\n",(0,o.jsx)(e.p,{children:'In the next chapter, you\'ll learn how to use large language models to decompose complex tasks ("clean the room") into executable action sequences, enabling truly intelligent humanoid behavior.'}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Optional Practice"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Add support for Spanish or French commands (Whisper multilingual)"}),"\n",(0,o.jsx)(e.li,{children:'Implement voice feedback ("I\'m going to the kitchen now")'}),"\n",(0,o.jsx)(e.li,{children:"Create a mobile app that sends voice commands over ROS 2"}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>a});var i=t(6540);const o={},r=i.createContext(o);function s(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);