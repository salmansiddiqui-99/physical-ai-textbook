"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[871],{8453:(n,e,o)=>{o.d(e,{R:()=>r,x:()=>a});var t=o(6540);const i={},s=t.createContext(i);function r(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),t.createElement(s.Provider,{value:e},n.children)}},8629:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module4-vla/ch3-capstone-project","title":"Capstone Project: Fully Autonomous Humanoid Robot","description":"Build a complete autonomous humanoid robot system integrating voice control, LLM planning, Isaac ROS perception, and Nav2 navigation from all 4 modules into a real-world demonstration.","source":"@site/docs/module4-vla/ch3-capstone-project.md","sourceDirName":"module4-vla","slug":"/module4-vla/ch3-capstone-project","permalink":"/physical-ai-textbook/docs/module4-vla/ch3-capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/salmansiddiqui-99/physical-ai-textbook/tree/main/docs/module4-vla/ch3-capstone-project.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"id":"ch3-capstone-project","title":"Capstone Project: Fully Autonomous Humanoid Robot","sidebar_label":"Capstone Project","sidebar_position":12,"description":"Build a complete autonomous humanoid robot system integrating voice control, LLM planning, Isaac ROS perception, and Nav2 navigation from all 4 modules into a real-world demonstration.","keywords":["capstone project","autonomous robot","integration","voice control","LLM planning","ROS 2","humanoid robot","final project"],"prerequisites":["Completion of all previous chapters (Modules 1-4)","Access to Isaac Sim or physical humanoid robot","OpenAI or Anthropic API key"],"learning_objectives":["Integrate all 4 modules into a unified humanoid robot system","Demonstrate autonomous task execution from voice commands","Implement error recovery and graceful degradation strategies","Deploy and test a complete humanoid robot application"],"estimated_time":"120 minutes"},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning","permalink":"/physical-ai-textbook/docs/module4-vla/ch2-cognitive-planning"}}');var i=o(4848),s=o(8453);const r={id:"ch3-capstone-project",title:"Capstone Project: Fully Autonomous Humanoid Robot",sidebar_label:"Capstone Project",sidebar_position:12,description:"Build a complete autonomous humanoid robot system integrating voice control, LLM planning, Isaac ROS perception, and Nav2 navigation from all 4 modules into a real-world demonstration.",keywords:["capstone project","autonomous robot","integration","voice control","LLM planning","ROS 2","humanoid robot","final project"],prerequisites:["Completion of all previous chapters (Modules 1-4)","Access to Isaac Sim or physical humanoid robot","OpenAI or Anthropic API key"],learning_objectives:["Integrate all 4 modules into a unified humanoid robot system","Demonstrate autonomous task execution from voice commands","Implement error recovery and graceful degradation strategies","Deploy and test a complete humanoid robot application"],estimated_time:"120 minutes"},a="Capstone Project: Fully Autonomous Humanoid Robot",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Section 1: System Architecture",id:"section-1-system-architecture",level:2},{value:"Subsection 1.1: Component Integration",id:"subsection-11-component-integration",level:3},{value:"Subsection 1.2: Launch File Structure",id:"subsection-12-launch-file-structure",level:3},{value:"Section 2: Capstone Implementation",id:"section-2-capstone-implementation",level:2},{value:"Subsection 2.1: Safety Monitor",id:"subsection-21-safety-monitor",level:3},{value:"Subsection 2.2: State Management",id:"subsection-22-state-management",level:3},{value:"Section 3: Demonstration Scenarios",id:"section-3-demonstration-scenarios",level:2},{value:"Scenario 1: Room Service Robot",id:"scenario-1-room-service-robot",level:3},{value:"Scenario 2: Home Assistant",id:"scenario-2-home-assistant",level:3},{value:"Section 4: Evaluation &amp; Testing",id:"section-4-evaluation--testing",level:2},{value:"Subsection 4.1: Test Plan",id:"subsection-41-test-plan",level:3},{value:"Subsection 4.2: Rubric",id:"subsection-42-rubric",level:3},{value:"Section 5: Extensions &amp; Future Work",id:"section-5-extensions--future-work",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Congratulations!",id:"congratulations",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"capstone-project-fully-autonomous-humanoid-robot",children:"Capstone Project: Fully Autonomous Humanoid Robot"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Integrate voice control, LLM planning, perception, and navigation into one system"}),"\n",(0,i.jsx)(e.li,{children:"Demonstrate end-to-end autonomous task execution"}),"\n",(0,i.jsx)(e.li,{children:"Implement robust error handling and recovery behaviors"}),"\n",(0,i.jsx)(e.li,{children:"Deploy a production-ready humanoid robot application"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(e.p,{children:"Before starting this capstone, you must have:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["Completed ",(0,i.jsx)(e.strong,{children:"all 11 previous chapters"})," (Modules 1-4)"]}),"\n",(0,i.jsx)(e.li,{children:"Working Isaac Sim or physical humanoid robot setup"}),"\n",(0,i.jsx)(e.li,{children:"All ROS 2 packages installed (Nav2, Isaac ROS, custom nodes)"}),"\n",(0,i.jsx)(e.li,{children:"API keys configured for LLM services"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(e.p,{children:["This capstone project brings together everything learned across all 4 modules into a ",(0,i.jsx)(e.strong,{children:"fully autonomous humanoid robot"})," capable of understanding voice commands, planning complex tasks, perceiving the environment, and navigating safely."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"System Components"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Voice Interface"})," (Module 4.1): Whisper speech recognition"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cognitive Planning"})," (Module 4.2): LLM task decomposition"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Perception"})," (Module 3): Isaac ROS cuVSLAM + stereo depth"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Navigation"})," (Module 3): Nav2 bipedal path planning"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Control"})," (Module 1): ROS 2 action servers and state management"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Demonstration Scenario"}),': "Robot, clean the living room"']}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Voice command triggers LLM planner"}),"\n",(0,i.jsx)(e.li,{children:"Plan: Navigate \u2192 Detect clutter \u2192 Grasp objects \u2192 Navigate to trash \u2192 Release \u2192 Repeat"}),"\n",(0,i.jsx)(e.li,{children:"Execute using Nav2 (navigation), YOLO (detection), and grasp planning"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-1-system-architecture",children:"Section 1: System Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"subsection-11-component-integration",children:"Subsection 1.1: Component Integration"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Architecture Diagram"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    User Interface                        \u2502\n\u2502         (Voice Commands / Mobile App)                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Voice Control Node (Whisper)                   \u2502\n\u2502    Transcribe \u2192 Parse Intent \u2192 Route to Planner         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        Cognitive Planning Node (LLM)                     \u2502\n\u2502    Task Decomposition \u2192 Action Graph Generation         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        Action Graph Executor (ROS 2)                     \u2502\n\u2502    Sequential Action Execution + Error Recovery         \u2502\n\u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2502        \u2502         \u2502            \u2502             \u2502\n  \u25bc        \u25bc         \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Nav\u2502   \u2502Vis\u2502    \u2502Grasp\u2502     \u2502State\u2502       \u2502Safety\u2502\n\u25022  \u2502   \u2502ion\u2502    \u2502Plan \u2502     \u2502Mgmt \u2502       \u2502Mon.  \u2502\n\u2514\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h3,{id:"subsection-12-launch-file-structure",children:"Subsection 1.2: Launch File Structure"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Complete System Launch"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# File: capstone_bringup.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # 1. Isaac Sim (external, must be running)\n        # 2. Isaac ROS Perception\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource('isaac_ros_visual_slam.launch.py')\n        ),\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource('isaac_ros_stereo.launch.py')\n        ),\n        # 3. Nav2 Navigation\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource('nav2_bringup.launch.py'),\n            launch_arguments={'params_file': 'nav2_bipedal_params.yaml'}.items()\n        ),\n        # 4. Voice Control\n        Node(\n            package='voice_control',\n            executable='voice_control_node',\n            parameters=[{'openai_api_key': os.environ.get('OPENAI_API_KEY')}]\n        ),\n        # 5. Cognitive Planning\n        Node(\n            package='cognitive_control',\n            executable='cognitive_humanoid_node',\n            parameters=[{'openai_api_key': os.environ.get('OPENAI_API_KEY')}]\n        ),\n        # 6. Action Executor\n        Node(\n            package='action_execution',\n            executable='action_graph_executor'\n        ),\n        # 7. Safety Monitor\n        Node(\n            package='safety',\n            executable='safety_monitor_node'\n        )\n    ])\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-2-capstone-implementation",children:"Section 2: Capstone Implementation"}),"\n",(0,i.jsx)(e.h3,{id:"subsection-21-safety-monitor",children:"Subsection 2.1: Safety Monitor"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Critical Component"}),": Emergency stop and collision avoidance."]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSafety monitor for humanoid robot\n\nPurpose: Emergency stop, collision detection, battery monitoring\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import BatteryState, PointCloud2\nimport numpy as np\n\n\nclass SafetyMonitor(Node):\n    def __init__(self):\n        super().__init__(\'safety_monitor\')\n\n        # Emergency stop publisher\n        self.estop_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Subscriptions\n        self.create_subscription(PointCloud2, \'/stereo/points2\', self.obstacle_callback, 10)\n        self.create_subscription(BatteryState, \'/battery_state\', self.battery_callback, 10)\n\n        # Safety thresholds\n        self.min_obstacle_distance = 0.3  # meters\n        self.min_battery_percent = 15.0\n\n        self.get_logger().info(\'Safety monitor active\')\n\n    def obstacle_callback(self, msg):\n        """Check for obstacles too close"""\n        # Parse point cloud, check minimum distance\n        # If obstacle < 0.3m ahead, publish STOP\n        pass  # Implement with sensor_msgs_py.point_cloud2\n\n    def battery_callback(self, msg):\n        """Monitor battery level"""\n        battery_percent = msg.percentage * 100\n\n        if battery_percent < self.min_battery_percent:\n            self.get_logger().warn(f\'Low battery: {battery_percent:.1f}%\')\n            # Trigger return-to-charger behavior\n'})}),"\n",(0,i.jsx)(e.h3,{id:"subsection-22-state-management",children:"Subsection 2.2: State Management"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Track execution state"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from enum import Enum\n\nclass RobotState(Enum):\n    IDLE = "idle"\n    LISTENING = "listening"  # Voice active\n    PLANNING = "planning"  # LLM generating plan\n    EXECUTING = "executing"  # Performing actions\n    ERROR = "error"  # Recovery needed\n    CHARGING = "charging"\n\nclass StateManager(Node):\n    def __init__(self):\n        super().__init__(\'state_manager\')\n        self.current_state = RobotState.IDLE\n\n        # State transitions\n        self.transitions = {\n            RobotState.IDLE: [RobotState.LISTENING],\n            RobotState.LISTENING: [RobotState.PLANNING, RobotState.IDLE],\n            RobotState.PLANNING: [RobotState.EXECUTING, RobotState.ERROR],\n            # ...\n        }\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-3-demonstration-scenarios",children:"Section 3: Demonstration Scenarios"}),"\n",(0,i.jsx)(e.h3,{id:"scenario-1-room-service-robot",children:"Scenario 1: Room Service Robot"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Task"}),': "Bring me a bottle of water from the kitchen"']}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Execution Flow"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Voice command captured by Whisper"}),"\n",(0,i.jsxs)(e.li,{children:["LLM generates plan:","\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-json",children:'[\n  {"action": "navigate_to", "params": {"location": "kitchen"}},\n  {"action": "detect_objects", "params": {"object_type": "bottle"}},\n  {"action": "grasp", "params": {"object_id": "bottle_0"}},\n  {"action": "navigate_to", "params": {"location": "user"}},\n  {"action": "place", "params": {"location": "user_hand"}}\n]\n'})}),"\n"]}),"\n",(0,i.jsx)(e.li,{children:"Nav2 navigates to kitchen (cuVSLAM localization)"}),"\n",(0,i.jsx)(e.li,{children:"YOLO detects bottles (Isaac ROS DNN inference)"}),"\n",(0,i.jsx)(e.li,{children:"Grasp planner executes pick (whole-body controller)"}),"\n",(0,i.jsx)(e.li,{children:"Nav2 navigates back to user"}),"\n",(0,i.jsx)(e.li,{children:"Place bottle in hand"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Error Recovery"}),': If bottle not found, LLM replans with "search alternate locations".']}),"\n",(0,i.jsx)(e.h3,{id:"scenario-2-home-assistant",children:"Scenario 2: Home Assistant"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Task"}),': "Clean the living room"']}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Execution"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Navigate to living room"}),"\n",(0,i.jsxs)(e.li,{children:["Loop:","\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Detect clutter objects"}),"\n",(0,i.jsx)(e.li,{children:"Grasp object"}),"\n",(0,i.jsx)(e.li,{children:"Navigate to trash"}),"\n",(0,i.jsx)(e.li,{children:"Release object"}),"\n",(0,i.jsx)(e.li,{children:"Return to living room"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.li,{children:"Until no clutter detected"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-4-evaluation--testing",children:"Section 4: Evaluation & Testing"}),"\n",(0,i.jsx)(e.h3,{id:"subsection-41-test-plan",children:"Subsection 4.1: Test Plan"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Functional Tests"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u2705 Voice command recognition (10 phrases, 95% accuracy target)"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 LLM plan generation (5 tasks, valid JSON output)"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 Navigation success rate (10 trials, 90% success)"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 Object detection precision (IoU > 0.7)"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 End-to-end task completion (3 scenarios, 80% success)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Performance Metrics"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Voice-to-action latency: < 5 seconds"}),"\n",(0,i.jsx)(e.li,{children:"Navigation speed: 0.3-0.5 m/s"}),"\n",(0,i.jsx)(e.li,{children:"Grasp success rate: > 70%"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"subsection-42-rubric",children:"Subsection 4.2: Rubric"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Criterion"}),(0,i.jsx)(e.th,{children:"Weight"}),(0,i.jsx)(e.th,{children:"Evaluation"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"System Integration"}),(0,i.jsx)(e.td,{children:"30%"}),(0,i.jsx)(e.td,{children:"All modules working together"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Task Completion"}),(0,i.jsx)(e.td,{children:"30%"}),(0,i.jsx)(e.td,{children:"Successfully completes 2/3 scenarios"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Code Quality"}),(0,i.jsx)(e.td,{children:"15%"}),(0,i.jsx)(e.td,{children:"Documented, modular, ROS 2 best practices"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Error Handling"}),(0,i.jsx)(e.td,{children:"15%"}),(0,i.jsx)(e.td,{children:"Graceful degradation, recovery behaviors"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Innovation"}),(0,i.jsx)(e.td,{children:"10%"}),(0,i.jsx)(e.td,{children:"Creative extensions beyond requirements"})]})]})]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Total: 100 points"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-5-extensions--future-work",children:"Section 5: Extensions & Future Work"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Suggested Enhancements"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-Robot Coordination"}),": Two humanoids collaborate on tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Visual Feedback"}),": Generate text explanations of robot actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Learning from Demonstrations"}),": Record and replay user-taught tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Adaptive Behaviors"}),": LLM adjusts plans based on environment state"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Mobile App Interface"}),": Control robot from smartphone"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"Congratulations! You've completed the Physical AI & Humanoid Robotics textbook."}),"\n",(0,i.jsx)(e.p,{children:"You've learned to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Module 1"}),": ROS 2 fundamentals and humanoid control"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Module 2"}),": Gazebo/Unity simulation for development"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Module 3"}),": NVIDIA Isaac platform for photorealistic sim and perception"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Module 4"}),": Voice-language-action integration with LLMs"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Final Capstone"}),": Autonomous humanoid executing complex tasks from voice commands!"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Advanced Topics"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://embodied-ai.org/",children:"Embodied AI Research"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://research.google/teams/brain/robotics/",children:"Google Robotics Research"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://robotics-transformer-x.github.io/",children:"Open X-Embodiment Dataset"})}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Community"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"ROS Discourse: discourse.ros.org"}),"\n",(0,i.jsx)(e.li,{children:"NVIDIA Isaac Forums: forums.developer.nvidia.com"}),"\n",(0,i.jsx)(e.li,{children:"Humanoid Robotics Discord: discord.gg/humanoid-robotics"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Career Paths"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Robotics Software Engineer"}),"\n",(0,i.jsx)(e.li,{children:"AI/ML Engineer (Robotics)"}),"\n",(0,i.jsx)(e.li,{children:"Embodied AI Researcher"}),"\n",(0,i.jsx)(e.li,{children:"Humanoid Systems Architect"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"congratulations",children:"Congratulations!"}),"\n",(0,i.jsxs)(e.p,{children:["You've built a complete ",(0,i.jsx)(e.strong,{children:"autonomous humanoid robot system"})," from scratch. This capstone demonstrates production-level integration of:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Speech recognition (Whisper)"}),"\n",(0,i.jsx)(e.li,{children:"Natural language understanding (GPT-4)"}),"\n",(0,i.jsx)(e.li,{children:"Computer vision (Isaac ROS)"}),"\n",(0,i.jsx)(e.li,{children:"Motion planning (Nav2)"}),"\n",(0,i.jsx)(e.li,{children:"Real-time control (ROS 2)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Next Steps"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Deploy on a physical humanoid robot"}),"\n",(0,i.jsx)(e.li,{children:"Contribute to open-source robotics projects"}),"\n",(0,i.jsx)(e.li,{children:"Join robotics competitions (RoboCup, DARPA)"}),"\n",(0,i.jsx)(e.li,{children:"Pursue graduate research in embodied AI"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Thank you for completing this course! \ud83e\udd16\ud83c\udf93"})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);