"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[394],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}},8960:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module3-isaac/ch1-isaac-sim","title":"Isaac Sim Basics: Photorealistic Robot Simulation","description":"Learn NVIDIA Isaac Sim fundamentals including photorealistic rendering, USD scene workflows, synthetic data generation for training perception models, and ROS 2 integration for humanoid robotics.","source":"@site/docs/module3-isaac/ch1-isaac-sim.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/ch1-isaac-sim","permalink":"/physical-ai-textbook/docs/module3-isaac/ch1-isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/salmansiddiqui-99/physical-ai-textbook/tree/main/docs/module3-isaac/ch1-isaac-sim.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"id":"ch1-isaac-sim","title":"Isaac Sim Basics: Photorealistic Robot Simulation","sidebar_label":"Isaac Sim Basics","sidebar_position":7,"description":"Learn NVIDIA Isaac Sim fundamentals including photorealistic rendering, USD scene workflows, synthetic data generation for training perception models, and ROS 2 integration for humanoid robotics.","keywords":["Isaac Sim","NVIDIA Omniverse","USD","photorealistic simulation","synthetic data","RTX rendering","robot simulation","ROS 2 Isaac"],"prerequisites":["Completion of Module 1 (ROS 2 Foundations)","Completion of Module 2 (Simulation Environments)","NVIDIA GPU with RTX support (recommended)","Basic understanding of 3D graphics concepts"],"learning_objectives":["Explain Isaac Sim architecture and its advantages over traditional simulators","Create and configure USD scenes with humanoid robots and environments","Implement synthetic data generation pipelines for computer vision tasks","Integrate Isaac Sim with ROS 2 for realistic humanoid control workflows"],"estimated_time":"90 minutes"},"sidebar":"tutorialSidebar","previous":{"title":"2.3 Unity Visualization","permalink":"/physical-ai-textbook/docs/module2-simulation/ch3-unity-visualization"},"next":{"title":"Isaac ROS Perception","permalink":"/physical-ai-textbook/docs/module3-isaac/ch2-isaac-ros-perception"}}');var r=i(4848),t=i(8453);const a={id:"ch1-isaac-sim",title:"Isaac Sim Basics: Photorealistic Robot Simulation",sidebar_label:"Isaac Sim Basics",sidebar_position:7,description:"Learn NVIDIA Isaac Sim fundamentals including photorealistic rendering, USD scene workflows, synthetic data generation for training perception models, and ROS 2 integration for humanoid robotics.",keywords:["Isaac Sim","NVIDIA Omniverse","USD","photorealistic simulation","synthetic data","RTX rendering","robot simulation","ROS 2 Isaac"],prerequisites:["Completion of Module 1 (ROS 2 Foundations)","Completion of Module 2 (Simulation Environments)","NVIDIA GPU with RTX support (recommended)","Basic understanding of 3D graphics concepts"],learning_objectives:["Explain Isaac Sim architecture and its advantages over traditional simulators","Create and configure USD scenes with humanoid robots and environments","Implement synthetic data generation pipelines for computer vision tasks","Integrate Isaac Sim with ROS 2 for realistic humanoid control workflows"],estimated_time:"90 minutes"},o="Isaac Sim Basics: Photorealistic Robot Simulation",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Section 1: Isaac Sim Architecture and USD Fundamentals",id:"section-1-isaac-sim-architecture-and-usd-fundamentals",level:2},{value:"Subsection 1.1: What is Isaac Sim?",id:"subsection-11-what-is-isaac-sim",level:3},{value:"Subsection 1.2: Universal Scene Description (USD)",id:"subsection-12-universal-scene-description-usd",level:3},{value:"Subsection 1.3: Isaac Sim Workflow Overview",id:"subsection-13-isaac-sim-workflow-overview",level:3},{value:"Section 2: Installing and Launching Isaac Sim",id:"section-2-installing-and-launching-isaac-sim",level:2},{value:"Subsection 2.1: System Requirements",id:"subsection-21-system-requirements",level:3},{value:"Subsection 2.2: Installation via Omniverse Launcher",id:"subsection-22-installation-via-omniverse-launcher",level:3},{value:"Subsection 2.3: Python Standalone Workflows",id:"subsection-23-python-standalone-workflows",level:3},{value:"Section 3: Creating Your First USD Scene",id:"section-3-creating-your-first-usd-scene",level:2},{value:"Subsection 3.1: Scene Composition Basics",id:"subsection-31-scene-composition-basics",level:3},{value:"Subsection 3.2: Importing a URDF Humanoid",id:"subsection-32-importing-a-urdf-humanoid",level:3},{value:"Subsection 3.3: Configuring Physics Parameters",id:"subsection-33-configuring-physics-parameters",level:3},{value:"Section 4: Photorealistic Rendering and Cameras",id:"section-4-photorealistic-rendering-and-cameras",level:2},{value:"Subsection 4.1: RTX Ray Tracing Basics",id:"subsection-41-rtx-ray-tracing-basics",level:3},{value:"Subsection 4.2: Adding Cameras for Perception",id:"subsection-42-adding-cameras-for-perception",level:3},{value:"Subsection 4.3: Semantic and Instance Segmentation",id:"subsection-43-semantic-and-instance-segmentation",level:3},{value:"Section 5: ROS 2 Integration",id:"section-5-ros-2-integration",level:2},{value:"Subsection 5.1: ROS 2 Bridge Architecture",id:"subsection-51-ros-2-bridge-architecture",level:3},{value:"Subsection 5.2: Creating ROS 2 Action Graph",id:"subsection-52-creating-ros-2-action-graph",level:3},{value:"Subsection 5.3: Subscribing to Joint Commands",id:"subsection-53-subscribing-to-joint-commands",level:3},{value:"Section 6: Synthetic Data Generation",id:"section-6-synthetic-data-generation",level:2},{value:"Subsection 6.1: Replicator API Overview",id:"subsection-61-replicator-api-overview",level:3},{value:"Subsection 6.2: Creating a Replicator Script",id:"subsection-62-creating-a-replicator-script",level:3},{value:"Hands-On Project: Humanoid in Warehouse Scene",id:"hands-on-project-humanoid-in-warehouse-scene",level:2},{value:"Step 1: Create Warehouse Environment",id:"step-1-create-warehouse-environment",level:3},{value:"Step 2: Import Humanoid and Attach Sensors",id:"step-2-import-humanoid-and-attach-sensors",level:3},{value:"Step 3: Create ROS 2 Bridge",id:"step-3-create-ros-2-bridge",level:3},{value:"Step 4: Run Simulation",id:"step-4-run-simulation",level:3},{value:"Step 5: Verify in RViz2",id:"step-5-verify-in-rviz2",level:3},{value:"Challenge: Test Your Understanding",id:"challenge-test-your-understanding",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"isaac-sim-basics-photorealistic-robot-simulation",children:"Isaac Sim Basics: Photorealistic Robot Simulation"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain Isaac Sim architecture, USD workflows, and photorealistic rendering capabilities"}),"\n",(0,r.jsx)(n.li,{children:"Create USD scenes with humanoid robots, sensors, and realistic environments"}),"\n",(0,r.jsx)(n.li,{children:"Implement synthetic data generation for training perception models"}),"\n",(0,r.jsx)(n.li,{children:"Integrate Isaac Sim with ROS 2 for realistic sensor simulation and control"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this chapter, you should:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Have completed ",(0,r.jsx)(n.a,{href:"/physical-ai-textbook/docs/module1-ros2/ch1-ros2-basics",children:"Module 1: ROS 2 Foundations"})," and ",(0,r.jsx)(n.a,{href:"/physical-ai-textbook/docs/module2-simulation/ch1-gazebo-essentials",children:"Module 2: Simulation"})]}),"\n",(0,r.jsx)(n.li,{children:"Understand basic 3D graphics concepts (meshes, materials, lighting)"}),"\n",(0,r.jsx)(n.li,{children:"Have access to an NVIDIA GPU with RTX support (RTX 2060 or higher recommended)"}),"\n",(0,r.jsx)(n.li,{children:"Have installed Isaac Sim 2023.1.1 or later (see installation guide below)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"NVIDIA Isaac Sim represents a paradigm shift in robot simulation. Unlike traditional simulators that prioritize speed over realism, Isaac Sim leverages RTX ray tracing and physically-based rendering to create photorealistic environments indistinguishable from real-world camera feeds. This realism is critical for humanoid robotics, where perception models trained on synthetic data must transfer seamlessly to physical robots."}),"\n",(0,r.jsx)(n.p,{children:"Isaac Sim is built on NVIDIA Omniverse, a platform for real-time 3D collaboration and simulation using Universal Scene Description (USD). USD, originally developed by Pixar, provides a powerful framework for composing, editing, and rendering complex 3D scenes. By adopting USD as its scene format, Isaac Sim enables artists, engineers, and roboticists to collaborate using industry-standard tools like Blender, Maya, and Unreal Engine."}),"\n",(0,r.jsx)(n.p,{children:"For humanoid robots operating in human environments, Isaac Sim offers unparalleled advantages. You can generate millions of labeled images showing humanoids grasping objects, navigating cluttered spaces, or interacting with people\u2014all with perfect ground truth for object poses, depth maps, and semantic segmentation. This chapter will guide you through setting up Isaac Sim, creating your first USD scene, and generating synthetic data for training vision-based humanoid controllers."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-1-isaac-sim-architecture-and-usd-fundamentals",children:"Section 1: Isaac Sim Architecture and USD Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-11-what-is-isaac-sim",children:"Subsection 1.1: What is Isaac Sim?"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Sim is NVIDIA's robotics simulation platform built on Omniverse. It combines:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PhysX 5.0"}),": High-fidelity physics engine with GPU acceleration for contacts, rigid bodies, and articulations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX Rendering"}),": Ray-traced lighting, shadows, and reflections for photorealistic visuals"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"USD Scene Format"}),": Industry-standard 3D scene description enabling interoperability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac SDK/ROS Integration"}),": Native ROS 2 bridge for seamless robot control"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Automated labeling for computer vision (bounding boxes, segmentation, depth)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Differences from Gazebo"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"Gazebo Classic/Fortress"}),(0,r.jsx)(n.th,{children:"Isaac Sim"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Rendering"}),(0,r.jsx)(n.td,{children:"OpenGL (approximate lighting)"}),(0,r.jsx)(n.td,{children:"RTX ray tracing (photorealistic)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Physics"}),(0,r.jsx)(n.td,{children:"ODE/Bullet/DART"}),(0,r.jsx)(n.td,{children:"PhysX 5.0 (GPU-accelerated)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Scene Format"}),(0,r.jsx)(n.td,{children:"SDF/URDF (custom)"}),(0,r.jsx)(n.td,{children:"USD (industry standard)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Synthetic Data"}),(0,r.jsx)(n.td,{children:"Manual scripting required"}),(0,r.jsx)(n.td,{children:"Built-in replicator API"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Performance"}),(0,r.jsx)(n.td,{children:"CPU-bound"}),(0,r.jsx)(n.td,{children:"GPU-accelerated (10-100x faster)"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-12-universal-scene-description-usd",children:"Subsection 1.2: Universal Scene Description (USD)"}),"\n",(0,r.jsx)(n.p,{children:'USD is a file format and runtime API for describing 3D scenes. Think of it as "Photoshop layers" for 3D content:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Layers"}),": Compose scenes from reusable components (robot, environment, sensors)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prims"}),": Scene graph nodes representing objects (meshes, cameras, lights)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Attributes"}),": Properties like position, color, or physics parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants"}),": Switchable configurations (e.g., day/night lighting, different robot payloads)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"USD File Types"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:".usd"}),": Binary format (fast loading, production use)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:".usda"}),": ASCII format (human-readable, version control friendly)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:".usdc"}),": Compact binary format (smallest file size)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example USD Hierarchy"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"World.usd\n\u251c\u2500\u2500 /Environment\n\u2502   \u251c\u2500\u2500 /Warehouse (mesh, materials)\n\u2502   \u251c\u2500\u2500 /Lighting (HDR dome light)\n\u2502   \u2514\u2500\u2500 /Ground (physics collision plane)\n\u251c\u2500\u2500 /Humanoid\n\u2502   \u251c\u2500\u2500 /robot_base (articulation root)\n\u2502   \u251c\u2500\u2500 /Sensors\n\u2502   \u2502   \u251c\u2500\u2500 /head_camera\n\u2502   \u2502   \u2514\u2500\u2500 /chest_lidar\n\u2502   \u2514\u2500\u2500 /Controllers (ROS 2 action graph)\n\u2514\u2500\u2500 /ReplicatorGraph (synthetic data config)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-13-isaac-sim-workflow-overview",children:"Subsection 1.3: Isaac Sim Workflow Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Typical Development Cycle"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scene Assembly"}),": Import robot URDF, add environments, configure sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physics Tuning"}),": Set friction, damping, contact parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Configuration"}),": Attach cameras, LiDAR, IMU with realistic noise"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Integration"}),": Create action graph for pub/sub communication"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synthetic Data Setup"}),": Define replicator for data generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execution"}),": Run simulation, collect data or test control algorithms"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-2-installing-and-launching-isaac-sim",children:"Section 2: Installing and Launching Isaac Sim"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-21-system-requirements",children:"Subsection 2.1: System Requirements"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Minimum Specifications"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OS"}),": Ubuntu 20.04/22.04 or Windows 10/11"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPU"}),": NVIDIA RTX 2060 or higher (6GB VRAM minimum)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CPU"}),": Intel i7 or AMD Ryzen 7 (8 cores recommended)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RAM"}),": 32GB (64GB recommended for large scenes)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Storage"}),": 50GB free space (SSD recommended)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Drivers"}),": NVIDIA GPU Driver 525.60.11 or later"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommended for Humanoid Workflows"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPU"}),": RTX 3090 / RTX 4080 / A6000 (24GB VRAM for complex scenes)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RAM"}),": 64GB+ (enables multiple humanoids in scene)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Storage"}),": NVMe SSD (faster scene loading and data writing)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-22-installation-via-omniverse-launcher",children:"Subsection 2.2: Installation via Omniverse Launcher"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Sim is distributed through the NVIDIA Omniverse Launcher."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 1: Download Omniverse Launcher"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Visit https://www.nvidia.com/en-us/omniverse/download/\n# Download and install Omniverse Launcher for your OS\n\n# On Linux, run the AppImage:\nchmod +x omniverse-launcher-linux.AppImage\n./omniverse-launcher-linux.AppImage\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 2: Install Isaac Sim"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Open Omniverse Launcher"}),"\n",(0,r.jsx)(n.li,{children:'Navigate to "Exchange" tab'}),"\n",(0,r.jsx)(n.li,{children:'Search for "Isaac Sim"'}),"\n",(0,r.jsx)(n.li,{children:'Click "Install" (downloads ~15GB)'}),"\n",(0,r.jsxs)(n.li,{children:["Choose installation path (default: ",(0,r.jsx)(n.code,{children:"~/.local/share/ov/pkg/isaac_sim-2023.1.1"}),")"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 3: Verify Installation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Launch Isaac Sim from Launcher or via command line\ncd ~/.local/share/ov/pkg/isaac_sim-2023.1.1\n./isaac-sim.sh\n"})}),"\n",(0,r.jsx)(n.p,{children:"You should see the Isaac Sim GUI with a welcome screen."}),"\n",(0,r.jsx)(n.h3,{id:"subsection-23-python-standalone-workflows",children:"Subsection 2.3: Python Standalone Workflows"}),"\n",(0,r.jsx)(n.p,{children:"For headless data generation or ROS 2 integration, use the Python API:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Running Isaac Sim Programmatically"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nMinimal Isaac Sim Python example\n\nPurpose: Launch Isaac Sim, create a simple scene, run simulation loop\nEnvironment: Isaac Sim 2023.1.1+, Python 3.10\n"""\n\nfrom isaacsim import SimulationApp\n\n# Create simulation app instance (headless=True for servers)\nsimulation_app = SimulationApp({"headless": False})\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.objects import DynamicCuboid\n\n# Create physics world\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_default_ground_plane()\n\n# Add a cube that will fall\ncube = DynamicCuboid(\n    prim_path="/World/Cube",\n    position=[0, 0, 2.0],  # 2 meters above ground\n    size=0.5,  # 50cm cube\n    color=[0.8, 0.2, 0.1]  # Orange\n)\nworld.scene.add(cube)\n\n# Reset simulation\nworld.reset()\n\n# Run simulation for 500 steps\nfor i in range(500):\n    world.step(render=True)  # Step physics and render\n    if i % 50 == 0:\n        position, _ = cube.get_world_pose()\n        print(f"Step {i}: Cube at height {position[2]:.2f}m")\n\n# Cleanup\nsimulation_app.close()\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Running the Script"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Must use Isaac Sim's Python interpreter\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/python.sh minimal_example.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Step 0: Cube at height 2.00m\nStep 50: Cube at height 1.52m\nStep 100: Cube at height 0.87m\nStep 150: Cube at height 0.31m\nStep 200: Cube at height 0.25m  # Settled on ground\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-3-creating-your-first-usd-scene",children:"Section 3: Creating Your First USD Scene"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-31-scene-composition-basics",children:"Subsection 3.1: Scene Composition Basics"}),"\n",(0,r.jsxs)(n.p,{children:["USD scenes are built through ",(0,r.jsx)(n.strong,{children:"composition"}),": layering multiple USD files to create complex environments."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Composition Arcs"})," (in order of strength):"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sublayers"}),": Merge layers (e.g., base scene + lighting overrides)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"References"}),": Reuse assets (e.g., include robot USD from library)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Payloads"}),": Lazy-load heavy assets (e.g., high-poly environment meshes)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants"}),": Switchable options (e.g., day/night lighting)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Best Practices"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"One robot = one USD file"}),": Package robot, sensors, controllers together"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment assets as references"}),": Reuse warehouse, office, outdoor scenes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensors as prims"}),": Cameras and LiDAR as child prims of robot links"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use variants for configurations"}),": Switch between simulation/real-world sensor params"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-32-importing-a-urdf-humanoid",children:"Subsection 3.2: Importing a URDF Humanoid"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Sim can import URDF files and convert them to USD format with physics."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Importing Humanoid URDF"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.urdf import _urdf\nimport omni.isaac.core.utils.prims as prims_utils\n\n# Import URDF (Isaac Sim converts to USD automatically)\nurdf_path = "/path/to/humanoid.urdf"\nrobot_prim_path = "/World/Humanoid"\n\n# Import with physics enabled\nimport_config = _urdf.ImportConfig()\nimport_config.merge_fixed_joints = False  # Keep kinematic structure\nimport_config.convex_decomp = True  # Use convex hulls for collision\nimport_config.import_inertia_tensor = True\nimport_config.default_drive_strength = 100000  # Joint motor strength\nimport_config.default_position_drive_damping = 10000\n\nsuccess, robot_prim = omni.kit.commands.execute(\n    "URDFParseAndImportFile",\n    urdf_path=urdf_path,\n    import_config=import_config,\n    dest_path=robot_prim_path\n)\n\nif success:\n    print(f"Robot imported at {robot_prim_path}")\nelse:\n    print("URDF import failed!")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What Happens During Import"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["URDF ",(0,r.jsx)(n.code,{children:"<link>"})," \u2192 USD ",(0,r.jsx)(n.code,{children:"Xformable"})," prims with meshes"]}),"\n",(0,r.jsxs)(n.li,{children:["URDF ",(0,r.jsx)(n.code,{children:"<joint>"})," \u2192 PhysX ",(0,r.jsx)(n.code,{children:"ArticulationJoint"})," (revolute/prismatic)"]}),"\n",(0,r.jsxs)(n.li,{children:["URDF ",(0,r.jsx)(n.code,{children:"<inertial>"})," \u2192 PhysX mass and inertia properties"]}),"\n",(0,r.jsxs)(n.li,{children:["URDF ",(0,r.jsx)(n.code,{children:"<collision>"})," \u2192 PhysX collision shapes (convex hulls if enabled)"]}),"\n",(0,r.jsxs)(n.li,{children:["URDF ",(0,r.jsx)(n.code,{children:"<visual>"})," \u2192 Mesh geometry with materials"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-33-configuring-physics-parameters",children:"Subsection 3.3: Configuring Physics Parameters"}),"\n",(0,r.jsx)(n.p,{children:"After importing, fine-tune physics for realistic behavior."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Parameters for Humanoids"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import omni.physx.scripts.utils as physx_utils\n\n# Set global simulation parameters\nphysx_scene = world.get_physics_context()\nphysx_scene.set_gravity(-9.81)  # Earth gravity\nphysx_scene.set_solver_type("TGS")  # Temporal Gauss-Seidel (stable)\n\n# Configure articulation (whole robot)\nfrom pxr import PhysxSchema\narticulation_prim = world.stage.GetPrimAtPath("/World/Humanoid")\nphysx_articulation_api = PhysxSchema.PhysxArticulationAPI.Apply(articulation_prim)\nphysx_articulation_api.CreateSolverPositionIterationCountAttr(32)  # More iterations = more stable\nphysx_articulation_api.CreateSolverVelocityIterationCountAttr(16)\n\n# Set ground contact friction\nground_prim = world.stage.GetPrimAtPath("/World/groundPlane")\nphysx_collision_api = PhysxSchema.PhysxCollisionAPI.Apply(ground_prim)\nphysx_material = PhysxSchema.PhysxMaterialAPI.Apply(ground_prim)\nphysx_material.CreateStaticFrictionAttr(0.8)  # Prevents slipping\nphysx_material.CreateDynamicFrictionAttr(0.6)\nphysx_material.CreateRestitutionAttr(0.01)  # Low bounce\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Troubleshooting Physics"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Issue"}),(0,r.jsx)(n.th,{children:"Cause"}),(0,r.jsx)(n.th,{children:"Solution"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Robot explodes on contact"}),(0,r.jsx)(n.td,{children:"Iteration count too low"}),(0,r.jsxs)(n.td,{children:["Increase ",(0,r.jsx)(n.code,{children:"solverPositionIterationCount"})," to 64+"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Feet slip during walking"}),(0,r.jsx)(n.td,{children:"Friction too low"}),(0,r.jsxs)(n.td,{children:["Increase ",(0,r.jsx)(n.code,{children:"staticFriction"})," to 1.0-1.5"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Simulation too slow"}),(0,r.jsx)(n.td,{children:"Too many collision checks"}),(0,r.jsx)(n.td,{children:"Use convex decomposition, reduce mesh complexity"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Joints drift over time"}),(0,r.jsx)(n.td,{children:"Damping too low"}),(0,r.jsxs)(n.td,{children:["Increase ",(0,r.jsx)(n.code,{children:"positionDriveDamping"})," in joints"]})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-4-photorealistic-rendering-and-cameras",children:"Section 4: Photorealistic Rendering and Cameras"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-41-rtx-ray-tracing-basics",children:"Subsection 4.1: RTX Ray Tracing Basics"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Sim uses NVIDIA RTX for real-time ray tracing."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Rendering Modes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX - Real-Time"}),": Interactive ray tracing (good for development)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX - Interactive (Path Tracing)"}),": Higher quality, slower (1-2 FPS for complex scenes)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX - Accurate (Path Tracing)"}),": Production quality (offline rendering)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Enabling RTX in Python"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import omni.kit.viewport.utility as viewport_utils\n\n# Get active viewport\nviewport = viewport_utils.get_active_viewport()\n\n# Set RTX real-time mode\nviewport_utils.get_viewport_widget(viewport).set_render_mode("rtx_realtime")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-42-adding-cameras-for-perception",children:"Subsection 4.2: Adding Cameras for Perception"}),"\n",(0,r.jsx)(n.p,{children:"Cameras are the primary perception sensors for humanoid robots."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Attach RGB Camera to Robot Head"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core.utils import stage\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\n# Create camera at robot head position\ncamera = Camera(\n    prim_path="/World/Humanoid/head/camera",\n    position=np.array([0.05, 0, 0.15]),  # 5cm forward, 15cm up from head link\n    frequency=30,  # 30 Hz (standard video framerate)\n    resolution=(1280, 720),  # HD resolution\n    orientation=np.array([1, 0, 0, 0])  # Quaternion (identity = looking forward)\n)\n\n# Initialize camera\ncamera.initialize()\n\n# Set camera properties\ncamera.set_focal_length(24.0)  # Focal length in mm (wide angle)\ncamera.set_focus_distance(400.0)  # Focus at 4 meters\ncamera.set_f_stop(8.0)  # Aperture (higher = more in focus)\ncamera.set_clipping_range(0.01, 10000.0)  # Near/far clip planes\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capturing Images"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# After world.step(), capture camera data\ncamera.initialize()\ncamera_data = camera.get_current_frame()\n\nrgb_image = camera_data["rgba"]  # Shape: (720, 1280, 4), dtype: uint8\ndepth_map = camera_data["depth"]  # Shape: (720, 1280), dtype: float32\n\nprint(f"Captured {rgb_image.shape[0]}x{rgb_image.shape[1]} RGB image")\nprint(f"Depth range: {depth_map.min():.2f}m to {depth_map.max():.2f}m")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-43-semantic-and-instance-segmentation",children:"Subsection 4.3: Semantic and Instance Segmentation"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Sim can render perfect ground truth for object detection."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Enable Segmentation Render Product"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core.utils import render_product\n\n# Create render product for segmentation\nrp = render_product.create_render_product(\n    camera.prim_path,\n    resolution=(1280, 720)\n)\n\n# Annotate with semantic segmentation\nfrom omni.syntheticdata import sensors\nsem_sensor = sensors.enable_sensors(\n    render_product=rp,\n    sensor_types=["semantic_segmentation", "instance_segmentation"]\n)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Accessing Segmentation Masks"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# After world.step()\nsemantic_data = sem_sensor.get_semantic_data()\ninstance_data = sem_sensor.get_instance_data()\n\n# semantic_data: (720, 1280) with class IDs per pixel\n# instance_data: (720, 1280) with unique instance IDs per object\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Semantic Class Mapping"}),":\nIsaac Sim uses a class mapping to label objects:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Class 0: Unlabeled/background"}),"\n",(0,r.jsx)(n.li,{children:'Class 1-N: Custom labels (e.g., "person", "chair", "door")'}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-5-ros-2-integration",children:"Section 5: ROS 2 Integration"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-51-ros-2-bridge-architecture",children:"Subsection 5.1: ROS 2 Bridge Architecture"}),"\n",(0,r.jsxs)(n.p,{children:["Isaac Sim includes a built-in ROS 2 bridge using ",(0,r.jsx)(n.strong,{children:"OmniGraph"})," (visual scripting for robotics)."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Components"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Context"}),": Initializes ROS 2 node within Isaac Sim"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Publishers"}),": Send sensor data, joint states, TF transforms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Subscribers"}),": Receive velocity commands, joint commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Clock"}),": Synchronize ROS 2 time with simulation time"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-52-creating-ros-2-action-graph",children:"Subsection 5.2: Creating ROS 2 Action Graph"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Publish Camera Images to ROS 2"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import omni.graph.core as og\n\n# Create action graph\n(graph, nodes, _, _) = og.Controller.edit(\n    {"graph_path": "/ActionGraph", "evaluator_name": "execution"},\n    {\n        og.Controller.Keys.CREATE_NODES: [\n            ("OnPlaybackTick", "omni.graph.action.OnPlaybackTick"),\n            ("ROS2Context", "omni.isaac.ros2_bridge.ROS2Context"),\n            ("CameraHelper", "omni.isaac.ros2_bridge.ROS2CameraHelper"),\n        ],\n        og.Controller.Keys.CONNECT: [\n            ("OnPlaybackTick.outputs:tick", "CameraHelper.inputs:execIn"),\n        ],\n        og.Controller.Keys.SET_VALUES: [\n            ("ROS2Context.inputs:domain_id", 0),\n            ("CameraHelper.inputs:topicName", "/humanoid/head_camera/image_raw"),\n            ("CameraHelper.inputs:frameId", "camera_link"),\n            ("CameraHelper.inputs:type", "rgb"),\n            ("CameraHelper.inputs:renderProductPath", rp),\n        ],\n    },\n)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Verifying ROS 2 Topics"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# In separate terminal (source ROS 2 workspace first)\nsource /opt/ros/humble/setup.bash\n\n# List topics published by Isaac Sim\nros2 topic list\n\n# Expected output:\n# /humanoid/head_camera/image_raw\n# /humanoid/head_camera/camera_info\n\n# View images in RViz2\nrviz2\n# Add Image display, set topic to /humanoid/head_camera/image_raw\n"})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-53-subscribing-to-joint-commands",children:"Subsection 5.3: Subscribing to Joint Commands"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Control Robot Joints via ROS 2"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Add articulation controller to action graph\n(graph, nodes, _, _) = og.Controller.edit(\n    {"graph_path": "/ActionGraph"},\n    {\n        og.Controller.Keys.CREATE_NODES: [\n            ("ArticulationController", "omni.isaac.core_nodes.IsaacArticulationController"),\n            ("ROS2SubscribeJointState", "omni.isaac.ros2_bridge.ROS2SubscribeJointState"),\n        ],\n        og.Controller.Keys.CONNECT: [\n            ("OnPlaybackTick.outputs:tick", "ROS2SubscribeJointState.inputs:execIn"),\n            ("ROS2SubscribeJointState.outputs:jointNames", "ArticulationController.inputs:jointNames"),\n            ("ROS2SubscribeJointState.outputs:positionCommand", "ArticulationController.inputs:positionCommand"),\n        ],\n        og.Controller.Keys.SET_VALUES: [\n            ("ROS2SubscribeJointState.inputs:topicName", "/joint_command"),\n            ("ArticulationController.inputs:robotPath", "/World/Humanoid"),\n        ],\n    },\n)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Publishing Joint Commands from ROS 2"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\n\nclass JointCommandPublisher(Node):\n    def __init__(self):\n        super().__init__('joint_command_publisher')\n        self.publisher = self.create_publisher(JointState, '/joint_command', 10)\n        self.timer = self.create_timer(0.1, self.publish_command)\n\n    def publish_command(self):\n        msg = JointState()\n        msg.name = ['left_shoulder_pitch', 'right_shoulder_pitch']\n        msg.position = [0.5, -0.5]  # Raise left arm, lower right arm\n        self.publisher.publish(msg)\n\nrclpy.init()\nnode = JointCommandPublisher()\nrclpy.spin(node)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-6-synthetic-data-generation",children:"Section 6: Synthetic Data Generation"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-61-replicator-api-overview",children:"Subsection 6.1: Replicator API Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Omniverse Replicator"})," is Isaac Sim's tool for generating synthetic training data at scale."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Features"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Randomization"}),": Procedurally vary lighting, textures, object poses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ground Truth Labels"}),": Automatic bounding boxes, segmentation, keypoints"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"High Throughput"}),": Generate 10,000+ labeled images per hour on RTX 3090"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization"}),": Reduce sim-to-real gap via visual diversity"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-62-creating-a-replicator-script",children:"Subsection 6.2: Creating a Replicator Script"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Generate 1000 Images of Humanoid Grasping Objects"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\n\n# Define randomization for camera position\nwith rep.new_layer():\n    # Camera orbiting around humanoid\n    camera = rep.create.camera(position=(2, 0, 1.5), look_at="/World/Humanoid/torso")\n\n    # Randomize camera position in spherical coordinates\n    with camera:\n        rep.modify.pose(\n            position=rep.distribution.sphere(\n                center=(0, 0, 1.2),  # Center on humanoid torso\n                radius=rep.distribution.uniform(1.5, 3.0)  # 1.5-3m distance\n            ),\n            look_at="/World/Humanoid/torso"\n        )\n\n    # Randomize lighting\n    light = rep.create.light(\n        light_type="Dome",\n        rotation=(rep.distribution.uniform(0, 360), 0, 0)\n    )\n\n    # Randomize object to grasp (place in humanoid\'s hand)\n    objects = ["/World/Objects/Mug", "/World/Objects/Bottle", "/World/Objects/Box"]\n    with rep.create.group(objects):\n        rep.randomizer.scatter_2d(\n            surface_prims="/World/Humanoid/right_hand",\n            check_for_collisions=True\n        )\n\n    # Render and capture\n    render_product = rep.create.render_product(camera, resolution=(1280, 720))\n\n    # Attach writers for data output\n    writer = rep.WriterRegistry.get("BasicWriter")\n    writer.initialize(\n        output_dir="/tmp/humanoid_grasp_dataset",\n        rgb=True,\n        semantic_segmentation=True,\n        bounding_box_2d_tight=True,\n        distance_to_camera=True\n    )\n    writer.attach([render_product])\n\n# Run replicator for 1000 frames\nwith rep.orchestrator.step(num_steps=1000):\n    rep.randomizer.randomize()\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Running Data Generation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"~/.local/share/ov/pkg/isaac_sim-2023.1.1/python.sh generate_dataset.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output Structure"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"/tmp/humanoid_grasp_dataset/\n\u251c\u2500\u2500 rgb_0000.png\n\u251c\u2500\u2500 rgb_0001.png\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 semantic_segmentation_0000.png\n\u251c\u2500\u2500 bounding_box_2d_tight_0000.json\n\u251c\u2500\u2500 distance_to_camera_0000.npy\n\u2514\u2500\u2500 metadata.json\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-project-humanoid-in-warehouse-scene",children:"Hands-On Project: Humanoid in Warehouse Scene"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Create a photorealistic warehouse scene with a humanoid robot, attach sensors, and publish data to ROS 2."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 45 minutes"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Import and assemble USD assets into a scene"}),"\n",(0,r.jsx)(n.li,{children:"Configure humanoid physics and sensors"}),"\n",(0,r.jsx)(n.li,{children:"Integrate ROS 2 for real-time teleoperation"}),"\n",(0,r.jsx)(n.li,{children:"Generate synthetic perception data"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-warehouse-environment",children:"Step 1: Create Warehouse Environment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\n\n# Initialize world\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_default_ground_plane()\n\n# Add warehouse asset (from NVIDIA assets library)\nadd_reference_to_stage(\n    usd_path="omniverse://localhost/NVIDIA/Assets/Environments/Industrial/warehouse.usd",\n    prim_path="/World/Warehouse"\n)\n\n# Add lighting\nimport omni.isaac.core.utils.prims as prims_utils\nprims_utils.create_prim(\n    prim_path="/World/DomeLight",\n    prim_type="DomeLight",\n    attributes={"intensity": 1000.0, "color": (1.0, 0.95, 0.9)}\n)\n\nprint("Warehouse scene created")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-import-humanoid-and-attach-sensors",children:"Step 2: Import Humanoid and Attach Sensors"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from omni.isaac.urdf import _urdf\n\n# Import humanoid URDF\nimport_config = _urdf.ImportConfig()\nimport_config.merge_fixed_joints = False\nimport_config.convex_decomp = True\nimport_config.default_drive_strength = 100000\n\nomni.kit.commands.execute(\n    "URDFParseAndImportFile",\n    urdf_path="/path/to/humanoid.urdf",\n    import_config=import_config,\n    dest_path="/World/Humanoid"\n)\n\n# Attach camera to head\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\nhead_camera = Camera(\n    prim_path="/World/Humanoid/head/camera",\n    position=np.array([0.05, 0, 0.15]),\n    frequency=30,\n    resolution=(1280, 720)\n)\nhead_camera.initialize()\n\nprint("Humanoid and sensors configured")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-create-ros-2-bridge",children:"Step 3: Create ROS 2 Bridge"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import omni.graph.core as og\n\n# Create ROS 2 action graph\nkeys = og.Controller.Keys\n(graph, nodes, _, _) = og.Controller.edit(\n    {"graph_path": "/ActionGraph", "evaluator_name": "execution"},\n    {\n        keys.CREATE_NODES: [\n            ("OnTick", "omni.graph.action.OnPlaybackTick"),\n            ("ROS2Context", "omni.isaac.ros2_bridge.ROS2Context"),\n            ("CameraPublisher", "omni.isaac.ros2_bridge.ROS2CameraHelper"),\n            ("JointStatePublisher", "omni.isaac.ros2_bridge.ROS2PublishJointState"),\n        ],\n        keys.CONNECT: [\n            ("OnTick.outputs:tick", "CameraPublisher.inputs:execIn"),\n            ("OnTick.outputs:tick", "JointStatePublisher.inputs:execIn"),\n        ],\n        keys.SET_VALUES: [\n            ("ROS2Context.inputs:domain_id", 0),\n            ("CameraPublisher.inputs:topicName", "/humanoid/camera/image_raw"),\n            ("CameraPublisher.inputs:type", "rgb"),\n            ("JointStatePublisher.inputs:topicName", "/joint_states"),\n            ("JointStatePublisher.inputs:targetPrim", "/World/Humanoid"),\n        ],\n    },\n)\n\nprint("ROS 2 bridge created")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-run-simulation",children:"Step 4: Run Simulation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Reset world\nworld.reset()\n\n# Run simulation loop\nfor i in range(1000):\n    world.step(render=True)\n\n    if i % 100 == 0:\n        # Get camera image\n        camera_data = head_camera.get_current_frame()\n        print(f"Step {i}: Captured image shape {camera_data[\'rgba\'].shape}")\n\nprint("Simulation complete")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-5-verify-in-rviz2",children:"Step 5: Verify in RViz2"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# In separate terminal\nsource /opt/ros/humble/setup.bash\n\n# Launch RViz2\nrviz2\n\n# Add displays:\n# - Image: /humanoid/camera/image_raw\n# - RobotModel: /robot_description\n# - TF: Shows coordinate frames\n\n# You should see photorealistic camera feed from Isaac Sim!\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"challenge-test-your-understanding",children:"Challenge: Test Your Understanding"}),"\n",(0,r.jsx)(n.p,{children:"Try these exercises to reinforce your learning:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Basic"}),": Modify the warehouse scene to include 5 random boxes on the ground"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Hint"}),": Use ",(0,r.jsx)(n.code,{children:"DynamicCuboid"})," with random positions via ",(0,r.jsx)(n.code,{children:"rep.distribution.uniform"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Intermediate"}),": Create a replicator script that generates 100 images with random humanoid arm poses"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Hint"}),": Use ",(0,r.jsx)(n.code,{children:"rep.modify.semantics"})," to change joint positions, then ",(0,r.jsx)(n.code,{children:"rep.trigger.on_frame()"})," to capture"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advanced"}),": Build a ROS 2 node that subscribes to Isaac Sim camera images and runs YOLO object detection"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Hint"}),": Use ",(0,r.jsx)(n.code,{children:"cv_bridge"})," to convert ROS Image messages to OpenCV format, then apply YOLOv8"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac Sim's architecture"}),": PhysX, RTX, USD, and how it differs from Gazebo (photorealism, GPU acceleration)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"USD workflows"}),": Creating scenes via composition, importing URDFs, configuring physics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Photorealistic rendering"}),": Attaching cameras, capturing RGB/depth/segmentation data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 integration"}),": Using OmniGraph to publish sensor data and subscribe to commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synthetic data generation"}),": Replicator API for domain randomization and ground truth labels"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Commands"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Launch Isaac Sim GUI\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/isaac-sim.sh\n\n# Run headless Python script\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/python.sh your_script.py\n\n# Verify ROS 2 topics from Isaac Sim\nros2 topic list\nros2 topic echo /joint_states\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Core Concepts"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"USD Composition"}),": Layering assets via references and sublayers for reusable scenes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PhysX Articulations"}),": GPU-accelerated physics for complex multi-body robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX Ray Tracing"}),": Photorealistic rendering enabling sim-to-real perception transfer"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Replicator"}),": Procedural data generation with automatic ground truth labeling"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsx)(n.p,{children:"Official Documentation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/index.html",children:"NVIDIA Isaac Sim Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://graphics.pixar.com/usd/release/intro.html",children:"USD Introduction by Pixar"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html",children:"Omniverse Replicator"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Tutorials and Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/tutorials/index.html",children:"Isaac Sim Tutorials (NVIDIA)"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/ros2_tutorials/tutorial_ros2_navigation.html",children:"ROS 2 Bridge Setup Guide"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/replicator_tutorials/index.html",children:"Synthetic Data Generation Pipeline"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Research Papers:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Sim-to-Real Transfer in Robotics: A Survey" (Zhao et al., 2020) - Domain randomization techniques'}),"\n",(0,r.jsx)(n.li,{children:'"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World" (Tobin et al., 2017)'}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"You're now ready to move on to the next chapter!"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-textbook/docs/module3-isaac/ch2-isaac-ros-perception",children:"Chapter 3.2: Isaac ROS Perception"})]}),"\n",(0,r.jsx)(n.p,{children:"In the next chapter, you'll learn about Isaac ROS GEMs (GPU-accelerated perception nodes), implement VSLAM for localization, and build complete perception pipelines for humanoid navigation."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optional Practice"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explore the NVIDIA Assets library in Omniverse (hundreds of free 3D models)"}),"\n",(0,r.jsx)(n.li,{children:"Try importing a custom URDF from your research or favorite open-source robot"}),"\n",(0,r.jsx)(n.li,{children:"Join the NVIDIA Omniverse Discord for community support and examples"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);