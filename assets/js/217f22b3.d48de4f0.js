"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[982],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},8931:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module3-isaac/ch2-isaac-ros-perception","title":"Isaac ROS Perception: GPU-Accelerated Vision Pipelines","description":"Learn Isaac ROS GEMs for GPU-accelerated perception including visual SLAM, stereo depth estimation, object detection, and DNN inference for real-time humanoid robot perception.","source":"@site/docs/module3-isaac/ch2-isaac-ros-perception.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/ch2-isaac-ros-perception","permalink":"/physical-ai-textbook/docs/module3-isaac/ch2-isaac-ros-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/salmansiddiqui-99/physical-ai-textbook/tree/main/docs/module3-isaac/ch2-isaac-ros-perception.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"id":"ch2-isaac-ros-perception","title":"Isaac ROS Perception: GPU-Accelerated Vision Pipelines","sidebar_label":"Isaac ROS Perception","sidebar_position":8,"description":"Learn Isaac ROS GEMs for GPU-accelerated perception including visual SLAM, stereo depth estimation, object detection, and DNN inference for real-time humanoid robot perception.","keywords":["Isaac ROS","VSLAM","cuVSLAM","stereo depth","DNN inference","GPU acceleration","perception pipeline","NITROS"],"prerequisites":["Completion of Module 1 (ROS 2 Foundations) and Module 2 (Simulation)","Chapter 3.1 (Isaac Sim Basics)","NVIDIA GPU with CUDA support (RTX 2060 or higher)","ROS 2 Humble installed"],"learning_objectives":["Explain Isaac ROS architecture and NITROS zero-copy acceleration framework","Implement visual SLAM using cuVSLAM for real-time localization","Configure stereo depth estimation pipelines with disparity and point clouds","Integrate DNN models for object detection and semantic segmentation on GPU"],"estimated_time":"90 minutes"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Basics","permalink":"/physical-ai-textbook/docs/module3-isaac/ch1-isaac-sim"},"next":{"title":"Humanoid Navigation","permalink":"/physical-ai-textbook/docs/module3-isaac/ch3-navigation-humanoids"}}');var r=i(4848),a=i(8453);const o={id:"ch2-isaac-ros-perception",title:"Isaac ROS Perception: GPU-Accelerated Vision Pipelines",sidebar_label:"Isaac ROS Perception",sidebar_position:8,description:"Learn Isaac ROS GEMs for GPU-accelerated perception including visual SLAM, stereo depth estimation, object detection, and DNN inference for real-time humanoid robot perception.",keywords:["Isaac ROS","VSLAM","cuVSLAM","stereo depth","DNN inference","GPU acceleration","perception pipeline","NITROS"],prerequisites:["Completion of Module 1 (ROS 2 Foundations) and Module 2 (Simulation)","Chapter 3.1 (Isaac Sim Basics)","NVIDIA GPU with CUDA support (RTX 2060 or higher)","ROS 2 Humble installed"],learning_objectives:["Explain Isaac ROS architecture and NITROS zero-copy acceleration framework","Implement visual SLAM using cuVSLAM for real-time localization","Configure stereo depth estimation pipelines with disparity and point clouds","Integrate DNN models for object detection and semantic segmentation on GPU"],estimated_time:"90 minutes"},t="Isaac ROS Perception: GPU-Accelerated Vision Pipelines",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Section 1: Isaac ROS Architecture and NITROS",id:"section-1-isaac-ros-architecture-and-nitros",level:2},{value:"Subsection 1.1: What is Isaac ROS?",id:"subsection-11-what-is-isaac-ros",level:3},{value:"Subsection 1.2: NITROS: Zero-Copy Transport",id:"subsection-12-nitros-zero-copy-transport",level:3},{value:"Subsection 1.3: Installation",id:"subsection-13-installation",level:3},{value:"Section 2: Visual SLAM with cuVSLAM",id:"section-2-visual-slam-with-cuvslam",level:2},{value:"Subsection 2.1: What is Visual SLAM?",id:"subsection-21-what-is-visual-slam",level:3},{value:"Subsection 2.2: Running cuVSLAM with Stereo Camera",id:"subsection-22-running-cuvslam-with-stereo-camera",level:3},{value:"Subsection 2.3: Visualizing SLAM Output",id:"subsection-23-visualizing-slam-output",level:3},{value:"Subsection 2.4: Integrating with Isaac Sim",id:"subsection-24-integrating-with-isaac-sim",level:3},{value:"Section 3: Stereo Depth Estimation",id:"section-3-stereo-depth-estimation",level:2},{value:"Subsection 3.1: Stereo Vision Fundamentals",id:"subsection-31-stereo-vision-fundamentals",level:3},{value:"Subsection 3.2: Isaac ROS Stereo Pipeline",id:"subsection-32-isaac-ros-stereo-pipeline",level:3},{value:"Subsection 3.3: Generating Point Clouds",id:"subsection-33-generating-point-clouds",level:3},{value:"Section 4: DNN Inference with TensorRT",id:"section-4-dnn-inference-with-tensorrt",level:2},{value:"Subsection 4.1: TensorRT Acceleration",id:"subsection-41-tensorrt-acceleration",level:3},{value:"Subsection 4.2: Object Detection with YOLO",id:"subsection-42-object-detection-with-yolo",level:3},{value:"Subsection 4.3: Semantic Segmentation",id:"subsection-43-semantic-segmentation",level:3},{value:"Section 5: Complete Perception Pipeline",id:"section-5-complete-perception-pipeline",level:2},{value:"Subsection 5.1: Integrating cuVSLAM + Stereo + DNN",id:"subsection-51-integrating-cuvslam--stereo--dnn",level:3},{value:"Subsection 5.2: Performance Profiling",id:"subsection-52-performance-profiling",level:3},{value:"Hands-On Project: Humanoid Navigation with Perception",id:"hands-on-project-humanoid-navigation-with-perception",level:2},{value:"Step 1: Launch Isaac Sim with Humanoid and Stereo Camera",id:"step-1-launch-isaac-sim-with-humanoid-and-stereo-camera",level:3},{value:"Step 2: Create Multi-Sensor Launch File",id:"step-2-create-multi-sensor-launch-file",level:3},{value:"Step 3: Configure RViz for Multi-Sensor Visualization",id:"step-3-configure-rviz-for-multi-sensor-visualization",level:3},{value:"Step 4: Run and Verify",id:"step-4-run-and-verify",level:3},{value:"Challenge: Test Your Understanding",id:"challenge-test-your-understanding",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"isaac-ros-perception-gpu-accelerated-vision-pipelines",children:"Isaac ROS Perception: GPU-Accelerated Vision Pipelines"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain Isaac ROS GEMs architecture and how NITROS enables zero-copy GPU acceleration"}),"\n",(0,r.jsx)(n.li,{children:"Implement cuVSLAM (visual SLAM) for real-time localization and mapping"}),"\n",(0,r.jsx)(n.li,{children:"Configure stereo depth pipelines to generate dense point clouds for obstacle avoidance"}),"\n",(0,r.jsx)(n.li,{children:"Integrate TensorRT-accelerated DNNs for object detection and semantic segmentation"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this chapter, you should:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Have completed ",(0,r.jsx)(n.a,{href:"/physical-ai-textbook/docs/module3-isaac/ch1-isaac-sim",children:"Chapter 3.1: Isaac Sim Basics"})]}),"\n",(0,r.jsxs)(n.li,{children:["Understand ROS 2 topics, nodes, and launch files from ",(0,r.jsx)(n.a,{href:"/physical-ai-textbook/docs/module1-ros2/ch1-ros2-basics",children:"Module 1"})]}),"\n",(0,r.jsx)(n.li,{children:"Have an NVIDIA GPU with CUDA support (RTX 2060+, Jetson Xavier/Orin for embedded)"}),"\n",(0,r.jsx)(n.li,{children:"Have ROS 2 Humble and Isaac ROS packages installed (installation guide below)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Traditional ROS 2 perception nodes (like ORB-SLAM3, OpenCV stereo matching) run on CPU, limiting throughput to 5-15 FPS for complex tasks. For humanoid robots navigating dynamic environments, this latency is unacceptable\u2014a robot moving at 1 m/s travels 20cm between 5 Hz perception updates, increasing collision risk."}),"\n",(0,r.jsxs)(n.p,{children:["NVIDIA Isaac ROS solves this with ",(0,r.jsx)(n.strong,{children:"GEMs"})," (GPU-Enabled Modules): drop-in ROS 2 nodes that offload perception to the GPU. Using ",(0,r.jsx)(n.strong,{children:"NITROS"})," (NVIDIA Isaac Transport for ROS), data flows directly between GPU-accelerated nodes without copying to CPU memory, achieving 10-100x speedups. For example, cuVSLAM runs visual SLAM at 60+ FPS on an RTX 3060, enabling real-time tracking for fast-moving humanoids."]}),"\n",(0,r.jsx)(n.p,{children:"This chapter covers the core Isaac ROS GEMs for humanoid perception: cuVSLAM for localization, stereo depth for 3D sensing, and TensorRT inference for AI-based object recognition. You'll build complete perception pipelines that run in Isaac Sim and transfer directly to physical robots with NVIDIA Jetson or discrete GPUs."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-1-isaac-ros-architecture-and-nitros",children:"Section 1: Isaac ROS Architecture and NITROS"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-11-what-is-isaac-ros",children:"Subsection 1.1: What is Isaac ROS?"}),"\n",(0,r.jsx)(n.p,{children:"Isaac ROS is a collection of hardware-accelerated ROS 2 packages for robotics perception, manipulation, and navigation."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Components"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"cuVSLAM"}),": Visual SLAM (Simultaneous Localization and Mapping)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo Depth"}),": Dense depth maps from stereo cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Image Processing"}),": Rectification, debayering, resize (all on GPU)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DNN Inference"}),": TensorRT-accelerated object detection, segmentation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AprilTag"}),": Fiducial marker detection for precise localization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NITROS"}),": Zero-copy transport layer for GPU data"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why GPU Acceleration Matters"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Task"}),(0,r.jsx)(n.th,{children:"CPU (8-core i7)"}),(0,r.jsx)(n.th,{children:"GPU (RTX 3060)"}),(0,r.jsx)(n.th,{children:"Speedup"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Stereo depth matching (1280x720)"}),(0,r.jsx)(n.td,{children:"12 FPS"}),(0,r.jsx)(n.td,{children:"120 FPS"}),(0,r.jsx)(n.td,{children:"10x"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLO object detection"}),(0,r.jsx)(n.td,{children:"8 FPS"}),(0,r.jsx)(n.td,{children:"60 FPS"}),(0,r.jsx)(n.td,{children:"7.5x"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Visual SLAM (cuVSLAM)"}),(0,r.jsx)(n.td,{children:"N/A (requires GPU)"}),(0,r.jsx)(n.td,{children:"60+ FPS"}),(0,r.jsx)(n.td,{children:"\u221e"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Semantic segmentation (DeepLabV3)"}),(0,r.jsx)(n.td,{children:"3 FPS"}),(0,r.jsx)(n.td,{children:"45 FPS"}),(0,r.jsx)(n.td,{children:"15x"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-12-nitros-zero-copy-transport",children:"Subsection 1.2: NITROS: Zero-Copy Transport"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem with Standard ROS 2"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Node A (GPU) processes image \u2192 copies result to CPU RAM"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 middleware serializes data and sends via DDS"}),"\n",(0,r.jsx)(n.li,{children:"Node B receives \u2192 deserializes \u2192 copies to GPU RAM"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"NITROS Solution"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Node A processes image \u2192 keeps data in GPU RAM"}),"\n",(0,r.jsx)(n.li,{children:"NITROS passes GPU memory pointer (no copy!)"}),"\n",(0,r.jsx)(n.li,{children:"Node B receives pointer \u2192 processes directly on GPU"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": 50-80% latency reduction, 3-5x throughput increase."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"NITROS Message Types"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"nitros_bridge/NitrosImage"}),": GPU-resident images"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"nitros_bridge/NitrosDisparity"}),": Stereo disparity maps"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"nitros_bridge/NitrosCameraInfo"}),": Camera calibration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"nitros_bridge/NitrosPointCloud"}),": 3D point clouds"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concept"}),": Standard ROS 2 nodes can subscribe to NITROS topics (NITROS auto-converts to CPU), but for best performance, chain NITROS-enabled nodes together."]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-13-installation",children:"Subsection 1.3: Installation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prerequisites"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Ubuntu 22.04 LTS"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA GPU with CUDA 11.8+ (or Jetson with JetPack 5.1+)"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install Isaac ROS"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install NVIDIA Container Toolkit (for Docker-based workflows)\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update\nsudo apt-get install -y nvidia-docker2\nsudo systemctl restart docker\n\n# Test GPU access in Docker\ndocker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\n\n# Clone Isaac ROS common repository\ncd ~/ros2_ws/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\ncd isaac_ros_common\ngit checkout main\n\n# Run Isaac ROS Docker container (includes all dependencies)\n./scripts/run_dev.sh\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install Specific Isaac ROS Packages"})," (inside container):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Inside Isaac ROS Docker container\ncd /workspaces/isaac_ros-dev/src\n\n# cuVSLAM\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\n\n# Stereo depth\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_image_pipeline.git\n\n# DNN inference\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_dnn_inference.git\n\n# Build\ncd /workspaces/isaac_ros-dev\ncolcon build --symlink-install\nsource install/setup.bash\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Verify Installation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 pkg list | grep isaac_ros\n# Should show: isaac_ros_visual_slam, isaac_ros_stereo_image_proc, etc.\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-2-visual-slam-with-cuvslam",children:"Section 2: Visual SLAM with cuVSLAM"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-21-what-is-visual-slam",children:"Subsection 2.1: What is Visual SLAM?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SLAM (Simultaneous Localization and Mapping)"})," solves two problems simultaneously:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localization"}),': "Where am I?" (estimate robot pose)']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mapping"}),': "What does the environment look like?" (build 3D map)']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM (VSLAM)"})," uses cameras instead of LiDAR. Advantages for humanoids:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Lightweight sensors (cameras are smaller/cheaper than LiDAR)"}),"\n",(0,r.jsx)(n.li,{children:"Rich texture information (useful for object recognition)"}),"\n",(0,r.jsx)(n.li,{children:"Passive sensing (no spinning parts, silent operation)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"cuVSLAM Algorithm"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature Extraction"}),": Detect ORB keypoints in each frame (GPU-accelerated)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tracking"}),": Match keypoints between frames to estimate camera motion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mapping"}),": Triangulate 3D positions of landmarks (bundle adjustment)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop Closure"}),": Detect revisited locations to correct drift"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-22-running-cuvslam-with-stereo-camera",children:"Subsection 2.2: Running cuVSLAM with Stereo Camera"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hardware Setup"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Stereo camera (e.g., RealSense D455, ZED 2) or simulated stereo in Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Baseline (distance between cameras): 10-20cm for humanoid head mounting"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Launch cuVSLAM"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Source workspace\nsource /workspaces/isaac_ros-dev/install/setup.bash\n\n# Launch cuVSLAM node\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py \\\n  camera_info_topic:=/camera/left/camera_info \\\n  image_topic:=/camera/left/image_raw \\\n  depth_image_topic:=/camera/depth/image_raw \\\n  imu_topic:=/imu/data\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Configuration Parameters"})," (",(0,r.jsx)(n.code,{children:"isaac_ros_visual_slam/config/params.yaml"}),"):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"visual_slam:\n  ros__parameters:\n    # Enable IMU fusion for better pose estimation\n    enable_imu_fusion: true\n    gyro_noise_density: 0.000244  # rad/s/\u221aHz (sensor-dependent)\n    gyro_random_walk: 0.000019    # rad/s\xb2/\u221aHz\n    accel_noise_density: 0.001    # m/s\xb2/\u221aHz\n    accel_random_walk: 0.0002     # m/s\xb3/\u221aHz\n\n    # Map management\n    enable_localization_n_mapping: true\n    map_frame: 'map'\n    odom_frame: 'odom'\n    base_frame: 'base_link'\n\n    # Performance tuning\n    num_cameras: 1  # Use 2 for stereo\n    rectified_images: true  # Already rectified by image pipeline\n    enable_slam_visualization: true  # Publish landmarks for RViz\n    enable_observations_view: true  # Debug feature matching\n\n    # Loop closure\n    enable_loop_closure: true\n    min_loop_closure_score: 0.15  # Lower = more aggressive closure\n"})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-23-visualizing-slam-output",children:"Subsection 2.3: Visualizing SLAM Output"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Published Topics"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 topic list\n# /visual_slam/tracking/odometry         # Pose estimate (nav_msgs/Odometry)\n# /visual_slam/tracking/vo_pose          # Visual odometry pose\n# /visual_slam/tracking/slam_path        # Full trajectory path\n# /visual_slam/status                    # SLAM status (tracking, lost, etc.)\n# /visual_slam/vis/landmarks_cloud       # 3D map points (for RViz)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visualize in RViz2"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run rviz2 rviz2\n"})}),"\n",(0,r.jsx)(n.p,{children:"Add these displays:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Odometry"})," \u2192 Topic: ",(0,r.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Path"})," \u2192 Topic: ",(0,r.jsx)(n.code,{children:"/visual_slam/tracking/slam_path"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PointCloud2"})," \u2192 Topic: ",(0,r.jsx)(n.code,{children:"/visual_slam/vis/landmarks_cloud"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TF"})," \u2192 Shows ",(0,r.jsx)(n.code,{children:"map"}),", ",(0,r.jsx)(n.code,{children:"odom"}),", ",(0,r.jsx)(n.code,{children:"base_link"})," frames"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You should see the robot trajectory as a colored path and map points as a 3D cloud."}),"\n",(0,r.jsx)(n.h3,{id:"subsection-24-integrating-with-isaac-sim",children:"Subsection 2.4: Integrating with Isaac Sim"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Run cuVSLAM with Isaac Sim Camera"}),":"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 1"}),": Launch Isaac Sim with humanoid and stereo camera (see Chapter 3.1)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 2"}),": Create launch file for cuVSLAM:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: isaac_sim_vslam.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # cuVSLAM node\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='isaac_ros_visual_slam',\n            name='visual_slam',\n            parameters=[{\n                'enable_imu_fusion': True,\n                'num_cameras': 1,\n                'rectified_images': True,\n                'enable_localization_n_mapping': True\n            }],\n            remappings=[\n                ('/camera/image_raw', '/humanoid/head_camera/left/image_raw'),\n                ('/camera/camera_info', '/humanoid/head_camera/left/camera_info'),\n                ('/imu/data', '/humanoid/imu/data')\n            ]\n        ),\n\n        # RViz for visualization\n        Node(\n            package='rviz2',\n            executable='rviz2',\n            name='rviz2',\n            arguments=['-d', 'isaac_slam.rviz']\n        )\n    ])\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 3"}),": Launch and verify:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_sim_vslam.launch.py\n\n# In Isaac Sim, move the robot (teleop or scripted path)\n# You should see SLAM tracking in RViz with path visualization\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-3-stereo-depth-estimation",children:"Section 3: Stereo Depth Estimation"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-31-stereo-vision-fundamentals",children:"Subsection 3.1: Stereo Vision Fundamentals"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Principle"}),": Two cameras separated by baseline ",(0,r.jsx)(n.em,{children:"B"})," capture the same scene. Objects appear shifted (disparity ",(0,r.jsx)(n.em,{children:"d"}),") between left/right images. Depth ",(0,r.jsx)(n.em,{children:"Z"})," is computed as:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Z = (f * B) / d\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"f"}),": Focal length (pixels)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"B"}),": Baseline (meters, distance between camera centers)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"d"}),": Disparity (pixels, shift between left/right image points)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Baseline = 0.12m (12cm)"}),"\n",(0,r.jsx)(n.li,{children:"Focal length = 400 pixels"}),"\n",(0,r.jsx)(n.li,{children:"Disparity = 50 pixels"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Depth = (400 * 0.12) / 50 = 0.96 meters"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-32-isaac-ros-stereo-pipeline",children:"Subsection 3.2: Isaac ROS Stereo Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Nodes in Pipeline"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rectify"}),": Align left/right images to same plane (removes lens distortion)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo Matching"}),": Compute disparity map (GPU-accelerated SGM algorithm)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Point Cloud"}),": Convert disparity to 3D points"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Launch Stereo Pipeline"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_ros_stereo_image_proc isaac_ros_stereo_image_pipeline.launch.py \\\n  left_image_topic:=/camera/left/image_raw \\\n  left_camera_info_topic:=/camera/left/camera_info \\\n  right_image_topic:=/camera/right/image_raw \\\n  right_camera_info_topic:=/camera/right/camera_info\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Configuration"})," (",(0,r.jsx)(n.code,{children:"stereo_pipeline_params.yaml"}),"):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"stereo_image_proc:\n  ros__parameters:\n    # Stereo matching algorithm parameters\n    use_color: false  # Use grayscale for matching (faster)\n\n    # Disparity range (inversely related to depth range)\n    min_disparity: 0  # Max depth = infinity\n    max_disparity: 128  # Min depth = (f*B)/128\n\n    # Smoothness vs. accuracy trade-off\n    p1: 8  # Penalty for small disparity changes (lower = smoother)\n    p2: 109  # Penalty for large disparity changes\n\n    # Matching window size\n    window_size: 5  # Larger = more robust but less detail\n\n    # Post-filtering\n    uniqueness_ratio: 15  # Reject ambiguous matches (%)\n    disp12_max_diff: 1  # Left-right consistency check\n"})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-33-generating-point-clouds",children:"Subsection 3.3: Generating Point Clouds"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Published Topics After Pipeline"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# /stereo/disparity         # Disparity image (stereo_msgs/DisparityImage)\n# /stereo/points2           # 3D point cloud (sensor_msgs/PointCloud2)\n# /stereo/depth             # Depth image (sensor_msgs/Image, float32)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visualize Point Cloud in RViz"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run rviz2 rviz2\n\n# Add PointCloud2 display\n# Topic: /stereo/points2\n# Fixed Frame: camera_link\n# You should see dense 3D reconstruction of the scene\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Accessing Point Cloud Data Programmatically"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2\nimport sensor_msgs_py.point_cloud2 as pc2\n\nclass PointCloudProcessor(Node):\n    def __init__(self):\n        super().__init__(\'point_cloud_processor\')\n        self.subscription = self.create_subscription(\n            PointCloud2,\n            \'/stereo/points2\',\n            self.point_cloud_callback,\n            10\n        )\n\n    def point_cloud_callback(self, msg):\n        # Extract XYZ points from point cloud\n        points = pc2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True)\n\n        count = 0\n        close_obstacles = 0\n        for p in points:\n            x, y, z = p\n            count += 1\n\n            # Count points within 1 meter (obstacle detection)\n            if z < 1.0 and abs(x) < 0.5:  # 1m ahead, \xb10.5m left/right\n                close_obstacles += 1\n\n        if count > 0:\n            obstacle_ratio = close_obstacles / count\n            self.get_logger().info(\n                f"Point cloud: {count} points, {obstacle_ratio*100:.1f}% obstacles within 1m"\n            )\n\nrclpy.init()\nnode = PointCloudProcessor()\nrclpy.spin(node)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-4-dnn-inference-with-tensorrt",children:"Section 4: DNN Inference with TensorRT"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-41-tensorrt-acceleration",children:"Subsection 4.1: TensorRT Acceleration"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"TensorRT"})," is NVIDIA's deep learning inference optimizer. It converts trained models (PyTorch, TensorFlow) to highly optimized GPU engines."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optimizations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Layer Fusion"}),": Combine Conv + BatchNorm + ReLU into single kernel"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Precision Calibration"}),": Use FP16 or INT8 instead of FP32 (2-4x speedup)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kernel Autotuning"}),": Profile and select fastest CUDA kernels per layer"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Typical Speedups"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"YOLOv5 (640x640): 80 FPS (TensorRT FP16) vs. 20 FPS (PyTorch FP32)"}),"\n",(0,r.jsx)(n.li,{children:"ResNet50: 500 FPS vs. 150 FPS"}),"\n",(0,r.jsx)(n.li,{children:"SegFormer: 35 FPS vs. 8 FPS"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"subsection-42-object-detection-with-yolo",children:"Subsection 4.2: Object Detection with YOLO"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS DNN Inference"})," supports TensorRT-optimized models for object detection."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 1: Convert YOLO Model to TensorRT"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Export PyTorch model to ONNX\npython export.py --weights yolov5s.pt --include onnx\n\n# Convert ONNX to TensorRT engine (FP16 precision)\n/usr/src/tensorrt/bin/trtexec \\\n  --onnx=yolov5s.onnx \\\n  --saveEngine=yolov5s_fp16.engine \\\n  --fp16 \\\n  --workspace=4096  # 4GB workspace for layer optimization\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 2: Launch Isaac ROS DNN Inference Node"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: yolo_detection.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='isaac_ros_dnn_inference',\n            executable='isaac_ros_dnn_inference',\n            name='yolo_inference',\n            parameters=[{\n                'model_file_path': '/path/to/yolov5s_fp16.engine',\n                'engine_file_path': '/path/to/yolov5s_fp16.engine',\n                'input_tensor_names': ['images'],\n                'input_binding_names': ['images'],\n                'output_tensor_names': ['output'],\n                'output_binding_names': ['output'],\n                'verbose': True,\n                'force_engine_update': False\n            }],\n            remappings=[\n                ('/tensor_pub', '/yolo/detections'),\n                ('/image', '/humanoid/camera/image_raw')\n            ]\n        ),\n\n        # Decoder node (converts raw tensors to bounding boxes)\n        Node(\n            package='isaac_ros_yolov5',\n            executable='yolov5_decoder_node',\n            name='yolo_decoder',\n            parameters=[{\n                'confidence_threshold': 0.5,\n                'nms_threshold': 0.4\n            }],\n            remappings=[\n                ('/detections', '/yolo/bounding_boxes')\n            ]\n        )\n    ])\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 3: Visualize Detections"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Subscribe to bounding boxes\nros2 topic echo /yolo/bounding_boxes\n\n# Or use image_view with overlays\nros2 run image_view image_view --ros-args \\\n  --remap /image:=/yolo/image_with_boxes\n"})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-43-semantic-segmentation",children:"Subsection 4.3: Semantic Segmentation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Semantic Segmentation"}),' labels every pixel with a class (e.g., "floor", "wall", "person").']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example: Run SegFormer for Scene Understanding"}),":"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 1"}),": Convert SegFormer to TensorRT (similar to YOLO)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 2"}),": Launch segmentation node:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_ros_dnn_inference segformer_inference.launch.py \\\n  model_path:=/path/to/segformer_b2_fp16.engine \\\n  image_topic:=/humanoid/camera/image_raw \\\n  output_topic:=/segmentation/mask\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 3"}),": Process segmentation mask:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nimport numpy as np\n\nclass SegmentationProcessor(Node):\n    def __init__(self):\n        super().__init__('segmentation_processor')\n        self.subscription = self.create_subscription(\n            Image,\n            '/segmentation/mask',\n            self.mask_callback,\n            10\n        )\n\n        # Class labels (example for indoor scenes)\n        self.classes = {\n            0: 'floor', 1: 'wall', 2: 'furniture',\n            3: 'door', 4: 'person', 5: 'object'\n        }\n\n    def mask_callback(self, msg):\n        # Convert ROS Image to numpy array\n        mask = np.frombuffer(msg.data, dtype=np.uint8).reshape(msg.height, msg.width)\n\n        # Count pixels per class\n        unique, counts = np.unique(mask, return_counts=True)\n        total_pixels = mask.size\n\n        for class_id, count in zip(unique, counts):\n            class_name = self.classes.get(class_id, 'unknown')\n            percentage = (count / total_pixels) * 100\n            self.get_logger().info(f\"{class_name}: {percentage:.1f}%\")\n\nrclpy.init()\nnode = SegmentationProcessor()\nrclpy.spin(node)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use Cases for Humanoids"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Floor detection"}),": Identify safe walking surfaces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Obstacle classification"}),": Distinguish between static furniture and dynamic people"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Door/opening detection"}),": Plan navigation through doorways"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"section-5-complete-perception-pipeline",children:"Section 5: Complete Perception Pipeline"}),"\n",(0,r.jsx)(n.h3,{id:"subsection-51-integrating-cuvslam--stereo--dnn",children:"Subsection 5.1: Integrating cuVSLAM + Stereo + DNN"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Isaac Sim Cameras \u2192 cuVSLAM (localization)\n                  \u2198\n                   Stereo Depth (obstacle avoidance)\n                  \u2198\n                   YOLO Detection (object recognition)\n                  \u2198\n                   Navigation Stack (path planning)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Launch File for Complete Pipeline"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: humanoid_perception_pipeline.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # 1. cuVSLAM for localization\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='isaac_ros_visual_slam',\n            name='visual_slam',\n            parameters=[{'enable_imu_fusion': True}],\n            remappings=[\n                ('/camera/image_raw', '/humanoid/camera/left/image_raw'),\n                ('/camera/camera_info', '/humanoid/camera/left/camera_info')\n            ]\n        ),\n\n        # 2. Stereo depth for 3D sensing\n        Node(\n            package='isaac_ros_stereo_image_proc',\n            executable='stereo_image_proc',\n            name='stereo_proc',\n            remappings=[\n                ('/left/image_raw', '/humanoid/camera/left/image_raw'),\n                ('/right/image_raw', '/humanoid/camera/right/image_raw')\n            ]\n        ),\n\n        # 3. YOLO object detection\n        Node(\n            package='isaac_ros_dnn_inference',\n            executable='isaac_ros_dnn_inference',\n            name='yolo_inference',\n            parameters=[{'model_file_path': '/models/yolov5s_fp16.engine'}],\n            remappings=[('/image', '/humanoid/camera/left/image_raw')]\n        ),\n\n        # 4. RViz for visualization\n        Node(\n            package='rviz2',\n            executable='rviz2',\n            name='rviz2',\n            arguments=['-d', 'perception_pipeline.rviz']\n        )\n    ])\n"})}),"\n",(0,r.jsx)(n.h3,{id:"subsection-52-performance-profiling",children:"Subsection 5.2: Performance Profiling"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Measure Pipeline Latency"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check node computation time\nros2 run ros2_performance_test latency_tester \\\n  --topic /humanoid/camera/left/image_raw \\\n  --output-topic /yolo/bounding_boxes\n\n# Typical results on RTX 3060:\n# Camera capture \u2192 YOLO output: 25-30ms (33 FPS)\n# Camera capture \u2192 Stereo depth: 15ms (66 FPS)\n# Camera capture \u2192 SLAM pose: 18ms (55 FPS)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optimization Tips"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use NITROS throughout"}),": Avoid CPU conversions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reduce image resolution"}),": 1280x720 instead of 1920x1080 (4x faster)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lower DNN precision"}),": FP16 or INT8 (2-4x faster, minimal accuracy loss)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Disable unused outputs"}),": Don't publish debug visualizations in production"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-project-humanoid-navigation-with-perception",children:"Hands-On Project: Humanoid Navigation with Perception"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Build a perception pipeline where a humanoid localizes with VSLAM, detects obstacles with stereo, and recognizes objects with YOLO."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 45 minutes"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate multiple Isaac ROS nodes into a single pipeline"}),"\n",(0,r.jsx)(n.li,{children:"Tune stereo depth parameters for humanoid height"}),"\n",(0,r.jsx)(n.li,{children:"Combine SLAM and object detection for semantic mapping"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-launch-isaac-sim-with-humanoid-and-stereo-camera",children:"Step 1: Launch Isaac Sim with Humanoid and Stereo Camera"}),"\n",(0,r.jsx)(n.p,{children:"(Use setup from Chapter 3.1, ensure stereo camera is attached to robot head)"}),"\n",(0,r.jsx)(n.h3,{id:"step-2-create-multi-sensor-launch-file",children:"Step 2: Create Multi-Sensor Launch File"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: humanoid_multi_sensor.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\ndef generate_launch_description():\n    container = ComposableNodeContainer(\n        name='perception_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container',\n        composable_node_descriptions=[\n            # cuVSLAM\n            ComposableNode(\n                package='isaac_ros_visual_slam',\n                plugin='nvidia::isaac_ros::visual_slam::VisualSlamNode',\n                name='visual_slam',\n                parameters=[{'enable_imu_fusion': True}]\n            ),\n            # Stereo depth\n            ComposableNode(\n                package='isaac_ros_stereo_image_proc',\n                plugin='nvidia::isaac_ros::stereo_image_proc::StereoImageProcNode',\n                name='stereo_proc'\n            ),\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription([container])\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-configure-rviz-for-multi-sensor-visualization",children:"Step 3: Configure RViz for Multi-Sensor Visualization"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"perception_pipeline.rviz"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"Panels:\n  - Class: rviz_common/Displays\n    Displays:\n      - Class: rviz_default_plugins/TF\n      - Class: rviz_default_plugins/Path\n        Topic: /visual_slam/tracking/slam_path\n      - Class: rviz_default_plugins/PointCloud2\n        Topic: /stereo/points2\n      - Class: rviz_default_plugins/Image\n        Topic: /yolo/image_with_boxes\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-run-and-verify",children:"Step 4: Run and Verify"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch Isaac Sim with humanoid\n# (From Chapter 3.1)\n\n# Terminal 2: Launch perception pipeline\nsource /workspaces/isaac_ros-dev/install/setup.bash\nros2 launch humanoid_multi_sensor.launch.py\n\n# Terminal 3: Teleop robot to move around\nros2 run teleop_twist_keyboard teleop_twist_keyboard\n\n# Observe in RViz:\n# - SLAM path (blue line showing robot trajectory)\n# - Point cloud (3D obstacles)\n# - Object detection boxes (if objects in scene)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"challenge-test-your-understanding",children:"Challenge: Test Your Understanding"}),"\n",(0,r.jsx)(n.p,{children:"Try these exercises to reinforce your learning:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Basic"}),": Modify stereo depth parameters to change detection range (min/max depth)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Hint"}),": Adjust ",(0,r.jsx)(n.code,{children:"max_disparity"})," to control minimum depth threshold"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Intermediate"}),': Create a ROS 2 node that subscribes to SLAM pose and stereo point cloud, publishes "obstacle ahead" warning if points detected within 1m']}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Hint"}),": Filter point cloud in base_link frame, count points in front cone"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advanced"}),": Integrate a custom-trained YOLO model to detect humanoid-specific objects (e.g., doorknobs, elevator buttons)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Hint"}),": Train YOLOv5 on custom dataset, convert to TensorRT, update DNN inference node config"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS architecture"}),": GPU-accelerated perception with NITROS zero-copy transport (10-100x speedups)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"cuVSLAM"}),": Visual SLAM for real-time localization at 60+ FPS using stereo cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo depth pipelines"}),": Dense 3D reconstruction for obstacle avoidance and navigation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TensorRT DNN inference"}),": Real-time object detection (YOLO) and semantic segmentation at 30-60 FPS"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Commands"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Launch cuVSLAM\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\n\n# Launch stereo depth pipeline\nros2 launch isaac_ros_stereo_image_proc isaac_ros_stereo_image_pipeline.launch.py\n\n# Check GPU utilization\nnvidia-smi\n\n# Monitor ROS 2 topics\nros2 topic hz /visual_slam/tracking/odometry\nros2 topic hz /stereo/points2\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Core Concepts"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NITROS"}),": Zero-copy GPU memory sharing between ROS 2 nodes (avoids CPU bottlenecks)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"cuVSLAM"}),": ORB feature tracking + bundle adjustment for drift-free localization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo Matching"}),": Disparity computation via Semi-Global Matching (SGM) on GPU"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TensorRT"}),": Optimized DNN inference with layer fusion and mixed precision"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsx)(n.p,{children:"Official Documentation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"Isaac ROS Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://developer.nvidia.com/isaac-vslam",children:"cuVSLAM Technical Details"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/ros/nitros/index.html",children:"NITROS Transport Whitepaper"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Tutorials and Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam",children:"Isaac ROS Tutorials (GitHub)"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_image_pipeline/isaac_ros_stereo_image_proc/index.html",children:"Stereo Depth with Isaac ROS"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/best-practices/index.html",children:"TensorRT Best Practices"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Research Papers:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM" (Campos et al., 2021)'}),"\n",(0,r.jsx)(n.li,{children:'"Semi-Global Matching for Stereo Vision" (Hirschmuller, 2008) - Algorithm used by Isaac ROS'}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"You're now ready to move on to the next chapter!"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-textbook/docs/module3-isaac/ch3-navigation-humanoids",children:"Chapter 3.3: Navigation for Humanoids"})]}),"\n",(0,r.jsx)(n.p,{children:"In the next chapter, you'll learn how to integrate Isaac ROS perception with the Nav2 navigation stack, implement bipedal-specific path planners, and deploy full autonomous navigation for humanoid robots."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optional Practice"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Benchmark cuVSLAM on your GPU and compare to CPU SLAM (ORB-SLAM3)"}),"\n",(0,r.jsx)(n.li,{children:"Collect a rosbag of stereo images in Isaac Sim, replay offline for testing"}),"\n",(0,r.jsx)(n.li,{children:"Train a custom object detector for your humanoid's task (e.g., detect cups for grasping)"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);