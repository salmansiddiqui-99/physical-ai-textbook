"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[804],{7744:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4-vla/ch2-cognitive-planning","title":"Cognitive Planning: LLM-Powered Task Decomposition","description":"Learn to use large language models (LLMs) to decompose high-level tasks into executable action sequences, enabling humanoid robots to understand and plan complex multi-step behaviors autonomously.","source":"@site/docs/module4-vla/ch2-cognitive-planning.md","sourceDirName":"module4-vla","slug":"/module4-vla/ch2-cognitive-planning","permalink":"/physical-ai-textbook/docs/module4-vla/ch2-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/salmansiddiqui-99/physical-ai-textbook/tree/main/docs/module4-vla/ch2-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"id":"ch2-cognitive-planning","title":"Cognitive Planning: LLM-Powered Task Decomposition","sidebar_label":"Cognitive Planning","sidebar_position":11,"description":"Learn to use large language models (LLMs) to decompose high-level tasks into executable action sequences, enabling humanoid robots to understand and plan complex multi-step behaviors autonomously.","keywords":["LLM","GPT-4","task planning","task decomposition","action graphs","cognitive architectures","RAG","few-shot prompting"],"prerequisites":["Chapter 4.1 (Voice-to-Action)","Python 3.10+ with OpenAI or Anthropic SDK","Understanding of ROS 2 actions and services"],"learning_objectives":["Explain how LLMs enable cognitive planning for humanoid robots","Implement task decomposition pipelines using GPT-4 or Claude","Create action graphs from natural language task descriptions","Integrate LLM planners with ROS 2 execution systems"],"estimated_time":"90 minutes"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action","permalink":"/physical-ai-textbook/docs/module4-vla/ch1-voice-to-action"},"next":{"title":"Capstone Project","permalink":"/physical-ai-textbook/docs/module4-vla/ch3-capstone-project"}}');var i=t(4848),a=t(8453);const s={id:"ch2-cognitive-planning",title:"Cognitive Planning: LLM-Powered Task Decomposition",sidebar_label:"Cognitive Planning",sidebar_position:11,description:"Learn to use large language models (LLMs) to decompose high-level tasks into executable action sequences, enabling humanoid robots to understand and plan complex multi-step behaviors autonomously.",keywords:["LLM","GPT-4","task planning","task decomposition","action graphs","cognitive architectures","RAG","few-shot prompting"],prerequisites:["Chapter 4.1 (Voice-to-Action)","Python 3.10+ with OpenAI or Anthropic SDK","Understanding of ROS 2 actions and services"],learning_objectives:["Explain how LLMs enable cognitive planning for humanoid robots","Implement task decomposition pipelines using GPT-4 or Claude","Create action graphs from natural language task descriptions","Integrate LLM planners with ROS 2 execution systems"],estimated_time:"90 minutes"},r="Cognitive Planning: LLM-Powered Task Decomposition",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Section 1: LLMs for Robot Task Planning",id:"section-1-llms-for-robot-task-planning",level:2},{value:"Subsection 1.1: Why Use LLMs for Planning?",id:"subsection-11-why-use-llms-for-planning",level:3},{value:"Subsection 1.2: LLM API Setup",id:"subsection-12-llm-api-setup",level:3},{value:"Section 2: Task Decomposition with Few-Shot Prompting",id:"section-2-task-decomposition-with-few-shot-prompting",level:2},{value:"Subsection 2.1: Prompt Engineering Basics",id:"subsection-21-prompt-engineering-basics",level:3},{value:"Subsection 2.2: Implementing LLM Task Planner",id:"subsection-22-implementing-llm-task-planner",level:3},{value:"Section 3: Action Graph Execution",id:"section-3-action-graph-execution",level:2},{value:"Subsection 3.1: Converting Plan to ROS 2 Actions",id:"subsection-31-converting-plan-to-ros-2-actions",level:3},{value:"Section 4: Integrating LLM Planning with Voice Control",id:"section-4-integrating-llm-planning-with-voice-control",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-llm-powered-task-decomposition",children:"Cognitive Planning: LLM-Powered Task Decomposition"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use LLMs (GPT-4, Claude) to decompose complex tasks into action sequences"}),"\n",(0,i.jsx)(e.li,{children:"Implement few-shot prompting for robust task planning"}),"\n",(0,i.jsx)(e.li,{children:"Create executable action graphs from LLM outputs"}),"\n",(0,i.jsx)(e.li,{children:"Integrate cognitive planning with ROS 2 humanoid control systems"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(e.p,{children:"Before starting this chapter, you should:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["Have completed ",(0,i.jsx)(e.a,{href:"/physical-ai-textbook/docs/module4-vla/ch1-voice-to-action",children:"Chapter 4.1: Voice-to-Action"})]}),"\n",(0,i.jsxs)(e.li,{children:["Understand ROS 2 actions from ",(0,i.jsx)(e.a,{href:"/physical-ai-textbook/docs/module1-ros2/ch2-ros2-humanoids",children:"Module 1"})]}),"\n",(0,i.jsx)(e.li,{children:"Have OpenAI API key or Anthropic API key for LLM access"}),"\n",(0,i.jsx)(e.li,{children:"Familiarity with JSON and structured data formats"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(e.p,{children:['Traditional robot task planning uses symbolic AI: domain-specific languages (PDDL), finite state machines, or behavior trees. These approaches require expert knowledge to encode every scenario. When a user says "clean the living room", a classical planner needs pre-programmed rules defining "clean" as a sequence like: ',(0,i.jsx)(e.code,{children:"navigate_to(living_room) \u2192 detect_objects(trash) \u2192 grasp(trash) \u2192 navigate_to(bin) \u2192 release(trash)"}),"."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Large Language Models (LLMs)"}),' like GPT-4 and Claude change this paradigm. Trained on vast amounts of human knowledge, LLMs understand common-sense reasoning: "cleaning" involves picking up clutter, vacuuming floors, and organizing items. By prompting an LLM with a task description and available robot actions, we can generate executable plans ',(0,i.jsx)(e.strong,{children:"without handcrafted rules"}),"."]}),"\n",(0,i.jsxs)(e.p,{children:["This chapter covers ",(0,i.jsx)(e.strong,{children:"cognitive planning"}),' for humanoid robots: using LLMs to transform high-level goals ("prepare dinner") into action graphs (',(0,i.jsx)(e.code,{children:"navigate \u2192 open_fridge \u2192 grasp_vegetables \u2192 close_fridge \u2192 navigate_kitchen \u2192 place_on_counter \u2192 ..."}),"). You'll learn prompt engineering techniques, action graph execution, and error recovery strategies for real-world deployment."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-1-llms-for-robot-task-planning",children:"Section 1: LLMs for Robot Task Planning"}),"\n",(0,i.jsx)(e.h3,{id:"subsection-11-why-use-llms-for-planning",children:"Subsection 1.1: Why Use LLMs for Planning?"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Advantages"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Common-sense reasoning"}),': Understands implicit knowledge (e.g., "set the table" requires plates, utensils)']}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural language interface"}),": Users describe tasks conversationally"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Few-shot learning"}),": Provide 2-3 examples, LLM generalizes to new tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Adaptability"}),": No need to reprogram for new scenarios"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Limitations"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Hallucinations"}),': May generate invalid actions ("fly to ceiling")']}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Non-deterministic"}),": Slight prompt changes alter plans"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Latency"}),": API calls take 1-3 seconds (not real-time)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cost"}),": GPT-4 costs $0.03 per 1K tokens (~$1-2 per hour of planning)"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Best Practices"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Validate outputs"}),": Check generated actions against available action set"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Use structured output"}),": JSON format for parsing"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Iterative refinement"}),": Re-prompt with error feedback if plan fails"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"subsection-12-llm-api-setup",children:"Subsection 1.2: LLM API Setup"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"OpenAI GPT-4"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'pip install openai\n\nexport OPENAI_API_KEY="sk-..."  # Your API key from platform.openai.com\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Anthropic Claude"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'pip install anthropic\n\nexport ANTHROPIC_API_KEY="sk-ant-..."  # Your API key from console.anthropic.com\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Example: Test API Connection"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import openai\n\nopenai.api_key = "sk-..."\n\nresponse = openai.ChatCompletion.create(\n    model="gpt-4",\n    messages=[{"role": "user", "content": "Say \'API connected!\'"}]\n)\n\nprint(response.choices[0].message.content)\n# Output: API connected!\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-2-task-decomposition-with-few-shot-prompting",children:"Section 2: Task Decomposition with Few-Shot Prompting"}),"\n",(0,i.jsx)(e.h3,{id:"subsection-21-prompt-engineering-basics",children:"Subsection 2.1: Prompt Engineering Basics"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Effective Prompt Structure"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"System prompt"}),": Define role and output format"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Few-shot examples"}),": Show input/output pairs"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task description"}),": User's high-level goal"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Available actions"}),": List of robot capabilities"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Example Prompt Template"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'You are a task planning AI for a humanoid robot.\n\nGiven a high-level task, decompose it into a sequence of primitive actions.\n\nAvailable actions:\n- navigate_to(location)\n- detect_objects(object_type)\n- grasp(object_id)\n- place(location)\n- open(container)\n- close(container)\n\nExamples:\n\nTask: "Bring me a water bottle"\nPlan:\n[\n  {"action": "navigate_to", "params": {"location": "kitchen"}},\n  {"action": "detect_objects", "params": {"object_type": "bottle"}},\n  {"action": "grasp", "params": {"object_id": "bottle_0"}},\n  {"action": "navigate_to", "params": {"location": "user"}},\n  {"action": "place", "params": {"location": "user_hand"}}\n]\n\nTask: "Clean the table"\nPlan:\n[\n  {"action": "navigate_to", "params": {"location": "dining_table"}},\n  {"action": "detect_objects", "params": {"object_type": "clutter"}},\n  {"action": "grasp", "params": {"object_id": "clutter_0"}},\n  {"action": "navigate_to", "params": {"location": "trash_bin"}},\n  {"action": "place", "params": {"location": "trash_bin"}},\n  {"action": "navigate_to", "params": {"location": "dining_table"}},\n  {"action": "detect_objects", "params": {"object_type": "clutter"}},\n  ...\n]\n\nNow decompose this task:\n\nTask: "{user_task}"\nPlan:\n'})}),"\n",(0,i.jsx)(e.h3,{id:"subsection-22-implementing-llm-task-planner",children:"Subsection 2.2: Implementing LLM Task Planner"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Example: GPT-4 Task Planner"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nLLM-based task planner for humanoid robots\n\nPurpose: Decompose natural language tasks into executable action sequences\nEnvironment: Python 3.10+, OpenAI API\n"""\n\nimport openai\nimport json\nfrom typing import List, Dict, Any\n\n\nclass LLMTaskPlanner:\n    """Task decomposition using GPT-4"""\n\n    def __init__(self, api_key: str, model: str = "gpt-4"):\n        self.model = model\n        openai.api_key = api_key\n\n        # Define available robot actions\n        self.actions = [\n            "navigate_to(location)",\n            "detect_objects(object_type)",\n            "grasp(object_id)",\n            "place(location)",\n            "open(container)",\n            "close(container)",\n            "rotate(angle_degrees)",\n            "wait(duration_seconds)"\n        ]\n\n        # Few-shot examples\n        self.examples = [\n            {\n                "task": "Bring me a water bottle",\n                "plan": [\n                    {"action": "navigate_to", "params": {"location": "kitchen"}},\n                    {"action": "detect_objects", "params": {"object_type": "bottle"}},\n                    {"action": "grasp", "params": {"object_id": "bottle_0"}},\n                    {"action": "navigate_to", "params": {"location": "user"}},\n                    {"action": "place", "params": {"location": "user_hand"}}\n                ]\n            },\n            {\n                "task": "Clean the table",\n                "plan": [\n                    {"action": "navigate_to", "params": {"location": "dining_table"}},\n                    {"action": "detect_objects", "params": {"object_type": "clutter"}},\n                    {"action": "grasp", "params": {"object_id": "clutter_0"}},\n                    {"action": "navigate_to", "params": {"location": "trash_bin"}},\n                    {"action": "place", "params": {"location": "trash_bin"}}\n                ]\n            }\n        ]\n\n    def plan(self, task: str) -> List[Dict[str, Any]]:\n        """\n        Decompose task into action sequence\n\n        Args:\n            task: Natural language task description\n\n        Returns:\n            List of action dictionaries\n        """\n        # Build prompt\n        prompt = self._build_prompt(task)\n\n        # Call GPT-4\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": "You are a task planning AI for humanoid robots. Output valid JSON only."},\n                {"role": "user", "content": prompt}\n            ],\n            temperature=0.2,  # Low temperature for consistency\n            max_tokens=1000\n        )\n\n        # Parse response\n        try:\n            plan_text = response.choices[0].message.content\n            # Extract JSON array (handle markdown code blocks)\n            if "```json" in plan_text:\n                plan_text = plan_text.split("```json")[1].split("```")[0]\n            elif "```" in plan_text:\n                plan_text = plan_text.split("```")[1].split("```")[0]\n\n            plan = json.loads(plan_text.strip())\n            return plan\n\n        except json.JSONDecodeError as e:\n            print(f"Failed to parse LLM output: {e}")\n            print(f"Raw output: {plan_text}")\n            return []\n\n    def _build_prompt(self, task: str) -> str:\n        """Build few-shot prompt with examples"""\n        prompt = "You are a task planning AI for a humanoid robot.\\n\\n"\n        prompt += "Given a high-level task, decompose it into a sequence of primitive actions.\\n\\n"\n        prompt += "Available actions:\\n"\n        for action in self.actions:\n            prompt += f"- {action}\\n"\n        prompt += "\\nExamples:\\n\\n"\n\n        # Add few-shot examples\n        for ex in self.examples:\n            prompt += f"Task: \\"{ex[\'task\']}\\"\\n"\n            prompt += f"Plan:\\n{json.dumps(ex[\'plan\'], indent=2)}\\n\\n"\n\n        # Add user task\n        prompt += f"Now decompose this task:\\n\\n"\n        prompt += f"Task: \\"{task}\\"\\n"\n        prompt += f"Plan:\\n"\n\n        return prompt\n\n\n# Usage example\nif __name__ == \'__main__\':\n    planner = LLMTaskPlanner(api_key="sk-...")  # Your API key\n\n    tasks = [\n        "Set the dining table for 2 people",\n        "Make me a cup of coffee",\n        "Water the plants in the living room"\n    ]\n\n    for task in tasks:\n        print(f"\\n{\'=\'*60}")\n        print(f"Task: {task}")\n        print(f"{\'=\'*60}")\n\n        plan = planner.plan(task)\n\n        print("Generated Plan:")\n        for i, step in enumerate(plan):\n            print(f"  {i+1}. {step[\'action\']}({step[\'params\']})")\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Example Output"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"============================================================\nTask: Set the dining table for 2 people\n============================================================\nGenerated Plan:\n  1. navigate_to({'location': 'kitchen'})\n  2. open({'container': 'cabinet'})\n  3. detect_objects({'object_type': 'plates'})\n  4. grasp({'object_id': 'plate_0'})\n  5. close({'container': 'cabinet'})\n  6. navigate_to({'location': 'dining_table'})\n  7. place({'location': 'table_position_1'})\n  8. navigate_to({'location': 'kitchen'})\n  9. open({'container': 'drawer'})\n  10. detect_objects({'object_type': 'utensils'})\n  11. grasp({'object_id': 'fork_0'})\n  12. close({'container': 'drawer'})\n  13. navigate_to({'location': 'dining_table'})\n  14. place({'location': 'table_position_1'})\n  ...\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-3-action-graph-execution",children:"Section 3: Action Graph Execution"}),"\n",(0,i.jsx)(e.h3,{id:"subsection-31-converting-plan-to-ros-2-actions",children:"Subsection 3.1: Converting Plan to ROS 2 Actions"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Action Graph Executor"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nExecute LLM-generated action graphs in ROS 2\n\nPurpose: Map LLM actions to ROS 2 action servers and execute sequentially\nEnvironment: ROS 2 Humble\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nimport math\n\n\nclass ActionGraphExecutor(Node):\n    """Execute action sequences from LLM planner"""\n\n    def __init__(self):\n        super().__init__(\'action_graph_executor\')\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Location database\n        self.locations = {\n            \'kitchen\': {\'x\': 5.0, \'y\': 2.0, \'yaw\': 0.0},\n            \'dining_table\': {\'x\': 0.0, \'y\': 3.0, \'yaw\': 1.57},\n            \'user\': {\'x\': -2.0, \'y\': 0.0, \'yaw\': 3.14},\n        }\n\n        self.get_logger().info(\'Action graph executor ready\')\n\n    def execute_plan(self, plan: list):\n        """Execute action sequence"""\n        self.get_logger().info(f"Executing plan with {len(plan)} steps")\n\n        for i, step in enumerate(plan):\n            self.get_logger().info(f"Step {i+1}/{len(plan)}: {step[\'action\']}")\n\n            success = self.execute_action(step)\n\n            if not success:\n                self.get_logger().error(f"Step {i+1} failed - aborting plan")\n                return False\n\n        self.get_logger().info("Plan execution complete!")\n        return True\n\n    def execute_action(self, action_dict: dict) -> bool:\n        """Execute single action"""\n        action = action_dict[\'action\']\n        params = action_dict[\'params\']\n\n        if action == \'navigate_to\':\n            return self.navigate_to(params[\'location\'])\n        elif action == \'detect_objects\':\n            return self.detect_objects(params[\'object_type\'])\n        elif action == \'grasp\':\n            return self.grasp(params[\'object_id\'])\n        elif action == \'place\':\n            return self.place(params[\'location\'])\n        elif action == \'open\' or action == \'close\':\n            return self.manipulate_container(action, params[\'container\'])\n        elif action == \'wait\':\n            return self.wait(params[\'duration_seconds\'])\n        else:\n            self.get_logger().warn(f"Unknown action: {action}")\n            return False\n\n    def navigate_to(self, location: str) -> bool:\n        """Navigate to named location"""\n        if location not in self.locations:\n            self.get_logger().error(f"Unknown location: {location}")\n            return False\n\n        loc = self.locations[location]\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose.position.x = loc[\'x\']\n        goal_msg.pose.pose.position.y = loc[\'y\']\n\n        qz = math.sin(loc[\'yaw\'] / 2.0)\n        qw = math.cos(loc[\'yaw\'] / 2.0)\n        goal_msg.pose.pose.orientation.z = qz\n        goal_msg.pose.pose.orientation.w = qw\n\n        self.get_logger().info(f"Navigating to {location}")\n        self.nav_client.wait_for_server()\n\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        rclpy.spin_until_future_complete(self, send_goal_future)\n\n        goal_handle = send_goal_future.result()\n        if not goal_handle.accepted:\n            return False\n\n        result_future = goal_handle.get_result_async()\n        rclpy.spin_until_future_complete(self, result_future)\n\n        return True\n\n    def detect_objects(self, object_type: str) -> bool:\n        """Object detection (placeholder)"""\n        self.get_logger().info(f"Detecting {object_type} objects")\n        # TODO: Integrate with YOLO from Module 3\n        return True\n\n    def grasp(self, object_id: str) -> bool:\n        """Grasp object (placeholder)"""\n        self.get_logger().info(f"Grasping {object_id}")\n        # TODO: Integrate with grasp planner\n        return True\n\n    def place(self, location: str) -> bool:\n        """Place object (placeholder)"""\n        self.get_logger().info(f"Placing object at {location}")\n        return True\n\n    def manipulate_container(self, action: str, container: str) -> bool:\n        """Open/close container (placeholder)"""\n        self.get_logger().info(f"{action.capitalize()}ing {container}")\n        return True\n\n    def wait(self, duration: float) -> bool:\n        """Wait for specified duration"""\n        import time\n        self.get_logger().info(f"Waiting {duration} seconds")\n        time.sleep(duration)\n        return True\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    executor = ActionGraphExecutor()\n\n    # Example: Execute LLM-generated plan\n    plan = [\n        {"action": "navigate_to", "params": {"location": "kitchen"}},\n        {"action": "detect_objects", "params": {"object_type": "bottle"}},\n        {"action": "grasp", "params": {"object_id": "bottle_0"}},\n        {"action": "navigate_to", "params": {"location": "user"}},\n        {"action": "place", "params": {"location": "user_hand"}}\n    ]\n\n    success = executor.execute_plan(plan)\n    print(f"Plan execution: {\'SUCCESS\' if success else \'FAILED\'}")\n\n    executor.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"section-4-integrating-llm-planning-with-voice-control",children:"Section 4: Integrating LLM Planning with Voice Control"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Complete Pipeline"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nComplete cognitive humanoid system: Voice \u2192 LLM Planning \u2192 Execution\n\nUsage: ros2 run cognitive_control cognitive_humanoid_node\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom llm_task_planner import LLMTaskPlanner\nfrom action_graph_executor import ActionGraphExecutor\nimport threading\n\n\nclass CognitiveHumanoidNode(Node):\n    """Cognitive planning system for humanoid robots"""\n\n    def __init__(self):\n        super().__init__(\'cognitive_humanoid\')\n\n        # Initialize components\n        api_key = self.declare_parameter(\'openai_api_key\', \'sk-...\').value\n        self.planner = LLMTaskPlanner(api_key=api_key)\n        self.executor = ActionGraphExecutor()\n\n        self.get_logger().info(\'Cognitive humanoid system ready\')\n\n    def execute_task(self, task_description: str):\n        """Plan and execute task from natural language"""\n        self.get_logger().info(f"Task received: \'{task_description}\'")\n\n        # Step 1: Generate plan with LLM\n        self.get_logger().info("Generating plan with LLM...")\n        plan = self.planner.plan(task_description)\n\n        if not plan:\n            self.get_logger().error("Failed to generate plan")\n            return False\n\n        self.get_logger().info(f"Generated {len(plan)}-step plan")\n\n        # Step 2: Execute plan\n        self.get_logger().info("Executing plan...")\n        success = self.executor.execute_plan(plan)\n\n        if success:\n            self.get_logger().info(f"Task \'{task_description}\' completed successfully!")\n        else:\n            self.get_logger().error(f"Task \'{task_description}\' failed during execution")\n\n        return success\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    cognitive_node = CognitiveHumanoidNode()\n\n    # Example tasks\n    tasks = [\n        "Bring me a water bottle",\n        "Clean the table",\n        "Set the table for dinner"\n    ]\n\n    for task in tasks:\n        cognitive_node.execute_task(task)\n        print()\n\n    cognitive_node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"LLM task planning"}),": Using GPT-4/Claude for task decomposition"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Few-shot prompting"}),": Providing examples for consistent outputs"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action graph execution"}),": Mapping LLM outputs to ROS 2 actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Complete integration"}),": Voice \u2192 LLM planning \u2192 Execution pipeline"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Key Concepts"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Common-sense reasoning"}),": LLMs understand implicit task requirements"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Structured output"}),": JSON format for reliable parsing"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Sequential execution"}),": Action graphs as ordered sequences"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error handling"}),": Validate plans and handle execution failures"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://platform.openai.com/docs",children:"OpenAI API Documentation"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://www.promptingguide.ai/",children:"Prompt Engineering Guide"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://python.langchain.com/docs/use_cases/robotics",children:"LangChain for Robotics"})}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Research Papers:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'"Do As I Can, Not As I Say" (Google SayCan, 2022) - LLMs for robot planning'}),"\n",(0,i.jsx)(e.li,{children:'"Inner Monologue" (Google, 2023) - LLMs with environment feedback'}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Next Chapter"}),": ",(0,i.jsx)(e.a,{href:"/physical-ai-textbook/docs/module4-vla/ch3-capstone-project",children:"Chapter 4.3: Capstone Project"})]}),"\n",(0,i.jsx)(e.p,{children:"In the final chapter, you'll integrate everything from Modules 1-4 into a complete autonomous humanoid system: voice commands, LLM planning, Isaac Sim perception, and Nav2 navigation working together!"})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);