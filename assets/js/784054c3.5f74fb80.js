"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[527],{6737:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module2-simulation/ch2-sensor-simulation","title":"Chapter 2.2: Sensor Simulation in Gazebo","description":"Simulating LiDAR, depth cameras, and IMUs for humanoid perception including realistic noise models and ROS 2 integration.","source":"@site/docs/module2-simulation/ch2-sensor-simulation.md","sourceDirName":"module2-simulation","slug":"/module2-simulation/ch2-sensor-simulation","permalink":"/physical-ai-textbook/docs/module2-simulation/ch2-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/salmansiddiqui-99/physical-ai-textbook/tree/main/docs/module2-simulation/ch2-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"ch2-sensor-simulation","title":"Chapter 2.2: Sensor Simulation in Gazebo","sidebar_label":"2.2 Sensor Simulation","sidebar_position":5,"description":"Simulating LiDAR, depth cameras, and IMUs for humanoid perception including realistic noise models and ROS 2 integration.","learning_objectives":["Configure and simulate LiDAR sensors for environment scanning","Integrate depth cameras for 3D perception and point cloud generation","Implement IMU sensors with realistic noise models for balance control","Analyze sensor data visualization in RViz2 and Gazebo"]},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Essentials","permalink":"/physical-ai-textbook/docs/module2-simulation/ch1-gazebo-essentials"},"next":{"title":"2.3 Unity Visualization","permalink":"/physical-ai-textbook/docs/module2-simulation/ch3-unity-visualization"}}');var s=r(4848),a=r(8453);const l={id:"ch2-sensor-simulation",title:"Chapter 2.2: Sensor Simulation in Gazebo",sidebar_label:"2.2 Sensor Simulation",sidebar_position:5,description:"Simulating LiDAR, depth cameras, and IMUs for humanoid perception including realistic noise models and ROS 2 integration.",learning_objectives:["Configure and simulate LiDAR sensors for environment scanning","Integrate depth cameras for 3D perception and point cloud generation","Implement IMU sensors with realistic noise models for balance control","Analyze sensor data visualization in RViz2 and Gazebo"]},o="Chapter 2.2: Sensor Simulation in Gazebo",t={},c=[{value:"Introduction: The Humanoid&#39;s Senses",id:"introduction-the-humanoids-senses",level:2},{value:"1. LiDAR Sensor Simulation",id:"1-lidar-sensor-simulation",level:2},{value:"What is LiDAR?",id:"what-is-lidar",level:3},{value:"LiDAR Configuration in Gazebo",id:"lidar-configuration-in-gazebo",level:3},{value:"Example: Adding LiDAR to a Humanoid",id:"example-adding-lidar-to-a-humanoid",level:3},{value:"Understanding LiDAR Parameters",id:"understanding-lidar-parameters",level:3},{value:"Visualizing LiDAR in RViz2",id:"visualizing-lidar-in-rviz2",level:3},{value:"2. Depth Camera Simulation",id:"2-depth-camera-simulation",level:2},{value:"What is a Depth Camera?",id:"what-is-a-depth-camera",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"Understanding Camera Parameters",id:"understanding-camera-parameters",level:3},{value:"Published Topics",id:"published-topics",level:3},{value:"Visualizing Depth Data in RViz2",id:"visualizing-depth-data-in-rviz2",level:3},{value:"3. IMU Sensor Simulation",id:"3-imu-sensor-simulation",level:2},{value:"What is an IMU?",id:"what-is-an-imu",level:3},{value:"IMU Configuration",id:"imu-configuration",level:3},{value:"Understanding IMU Noise Parameters",id:"understanding-imu-noise-parameters",level:3},{value:"IMU Data Structure",id:"imu-data-structure",level:3},{value:"4. Sensor Noise Models and Realism",id:"4-sensor-noise-models-and-realism",level:2},{value:"Why Add Noise to Simulations?",id:"why-add-noise-to-simulations",level:3},{value:"Types of Noise",id:"types-of-noise",level:3},{value:"Calibrating Noise to Real Sensors",id:"calibrating-noise-to-real-sensors",level:3},{value:"Sensor Update Rates",id:"sensor-update-rates",level:3},{value:"5. Sensor Placement for Humanoids",id:"5-sensor-placement-for-humanoids",level:2},{value:"Best Practices",id:"best-practices",level:3},{value:"Example: Full Sensor Suite",id:"example-full-sensor-suite",level:3},{value:"6. Visualizing Sensor Data",id:"6-visualizing-sensor-data",level:2},{value:"RViz2 Configuration",id:"rviz2-configuration",level:3},{value:"Launch File with RViz2",id:"launch-file-with-rviz2",level:3},{value:"7. Processing Sensor Data in ROS 2",id:"7-processing-sensor-data-in-ros-2",level:2},{value:"Subscribing to Sensor Topics",id:"subscribing-to-sensor-topics",level:3},{value:"8. Sensor Fusion with robot_localization",id:"8-sensor-fusion-with-robot_localization",level:2},{value:"Why Sensor Fusion?",id:"why-sensor-fusion",level:3},{value:"Using robot_localization Package",id:"using-robot_localization-package",level:3},{value:"9. Hands-On Project: Sensor-Based Balance Controller",id:"9-hands-on-project-sensor-based-balance-controller",level:2},{value:"Project Goal",id:"project-goal",level:3},{value:"Step 1: Create the Controller Node",id:"step-1-create-the-controller-node",level:3},{value:"Step 2: Test the Controller",id:"step-2-test-the-controller",level:3},{value:"Expected Behavior",id:"expected-behavior",level:3},{value:"10. Advanced Topics",id:"10-advanced-topics",level:2},{value:"Multi-Sensor Calibration",id:"multi-sensor-calibration",level:3},{value:"Sensor Simulation Performance",id:"sensor-simulation-performance",level:3},{value:"Sensor Plugins Beyond Basics",id:"sensor-plugins-beyond-basics",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Tune LiDAR Configuration",id:"exercise-1-tune-lidar-configuration",level:3},{value:"Exercise 2: Dual Camera Setup",id:"exercise-2-dual-camera-setup",level:3},{value:"Exercise 3: IMU Filtering",id:"exercise-3-imu-filtering",level:3},{value:"Exercise 4: Obstacle Avoidance",id:"exercise-4-obstacle-avoidance",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Problem: No LiDAR Data in RViz2",id:"problem-no-lidar-data-in-rviz2",level:3},{value:"Problem: Depth Camera Shows Black Image",id:"problem-depth-camera-shows-black-image",level:3},{value:"Problem: IMU Shows Only Gravity",id:"problem-imu-shows-only-gravity",level:3},{value:"Problem: Gazebo Runs Slowly with Sensors",id:"problem-gazebo-runs-slowly-with-sensors",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-22-sensor-simulation-in-gazebo",children:"Chapter 2.2: Sensor Simulation in Gazebo"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Learning Objectives:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Configure and simulate LiDAR sensors for environment scanning"}),"\n",(0,s.jsx)(e.li,{children:"Integrate depth cameras for 3D perception"}),"\n",(0,s.jsx)(e.li,{children:"Add IMU sensors for orientation and motion sensing"}),"\n",(0,s.jsx)(e.li,{children:"Understand sensor noise models and realistic simulation"}),"\n",(0,s.jsx)(e.li,{children:"Visualize sensor data in RViz2 and Gazebo"}),"\n",(0,s.jsx)(e.li,{children:"Process sensor data in ROS 2 for humanoid control"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"introduction-the-humanoids-senses",children:"Introduction: The Humanoid's Senses"}),"\n",(0,s.jsxs)(e.p,{children:["A humanoid robot without sensors is like a person blindfolded in an unfamiliar room. Sensors are the ",(0,s.jsx)(e.strong,{children:"digital senses"})," that allow robots to perceive their environment, maintain balance, and navigate obstacles."]}),"\n",(0,s.jsx)(e.p,{children:"In this chapter, you'll learn to simulate three critical sensor types:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LiDAR"}),": Laser-based distance measurement for mapping"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Depth Cameras"}),": 3D vision for object detection and manipulation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"IMUs"}),": Inertial measurement for balance and orientation"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Why Sensor Simulation Matters:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test perception algorithms without expensive hardware"}),"\n",(0,s.jsx)(e.li,{children:"Generate training data for machine learning models"}),"\n",(0,s.jsx)(e.li,{children:"Debug sensor fusion before deploying to real robots"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate sensor placement and configuration"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"1-lidar-sensor-simulation",children:"1. LiDAR Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"what-is-lidar",children:"What is LiDAR?"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"LiDAR (Light Detection and Ranging)"})," uses laser pulses to measure distances. It's essential for:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Environment mapping (SLAM - Simultaneous Localization and Mapping)"}),"\n",(0,s.jsx)(e.li,{children:"Obstacle detection"}),"\n",(0,s.jsx)(e.li,{children:"Navigation planning"}),"\n",(0,s.jsx)(e.li,{children:"Terrain analysis"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"lidar-configuration-in-gazebo",children:"LiDAR Configuration in Gazebo"}),"\n",(0,s.jsxs)(e.p,{children:["Gazebo uses the ",(0,s.jsx)(e.strong,{children:"libgazebo_ros_ray_sensor"})," plugin for LiDAR simulation."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Key Parameters:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Update Rate"}),": How often the sensor publishes (Hz)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Range"}),": Minimum and maximum detection distance (meters)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Samples"}),": Number of laser rays per scan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Resolution"}),": Angular spacing between rays"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Noise"}),": Simulated measurement uncertainty"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"example-adding-lidar-to-a-humanoid",children:"Example: Adding LiDAR to a Humanoid"}),"\n",(0,s.jsx)(e.p,{children:"Let's add a 2D scanning LiDAR to a humanoid's chest."}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:["File: ",(0,s.jsx)(e.code,{children:"chest_lidar.urdf.xacro"})]})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro">\r\n\r\n  \x3c!-- LiDAR sensor mounted on chest --\x3e\r\n  <xacro:macro name="chest_lidar" params="parent">\r\n\r\n    \x3c!-- Sensor link --\x3e\r\n    <link name="chest_lidar_link">\r\n      <visual>\r\n        <geometry>\r\n          <cylinder radius="0.03" length="0.05"/>\r\n        </geometry>\r\n        <material name="black">\r\n          <color rgba="0 0 0 1"/>\r\n        </material>\r\n      </visual>\r\n\r\n      <collision>\r\n        <geometry>\r\n          <cylinder radius="0.03" length="0.05"/>\r\n        </geometry>\r\n      </collision>\r\n\r\n      <inertial>\r\n        <mass value="0.2"/>\r\n        <inertia ixx="0.0001" ixy="0" ixz="0"\r\n                 iyy="0.0001" iyz="0" izz="0.0001"/>\r\n      </inertial>\r\n    </link>\r\n\r\n    \x3c!-- Attach to parent (chest) --\x3e\r\n    <joint name="chest_lidar_joint" type="fixed">\r\n      <parent link="${parent}"/>\r\n      <child link="chest_lidar_link"/>\r\n      <origin xyz="0.1 0 0.2" rpy="0 0 0"/>\r\n    </joint>\r\n\r\n    \x3c!-- Gazebo LiDAR plugin --\x3e\r\n    <gazebo reference="chest_lidar_link">\r\n      <sensor type="ray" name="chest_lidar_sensor">\r\n        <pose>0 0 0 0 0 0</pose>\r\n        <visualize>true</visualize>\r\n        <update_rate>10</update_rate>\r\n\r\n        <ray>\r\n          <scan>\r\n            <horizontal>\r\n              <samples>720</samples>\r\n              <resolution>1</resolution>\r\n              <min_angle>-3.14159</min_angle>  \x3c!-- -180 degrees --\x3e\r\n              <max_angle>3.14159</max_angle>   \x3c!-- +180 degrees --\x3e\r\n            </horizontal>\r\n          </scan>\r\n\r\n          <range>\r\n            <min>0.1</min>\r\n            <max>30.0</max>\r\n            <resolution>0.01</resolution>\r\n          </range>\r\n\r\n          <noise>\r\n            <type>gaussian</type>\r\n            <mean>0.0</mean>\r\n            <stddev>0.01</stddev>\r\n          </noise>\r\n        </ray>\r\n\r\n        <plugin name="gazebo_ros_head_hokuyo_controller" filename="libgazebo_ros_ray_sensor.so">\r\n          <ros>\r\n            <namespace>/humanoid</namespace>\r\n            <remapping>~/out:=scan</remapping>\r\n          </ros>\r\n          <output_type>sensor_msgs/LaserScan</output_type>\r\n          <frame_name>chest_lidar_link</frame_name>\r\n        </plugin>\r\n      </sensor>\r\n    </gazebo>\r\n\r\n  </xacro:macro>\r\n\r\n</robot>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"understanding-lidar-parameters",children:"Understanding LiDAR Parameters"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Scan Configuration:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<samples>720</samples>           \x3c!-- 720 rays in one scan --\x3e\r\n<min_angle>-3.14159</min_angle>  \x3c!-- Start at -180\xb0 --\x3e\r\n<max_angle>3.14159</max_angle>   \x3c!-- End at +180\xb0 --\x3e\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Angular Resolution"}),": (max - min) / samples = 360\xb0 / 720 = 0.5\xb0 per ray"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Higher samples"})," = better resolution but more computational cost"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Range Configuration:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<min>0.1</min>   \x3c!-- Objects closer than 10 cm are not detected --\x3e\r\n<max>30.0</max>  \x3c!-- Maximum detection range is 30 meters --\x3e\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Noise Model:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<noise>\r\n  <type>gaussian</type>\r\n  <mean>0.0</mean>\r\n  <stddev>0.01</stddev>  \x3c!-- \xb11 cm standard deviation --\x3e\r\n</noise>\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Gaussian noise"})," simulates real-world sensor uncertainty"]}),"\n",(0,s.jsxs)(e.li,{children:["Increase ",(0,s.jsx)(e.code,{children:"stddev"})," for cheaper/noisier sensors"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"visualizing-lidar-in-rviz2",children:"Visualizing LiDAR in RViz2"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Launch file snippet:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import os\r\nfrom ament_index_python.packages import get_package_share_directory\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # RViz2 for visualization\r\n        Node(\r\n            package='rviz2',\r\n            executable='rviz2',\r\n            name='rviz2',\r\n            arguments=['-d', os.path.join(\r\n                get_package_share_directory('your_package'),\r\n                'config', 'lidar_view.rviz'\r\n            )]\r\n        ),\r\n    ])\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"In RViz2:"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:'Add "LaserScan" display'}),"\n",(0,s.jsxs)(e.li,{children:["Set topic to ",(0,s.jsx)(e.code,{children:"/humanoid/scan"})]}),"\n",(0,s.jsx)(e.li,{children:"Set size to 0.05 for better visibility"}),"\n",(0,s.jsx)(e.li,{children:"Choose color scheme (intensity or flat)"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"2-depth-camera-simulation",children:"2. Depth Camera Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"what-is-a-depth-camera",children:"What is a Depth Camera?"}),"\n",(0,s.jsxs)(e.p,{children:["A ",(0,s.jsx)(e.strong,{children:"depth camera"})," (like Intel RealSense or Microsoft Kinect) provides:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RGB image"}),": Color information"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Depth image"}),": Distance to each pixel"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Point cloud"}),": 3D coordinates of all points in view"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Use cases for humanoids:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Object recognition and manipulation"}),"\n",(0,s.jsx)(e.li,{children:"Facial recognition for HRI (Human-Robot Interaction)"}),"\n",(0,s.jsx)(e.li,{children:"Grasping pose estimation"}),"\n",(0,s.jsx)(e.li,{children:"Obstacle avoidance"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,s.jsxs)(e.p,{children:["Gazebo uses the ",(0,s.jsx)(e.strong,{children:"libgazebo_ros_camera"})," plugin with depth capabilities."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:["File: ",(0,s.jsx)(e.code,{children:"head_depth_camera.urdf.xacro"})]})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro">\r\n\r\n  \x3c!-- Depth camera in the head --\x3e\r\n  <xacro:macro name="head_depth_camera" params="parent">\r\n\r\n    \x3c!-- Camera link --\x3e\r\n    <link name="head_camera_link">\r\n      <visual>\r\n        <geometry>\r\n          <box size="0.05 0.1 0.03"/>\r\n        </geometry>\r\n        <material name="dark_grey">\r\n          <color rgba="0.3 0.3 0.3 1"/>\r\n        </material>\r\n      </visual>\r\n\r\n      <collision>\r\n        <geometry>\r\n          <box size="0.05 0.1 0.03"/>\r\n        </geometry>\r\n      </collision>\r\n\r\n      <inertial>\r\n        <mass value="0.1"/>\r\n        <inertia ixx="0.0001" ixy="0" ixz="0"\r\n                 iyy="0.0001" iyz="0" izz="0.0001"/>\r\n      </inertial>\r\n    </link>\r\n\r\n    \x3c!-- Optical frame (standard for cameras in ROS) --\x3e\r\n    <link name="head_camera_optical_frame"/>\r\n\r\n    \x3c!-- Attach camera to parent (head) --\x3e\r\n    <joint name="head_camera_joint" type="fixed">\r\n      <parent link="${parent}"/>\r\n      <child link="head_camera_link"/>\r\n      <origin xyz="0.08 0 0.05" rpy="0 0 0"/>\r\n    </joint>\r\n\r\n    \x3c!-- Optical frame transformation (ROS camera convention) --\x3e\r\n    <joint name="head_camera_optical_joint" type="fixed">\r\n      <parent link="head_camera_link"/>\r\n      <child link="head_camera_optical_frame"/>\r\n      <origin xyz="0 0 0" rpy="-1.57079632679 0 -1.57079632679"/>\r\n    </joint>\r\n\r\n    \x3c!-- Gazebo depth camera plugin --\x3e\r\n    <gazebo reference="head_camera_link">\r\n      <sensor type="depth" name="head_depth_camera">\r\n        <update_rate>30.0</update_rate>\r\n        <visualize>true</visualize>\r\n\r\n        <camera name="head_camera">\r\n          <horizontal_fov>1.047198</horizontal_fov>  \x3c!-- 60 degrees --\x3e\r\n          <image>\r\n            <width>640</width>\r\n            <height>480</height>\r\n            <format>R8G8B8</format>\r\n          </image>\r\n          <clip>\r\n            <near>0.05</near>\r\n            <far>10.0</far>\r\n          </clip>\r\n\r\n          <noise>\r\n            <type>gaussian</type>\r\n            <mean>0.0</mean>\r\n            <stddev>0.007</stddev>\r\n          </noise>\r\n        </camera>\r\n\r\n        <plugin name="head_depth_camera_controller" filename="libgazebo_ros_camera.so">\r\n          <ros>\r\n            <namespace>/humanoid</namespace>\r\n            <remapping>~/image_raw:=camera/image_raw</remapping>\r\n            <remapping>~/depth/image_raw:=camera/depth/image_raw</remapping>\r\n            <remapping>~/camera_info:=camera/camera_info</remapping>\r\n            <remapping>~/depth/camera_info:=camera/depth/camera_info</remapping>\r\n            <remapping>~/points:=camera/points</remapping>\r\n          </ros>\r\n\r\n          <camera_name>head_camera</camera_name>\r\n          <frame_name>head_camera_optical_frame</frame_name>\r\n          <hack_baseline>0.07</hack_baseline>\r\n          <min_depth>0.05</min_depth>\r\n          <max_depth>10.0</max_depth>\r\n        </plugin>\r\n      </sensor>\r\n    </gazebo>\r\n\r\n  </xacro:macro>\r\n\r\n</robot>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"understanding-camera-parameters",children:"Understanding Camera Parameters"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Field of View (FOV):"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<horizontal_fov>1.047198</horizontal_fov>  \x3c!-- 60\xb0 in radians --\x3e\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Typical values: 60\xb0-90\xb0 (1.047 - 1.571 radians)"}),"\n",(0,s.jsx)(e.li,{children:"Wider FOV = more visible area but lower resolution per degree"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Image Resolution:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<width>640</width>\r\n<height>480</height>  \x3c!-- VGA resolution --\x3e\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Higher resolution = better detail but more processing"}),"\n",(0,s.jsx)(e.li,{children:"Common: 640\xd7480 (VGA), 1280\xd7720 (HD), 1920\xd71080 (Full HD)"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Depth Range:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<near>0.05</near>  \x3c!-- 5 cm minimum --\x3e\r\n<far>10.0</far>    \x3c!-- 10 m maximum --\x3e\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Optical Frame Convention:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- ROS camera convention: X=right, Y=down, Z=forward --\x3e\r\n<origin xyz="0 0 0" rpy="-1.57079632679 0 -1.57079632679"/>\n'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"This rotation aligns camera frame with ROS standards"}),"\n",(0,s.jsx)(e.li,{children:"Critical for correct point cloud generation"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"published-topics",children:"Published Topics"}),"\n",(0,s.jsx)(e.p,{children:"The depth camera publishes five topics:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:(0,s.jsx)(e.code,{children:"/humanoid/camera/image_raw"})}),": RGB image (sensor_msgs/Image)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:(0,s.jsx)(e.code,{children:"/humanoid/camera/depth/image_raw"})}),": Depth image (sensor_msgs/Image)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:(0,s.jsx)(e.code,{children:"/humanoid/camera/camera_info"})}),": RGB calibration (sensor_msgs/CameraInfo)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:(0,s.jsx)(e.code,{children:"/humanoid/camera/depth/camera_info"})}),": Depth calibration (sensor_msgs/CameraInfo)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:(0,s.jsx)(e.code,{children:"/humanoid/camera/points"})}),": Point cloud (sensor_msgs/PointCloud2)"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"visualizing-depth-data-in-rviz2",children:"Visualizing Depth Data in RViz2"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Add displays in RViz2:"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RGB Image"}),': Add "Image" display \u2192 topic ',(0,s.jsx)(e.code,{children:"/humanoid/camera/image_raw"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Depth Image"}),': Add "Image" display \u2192 topic ',(0,s.jsx)(e.code,{children:"/humanoid/camera/depth/image_raw"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Point Cloud"}),': Add "PointCloud2" display \u2192 topic ',(0,s.jsx)(e.code,{children:"/humanoid/camera/points"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Point Cloud styling:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Color Transformer: "RGB8" (shows real colors)'}),"\n",(0,s.jsx)(e.li,{children:"Size (m): 0.01"}),"\n",(0,s.jsx)(e.li,{children:"Style: Points or Squares"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"3-imu-sensor-simulation",children:"3. IMU Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"what-is-an-imu",children:"What is an IMU?"}),"\n",(0,s.jsxs)(e.p,{children:["An ",(0,s.jsx)(e.strong,{children:"Inertial Measurement Unit (IMU)"})," measures:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Linear acceleration"}),": Motion along X, Y, Z axes (m/s\xb2)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Angular velocity"}),": Rotation around X, Y, Z axes (rad/s)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Orientation"}),": Current pose as quaternion (optional, requires sensor fusion)"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Critical for humanoids:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Balance control (detect tipping)"}),"\n",(0,s.jsx)(e.li,{children:"Fall detection"}),"\n",(0,s.jsx)(e.li,{children:"Gait stabilization"}),"\n",(0,s.jsx)(e.li,{children:"Odometry correction"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"imu-configuration",children:"IMU Configuration"}),"\n",(0,s.jsxs)(e.p,{children:["Gazebo uses the ",(0,s.jsx)(e.strong,{children:"libgazebo_ros_imu_sensor"})," plugin."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:["File: ",(0,s.jsx)(e.code,{children:"torso_imu.urdf.xacro"})]})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro">\r\n\r\n  \x3c!-- IMU sensor in the torso (center of mass) --\x3e\r\n  <xacro:macro name="torso_imu" params="parent">\r\n\r\n    \x3c!-- IMU link (usually small/invisible) --\x3e\r\n    <link name="torso_imu_link">\r\n      <inertial>\r\n        <mass value="0.01"/>\r\n        <inertia ixx="0.00001" ixy="0" ixz="0"\r\n                 iyy="0.00001" iyz="0" izz="0.00001"/>\r\n      </inertial>\r\n    </link>\r\n\r\n    \x3c!-- Attach to parent (torso) at center of mass --\x3e\r\n    <joint name="torso_imu_joint" type="fixed">\r\n      <parent link="${parent}"/>\r\n      <child link="torso_imu_link"/>\r\n      <origin xyz="0 0 0.15" rpy="0 0 0"/>\r\n    </joint>\r\n\r\n    \x3c!-- Gazebo IMU plugin --\x3e\r\n    <gazebo reference="torso_imu_link">\r\n      <sensor name="torso_imu_sensor" type="imu">\r\n        <always_on>true</always_on>\r\n        <update_rate>100</update_rate>\r\n        <visualize>true</visualize>\r\n\r\n        <imu>\r\n          <angular_velocity>\r\n            <x>\r\n              <noise type="gaussian">\r\n                <mean>0.0</mean>\r\n                <stddev>0.009</stddev>\r\n                <bias_mean>0.00075</bias_mean>\r\n                <bias_stddev>0.0000008</bias_stddev>\r\n              </noise>\r\n            </x>\r\n            <y>\r\n              <noise type="gaussian">\r\n                <mean>0.0</mean>\r\n                <stddev>0.009</stddev>\r\n                <bias_mean>0.00075</bias_mean>\r\n                <bias_stddev>0.0000008</bias_stddev>\r\n              </noise>\r\n            </y>\r\n            <z>\r\n              <noise type="gaussian">\r\n                <mean>0.0</mean>\r\n                <stddev>0.009</stddev>\r\n                <bias_mean>0.00075</bias_mean>\r\n                <bias_stddev>0.0000008</bias_stddev>\r\n              </noise>\r\n            </z>\r\n          </angular_velocity>\r\n\r\n          <linear_acceleration>\r\n            <x>\r\n              <noise type="gaussian">\r\n                <mean>0.0</mean>\r\n                <stddev>0.017</stddev>\r\n                <bias_mean>0.1</bias_mean>\r\n                <bias_stddev>0.001</bias_stddev>\r\n              </noise>\r\n            </x>\r\n            <y>\r\n              <noise type="gaussian">\r\n                <mean>0.0</mean>\r\n                <stddev>0.017</stddev>\r\n                <bias_mean>0.1</bias_mean>\r\n                <bias_stddev>0.001</bias_stddev>\r\n              </noise>\r\n            </y>\r\n            <z>\r\n              <noise type="gaussian">\r\n                <mean>0.0</mean>\r\n                <stddev>0.017</stddev>\r\n                <bias_mean>0.1</bias_mean>\r\n                <bias_stddev>0.001</bias_stddev>\r\n              </noise>\r\n            </z>\r\n          </linear_acceleration>\r\n        </imu>\r\n\r\n        <plugin name="torso_imu_plugin" filename="libgazebo_ros_imu_sensor.so">\r\n          <ros>\r\n            <namespace>/humanoid</namespace>\r\n            <remapping>~/out:=imu</remapping>\r\n          </ros>\r\n          <frame_name>torso_imu_link</frame_name>\r\n          <initial_orientation_as_reference>false</initial_orientation_as_reference>\r\n        </plugin>\r\n      </sensor>\r\n    </gazebo>\r\n\r\n  </xacro:macro>\r\n\r\n</robot>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"understanding-imu-noise-parameters",children:"Understanding IMU Noise Parameters"}),"\n",(0,s.jsx)(e.p,{children:"IMUs have two types of errors:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"1. White Noise (stddev):"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<stddev>0.009</stddev>  \x3c!-- Random noise per measurement --\x3e\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"High-frequency random fluctuations"}),"\n",(0,s.jsx)(e.li,{children:"Reduced by low-pass filtering"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"2. Bias (drift):"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"<bias_mean>0.00075</bias_mean>      \x3c!-- Constant offset --\x3e\r\n<bias_stddev>0.0000008</bias_stddev>  \x3c!-- Bias variation over time --\x3e\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Slowly changing offset (sensor drift)"}),"\n",(0,s.jsx)(e.li,{children:"Critical for long-term integration (odometry)"}),"\n",(0,s.jsx)(e.li,{children:"Requires periodic recalibration"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Realistic values"})," (based on consumer-grade IMUs like MPU-6050):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Angular velocity noise: ~0.01 rad/s"}),"\n",(0,s.jsx)(e.li,{children:"Linear acceleration noise: ~0.02 m/s\xb2"}),"\n",(0,s.jsx)(e.li,{children:"Bias: 0.001-0.1 depending on sensor quality"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"imu-data-structure",children:"IMU Data Structure"}),"\n",(0,s.jsxs)(e.p,{children:["The IMU publishes to ",(0,s.jsx)(e.code,{children:"/humanoid/imu"})," with message type ",(0,s.jsx)(e.code,{children:"sensor_msgs/Imu"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# sensor_msgs/Imu structure\r\norientation:         # Quaternion (x, y, z, w)\r\n  x: 0.0\r\n  y: 0.0\r\n  z: 0.0\r\n  w: 1.0\r\n\r\nangular_velocity:    # rad/s\r\n  x: 0.01\r\n  y: -0.02\r\n  z: 0.005\r\n\r\nlinear_acceleration: # m/s\xb2 (includes gravity!)\r\n  x: 0.1\r\n  y: 0.05\r\n  z: 9.81  # Gravity on Z-axis when stationary\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Important:"})," Linear acceleration includes gravity! When the robot is stationary and upright, you'll see ~9.81 m/s\xb2 on the Z-axis."]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"4-sensor-noise-models-and-realism",children:"4. Sensor Noise Models and Realism"}),"\n",(0,s.jsx)(e.h3,{id:"why-add-noise-to-simulations",children:"Why Add Noise to Simulations?"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Real sensors are imperfect."})," If you train algorithms on perfect simulated data, they will fail on real robots. Noise simulation provides:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Robustness testing"}),"\n",(0,s.jsx)(e.li,{children:"Realistic training data for machine learning"}),"\n",(0,s.jsx)(e.li,{children:"Algorithm parameter tuning"}),"\n",(0,s.jsx)(e.li,{children:"Performance benchmarking"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"types-of-noise",children:"Types of Noise"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"1. Gaussian (Normal) Noise:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<noise type="gaussian">\r\n  <mean>0.0</mean>\r\n  <stddev>0.01</stddev>\r\n</noise>\n'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Most common type"}),"\n",(0,s.jsx)(e.li,{children:"Models random measurement errors"}),"\n",(0,s.jsx)(e.li,{children:"Characterized by mean (center) and standard deviation (spread)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"2. Quantization Noise:"}),"\r\nSimulates limited sensor resolution (e.g., 12-bit ADC)."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"3. Bias/Drift:"}),"\r\nSlow changes in sensor offset over time."]}),"\n",(0,s.jsx)(e.h3,{id:"calibrating-noise-to-real-sensors",children:"Calibrating Noise to Real Sensors"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Step 1:"})," Collect real sensor data while stationary"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"ros2 topic echo /real_robot/imu > imu_data.txt\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Step 2:"})," Calculate standard deviation"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\n\r\n# Load data and calculate stddev\r\ndata = np.loadtxt(\'imu_data.txt\')\r\nstddev_gyro = np.std(data[:, 0:3], axis=0)\r\nstddev_accel = np.std(data[:, 3:6], axis=0)\r\n\r\nprint(f"Gyro noise: {stddev_gyro}")\r\nprint(f"Accel noise: {stddev_accel}")\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Step 3:"})," Update Gazebo sensor configuration with measured values"]}),"\n",(0,s.jsx)(e.h3,{id:"sensor-update-rates",children:"Sensor Update Rates"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Typical rates:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"IMU"}),": 100-200 Hz (high-rate for control)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LiDAR"}),": 10-40 Hz"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Depth camera"}),": 30 Hz (matches video frame rate)"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Trade-off:"})," Higher rate = more data but more CPU usage."]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"5-sensor-placement-for-humanoids",children:"5. Sensor Placement for Humanoids"}),"\n",(0,s.jsx)(e.h3,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"IMU Placement:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Torso (center of mass)"}),": Best for balance and orientation"]}),"\n",(0,s.jsx)(e.li,{children:"Avoid limbs (too much local motion)"}),"\n",(0,s.jsx)(e.li,{children:"Mount rigidly (vibration adds noise)"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"LiDAR Placement:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Chest or waist"}),": Horizontal scanning for navigation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Head"}),": Multi-plane scanning (but head moves)"]}),"\n",(0,s.jsx)(e.li,{children:"Height: 0.3-1.0 m for obstacle detection"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Camera Placement:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Head"}),": Natural viewpoint for manipulation"]}),"\n",(0,s.jsx)(e.li,{children:"Tilt down 15-30\xb0 to see floor and nearby objects"}),"\n",(0,s.jsx)(e.li,{children:"Stereo cameras: 6-10 cm baseline for depth"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"example-full-sensor-suite",children:"Example: Full Sensor Suite"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Torso IMU for balance --\x3e\r\n<xacro:torso_imu parent="torso_link"/>\r\n\r\n\x3c!-- Chest LiDAR for navigation --\x3e\r\n<xacro:chest_lidar parent="torso_link"/>\r\n\r\n\x3c!-- Head depth camera for manipulation --\x3e\r\n<xacro:head_depth_camera parent="head_link"/>\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"6-visualizing-sensor-data",children:"6. Visualizing Sensor Data"}),"\n",(0,s.jsx)(e.h3,{id:"rviz2-configuration",children:"RViz2 Configuration"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Create a saved configuration"})," (",(0,s.jsx)(e.code,{children:"sensor_view.rviz"}),"):"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Global Options"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Fixed Frame: ",(0,s.jsx)(e.code,{children:"base_link"})," or ",(0,s.jsx)(e.code,{children:"odom"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"TF Display"})," (show coordinate frames)"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"All Enabled: true"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"LaserScan"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Topic: ",(0,s.jsx)(e.code,{children:"/humanoid/scan"})]}),"\n",(0,s.jsx)(e.li,{children:"Size: 0.05"}),"\n",(0,s.jsx)(e.li,{children:"Color: By intensity"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Image"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Topic: ",(0,s.jsx)(e.code,{children:"/humanoid/camera/image_raw"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"PointCloud2"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Topic: ",(0,s.jsx)(e.code,{children:"/humanoid/camera/points"})]}),"\n",(0,s.jsx)(e.li,{children:"Color Transformer: RGB8"}),"\n",(0,s.jsx)(e.li,{children:"Size: 0.01"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Imu"})," (via plugin or custom visualization)"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Topic: ",(0,s.jsx)(e.code,{children:"/humanoid/imu"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"launch-file-with-rviz2",children:"Launch File with RViz2"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import os\r\nfrom ament_index_python.packages import get_package_share_directory\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import IncludeLaunchDescription\r\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    pkg_share = get_package_share_directory('your_humanoid_package')\r\n\r\n    return LaunchDescription([\r\n        # Spawn robot in Gazebo (from previous chapter)\r\n        IncludeLaunchDescription(\r\n            PythonLaunchDescriptionSource([\r\n                os.path.join(pkg_share, 'launch', 'spawn_humanoid.launch.py')\r\n            ])\r\n        ),\r\n\r\n        # RViz2 with sensor visualization\r\n        Node(\r\n            package='rviz2',\r\n            executable='rviz2',\r\n            name='rviz2',\r\n            arguments=['-d', os.path.join(pkg_share, 'rviz', 'sensor_view.rviz')],\r\n            output='screen'\r\n        ),\r\n    ])\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"7-processing-sensor-data-in-ros-2",children:"7. Processing Sensor Data in ROS 2"}),"\n",(0,s.jsx)(e.h3,{id:"subscribing-to-sensor-topics",children:"Subscribing to Sensor Topics"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Example: IMU subscriber for tilt detection"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu\r\nimport math\r\n\r\nclass TiltDetector(Node):\r\n    def __init__(self):\r\n        super().__init__('tilt_detector')\r\n\r\n        self.subscription = self.create_subscription(\r\n            Imu,\r\n            '/humanoid/imu',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        self.tilt_threshold = 0.3  # radians (~17 degrees)\r\n\r\n        self.get_logger().info('Tilt detector started')\r\n\r\n    def imu_callback(self, msg):\r\n        # Extract orientation (quaternion)\r\n        x = msg.orientation.x\r\n        y = msg.orientation.y\r\n        z = msg.orientation.z\r\n        w = msg.orientation.w\r\n\r\n        # Convert to Euler angles (roll, pitch, yaw)\r\n        roll, pitch, yaw = self.quaternion_to_euler(x, y, z, w)\r\n\r\n        # Check if tilted beyond threshold\r\n        if abs(roll) > self.tilt_threshold or abs(pitch) > self.tilt_threshold:\r\n            self.get_logger().warn(\r\n                f'TILT DETECTED! Roll: {math.degrees(roll):.1f}\xb0, '\r\n                f'Pitch: {math.degrees(pitch):.1f}\xb0'\r\n            )\r\n\r\n    def quaternion_to_euler(self, x, y, z, w):\r\n        # Roll (x-axis rotation)\r\n        sinr_cosp = 2 * (w * x + y * z)\r\n        cosr_cosp = 1 - 2 * (x * x + y * y)\r\n        roll = math.atan2(sinr_cosp, cosr_cosp)\r\n\r\n        # Pitch (y-axis rotation)\r\n        sinp = 2 * (w * y - z * x)\r\n        if abs(sinp) >= 1:\r\n            pitch = math.copysign(math.pi / 2, sinp)  # use 90 degrees if out of range\r\n        else:\r\n            pitch = math.asin(sinp)\r\n\r\n        # Yaw (z-axis rotation)\r\n        siny_cosp = 2 * (w * z + x * y)\r\n        cosy_cosp = 1 - 2 * (y * y + z * z)\r\n        yaw = math.atan2(siny_cosp, cosy_cosp)\r\n\r\n        return roll, pitch, yaw\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = TiltDetector()\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Running the detector:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Terminal 1: Launch Gazebo with sensors\r\nros2 launch your_package spawn_humanoid_with_sensors.launch.py\r\n\r\n# Terminal 2: Run tilt detector\r\npython3 tilt_detector.py\r\n\r\n# Terminal 3: Push the robot in Gazebo to see warnings\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"8-sensor-fusion-with-robot_localization",children:"8. Sensor Fusion with robot_localization"}),"\n",(0,s.jsx)(e.h3,{id:"why-sensor-fusion",children:"Why Sensor Fusion?"}),"\n",(0,s.jsx)(e.p,{children:"Individual sensors have weaknesses:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"IMU: Drifts over time"}),"\n",(0,s.jsx)(e.li,{children:"Odometry: Wheel slip errors"}),"\n",(0,s.jsx)(e.li,{children:"Vision: Fails in poor lighting"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Sensor fusion"})," combines multiple sources for better accuracy."]}),"\n",(0,s.jsx)(e.h3,{id:"using-robot_localization-package",children:"Using robot_localization Package"}),"\n",(0,s.jsxs)(e.p,{children:["The ",(0,s.jsx)(e.code,{children:"robot_localization"})," package implements Extended Kalman Filters (EKF) for fusing:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"IMU data"}),"\n",(0,s.jsx)(e.li,{children:"Odometry"}),"\n",(0,s.jsx)(e.li,{children:"GPS (outdoor robots)"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Installation:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"sudo apt install ros-humble-robot-localization\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Configuration file"})," (",(0,s.jsx)(e.code,{children:"ekf.yaml"}),"):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:"ekf_filter_node:\r\n  ros__parameters:\r\n    frequency: 30.0\r\n    sensor_timeout: 0.1\r\n    two_d_mode: false\r\n\r\n    # Frame configuration\r\n    map_frame: map\r\n    odom_frame: odom\r\n    base_link_frame: base_link\r\n    world_frame: odom\r\n\r\n    # IMU input\r\n    imu0: /humanoid/imu\r\n    imu0_config: [false, false, false,  # x, y, z position (not from IMU)\r\n                  true,  true,  true,   # roll, pitch, yaw orientation\r\n                  false, false, false,  # x, y, z velocity\r\n                  true,  true,  true,   # roll, pitch, yaw velocity\r\n                  true,  true,  true]   # x, y, z acceleration\r\n    imu0_differential: false\r\n    imu0_relative: false\r\n\r\n    # Odometry input (wheel encoders)\r\n    odom0: /humanoid/odom\r\n    odom0_config: [true,  true,  false,  # x, y, z position\r\n                   false, false, true,   # roll, pitch, yaw\r\n                   true,  true,  false,  # x, y, z velocity\r\n                   false, false, true,   # roll, pitch, yaw velocity\r\n                   false, false, false]  # x, y, z acceleration\r\n    odom0_differential: false\r\n    odom0_relative: false\r\n\r\n    # Process noise covariance (tuning parameters)\r\n    process_noise_covariance: [0.05, 0.0, 0.0, ...]  # 15x15 matrix\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Launch with EKF:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"Node(\r\n    package='robot_localization',\r\n    executable='ekf_node',\r\n    name='ekf_filter_node',\r\n    parameters=[os.path.join(pkg_share, 'config', 'ekf.yaml')],\r\n    output='screen'\r\n)\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"9-hands-on-project-sensor-based-balance-controller",children:"9. Hands-On Project: Sensor-Based Balance Controller"}),"\n",(0,s.jsx)(e.h3,{id:"project-goal",children:"Project Goal"}),"\n",(0,s.jsx)(e.p,{children:"Create a controller that uses IMU data to detect when the humanoid is tilting and applies corrective torques to joints."}),"\n",(0,s.jsx)(e.h3,{id:"step-1-create-the-controller-node",children:"Step 1: Create the Controller Node"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:["File: ",(0,s.jsx)(e.code,{children:"balance_controller.py"})]})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu, JointState\r\nfrom std_msgs.msg import Float64MultiArray\r\nimport math\r\n\r\nclass BalanceController(Node):\r\n    def __init__(self):\r\n        super().__init__('balance_controller')\r\n\r\n        # Subscribers\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, '/humanoid/imu', self.imu_callback, 10\r\n        )\r\n\r\n        # Publisher for joint commands\r\n        self.joint_pub = self.create_publisher(\r\n            Float64MultiArray, '/joint_commands', 10\r\n        )\r\n\r\n        # Control parameters\r\n        self.kp_roll = 50.0   # Proportional gain for roll\r\n        self.kp_pitch = 50.0  # Proportional gain for pitch\r\n        self.kd_roll = 10.0   # Derivative gain for roll\r\n        self.kd_pitch = 10.0  # Derivative gain for pitch\r\n\r\n        # State variables\r\n        self.current_roll = 0.0\r\n        self.current_pitch = 0.0\r\n        self.previous_roll = 0.0\r\n        self.previous_pitch = 0.0\r\n\r\n        # Control loop timer (50 Hz)\r\n        self.timer = self.create_timer(0.02, self.control_loop)\r\n\r\n        self.get_logger().info('Balance controller started')\r\n\r\n    def imu_callback(self, msg):\r\n        # Convert quaternion to Euler angles\r\n        x = msg.orientation.x\r\n        y = msg.orientation.y\r\n        z = msg.orientation.z\r\n        w = msg.orientation.w\r\n\r\n        roll, pitch, yaw = self.quaternion_to_euler(x, y, z, w)\r\n\r\n        self.previous_roll = self.current_roll\r\n        self.previous_pitch = self.current_pitch\r\n        self.current_roll = roll\r\n        self.current_pitch = pitch\r\n\r\n    def control_loop(self):\r\n        # PD control for balance\r\n        # Roll correction (left-right tilt)\r\n        roll_error = 0.0 - self.current_roll  # Target is upright (0)\r\n        roll_rate = (self.current_roll - self.previous_roll) / 0.02\r\n        roll_torque = self.kp_roll * roll_error - self.kd_roll * roll_rate\r\n\r\n        # Pitch correction (forward-backward tilt)\r\n        pitch_error = 0.0 - self.current_pitch\r\n        pitch_rate = (self.current_pitch - self.previous_pitch) / 0.02\r\n        pitch_torque = self.kp_pitch * pitch_error - self.kd_pitch * pitch_rate\r\n\r\n        # Map torques to ankle joints\r\n        # Simplified: apply roll torque to ankle roll, pitch to ankle pitch\r\n        cmd = Float64MultiArray()\r\n        cmd.data = [\r\n            roll_torque,   # left_ankle_roll\r\n            pitch_torque,  # left_ankle_pitch\r\n            -roll_torque,  # right_ankle_roll (opposite)\r\n            pitch_torque   # right_ankle_pitch\r\n        ]\r\n\r\n        self.joint_pub.publish(cmd)\r\n\r\n        # Log significant corrections\r\n        if abs(roll_error) > 0.1 or abs(pitch_error) > 0.1:\r\n            self.get_logger().info(\r\n                f'Correcting - Roll error: {math.degrees(roll_error):.1f}\xb0, '\r\n                f'Pitch error: {math.degrees(pitch_error):.1f}\xb0'\r\n            )\r\n\r\n    def quaternion_to_euler(self, x, y, z, w):\r\n        # (same as tilt_detector implementation)\r\n        sinr_cosp = 2 * (w * x + y * z)\r\n        cosr_cosp = 1 - 2 * (x * x + y * y)\r\n        roll = math.atan2(sinr_cosp, cosr_cosp)\r\n\r\n        sinp = 2 * (w * y - z * x)\r\n        if abs(sinp) >= 1:\r\n            pitch = math.copysign(math.pi / 2, sinp)\r\n        else:\r\n            pitch = math.asin(sinp)\r\n\r\n        siny_cosp = 2 * (w * z + x * y)\r\n        cosy_cosp = 1 - 2 * (y * y + z * z)\r\n        yaw = math.atan2(siny_cosp, cosy_cosp)\r\n\r\n        return roll, pitch, yaw\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = BalanceController()\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"step-2-test-the-controller",children:"Step 2: Test the Controller"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Terminal 1:"})," Launch Gazebo with humanoid"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"ros2 launch your_package spawn_humanoid_with_imu.launch.py\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Terminal 2:"})," Run balance controller"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"python3 balance_controller.py\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Terminal 3:"})," Apply external force in Gazebo"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Click on the robot in Gazebo"}),"\n",(0,s.jsx)(e.li,{children:'Use "Apply Force" tool to push it sideways'}),"\n",(0,s.jsx)(e.li,{children:"Watch the controller respond to maintain balance"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"expected-behavior",children:"Expected Behavior"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Robot should resist tilting"}),"\n",(0,s.jsx)(e.li,{children:"Ankle joints apply corrective torques"}),"\n",(0,s.jsx)(e.li,{children:"Small oscillations are normal (like human balance)"}),"\n",(0,s.jsx)(e.li,{children:"Large pushes may cause falling (limited joint torque)"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"10-advanced-topics",children:"10. Advanced Topics"}),"\n",(0,s.jsx)(e.h3,{id:"multi-sensor-calibration",children:"Multi-Sensor Calibration"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Extrinsic calibration"})," determines the spatial relationship between sensors (e.g., camera relative to LiDAR)."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Tools:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"camera_calibration"})," package (ROS 2)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"lidar_camera_calibration"})," for LiDAR-camera alignment"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"sensor-simulation-performance",children:"Sensor Simulation Performance"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Optimizing Gazebo:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Reduce update rates for non-critical sensors"}),"\n",(0,s.jsx)(e.li,{children:"Lower camera resolution during development"}),"\n",(0,s.jsxs)(e.li,{children:["Disable visualization (",(0,s.jsx)(e.code,{children:"<visualize>false</visualize>"}),")"]}),"\n",(0,s.jsxs)(e.li,{children:["Use headless Gazebo (",(0,s.jsx)(e.code,{children:"gzserver"})," without GUI)"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Benchmarking:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Check real-time factor\r\ngz stats -p\r\n\r\n# Target: real_time_factor \u2248 1.0\r\n# < 1.0 means simulation is slower than real-time\n"})}),"\n",(0,s.jsx)(e.h3,{id:"sensor-plugins-beyond-basics",children:"Sensor Plugins Beyond Basics"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"GPS"}),": ",(0,s.jsx)(e.code,{children:"libgazebo_ros_gps_sensor"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Contact sensors"}),": ",(0,s.jsx)(e.code,{children:"libgazebo_ros_bumper"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Force-torque sensors"}),": ",(0,s.jsx)(e.code,{children:"libgazebo_ros_ft_sensor"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sonar/ultrasonic"}),": ",(0,s.jsx)(e.code,{children:"libgazebo_ros_ray_sensor"})," (configured differently)"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"You've learned to simulate three essential sensor types for humanoid robots:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"LiDAR:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"2D/3D laser scanning for mapping and navigation"}),"\n",(0,s.jsx)(e.li,{children:"Configuration: samples, range, update rate, noise"}),"\n",(0,s.jsx)(e.li,{children:"Visualization in RViz2 as LaserScan"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Depth Cameras:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"RGB-D sensors for 3D perception"}),"\n",(0,s.jsx)(e.li,{children:"Outputs: RGB image, depth image, point cloud"}),"\n",(0,s.jsx)(e.li,{children:"Camera parameters: FOV, resolution, depth range"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"IMU:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Acceleration and angular velocity measurement"}),"\n",(0,s.jsx)(e.li,{children:"Noise models: Gaussian noise and bias drift"}),"\n",(0,s.jsx)(e.li,{children:"Critical for balance and orientation estimation"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Best Practices:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Add realistic noise for robustness"}),"\n",(0,s.jsx)(e.li,{children:"Place sensors strategically (IMU at COM, LiDAR at chest, camera in head)"}),"\n",(0,s.jsxs)(e.li,{children:["Fuse multiple sensors with ",(0,s.jsx)(e.code,{children:"robot_localization"})]}),"\n",(0,s.jsx)(e.li,{children:"Visualize all sensor data in RViz2"}),"\n",(0,s.jsx)(e.li,{children:"Test with external disturbances (pushes, obstacles)"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(e.h3,{id:"exercise-1-tune-lidar-configuration",children:"Exercise 1: Tune LiDAR Configuration"}),"\n",(0,s.jsx)(e.p,{children:"Modify the chest LiDAR to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"180\xb0 field of view (front only)"}),"\n",(0,s.jsx)(e.li,{children:"360 samples (0.5\xb0 resolution)"}),"\n",(0,s.jsx)(e.li,{children:"15 m maximum range"}),"\n",(0,s.jsx)(e.li,{children:"0.02 m noise standard deviation"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Test:"})," Place objects at various distances and verify detection."]}),"\n",(0,s.jsx)(e.h3,{id:"exercise-2-dual-camera-setup",children:"Exercise 2: Dual Camera Setup"}),"\n",(0,s.jsx)(e.p,{children:"Add two cameras to the humanoid head:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Left camera at ",(0,s.jsx)(e.code,{children:'xyz="0.08 0.05 0.05"'})]}),"\n",(0,s.jsxs)(e.li,{children:["Right camera at ",(0,s.jsx)(e.code,{children:'xyz="0.08 -0.05 0.05"'})]}),"\n",(0,s.jsx)(e.li,{children:"Stereo baseline: 0.1 m"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Test:"})," Visualize both camera feeds and compare depth perception."]}),"\n",(0,s.jsx)(e.h3,{id:"exercise-3-imu-filtering",children:"Exercise 3: IMU Filtering"}),"\n",(0,s.jsx)(e.p,{children:"Implement a complementary filter to combine IMU acceleration (short-term accuracy) with gyroscope integration (drift-free orientation)."}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Hint:"})," ",(0,s.jsx)(e.code,{children:"orientation = 0.98 * (prev_orientation + gyro_change) + 0.02 * accel_orientation"})]}),"\n",(0,s.jsx)(e.h3,{id:"exercise-4-obstacle-avoidance",children:"Exercise 4: Obstacle Avoidance"}),"\n",(0,s.jsx)(e.p,{children:"Create a node that:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Subscribes to ",(0,s.jsx)(e.code,{children:"/humanoid/scan"})]}),"\n",(0,s.jsx)(e.li,{children:"Detects obstacles closer than 1.0 m"}),"\n",(0,s.jsxs)(e.li,{children:["Publishes stop command to ",(0,s.jsx)(e.code,{children:"/cmd_vel"})," when obstacles detected"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(e.h3,{id:"problem-no-lidar-data-in-rviz2",children:"Problem: No LiDAR Data in RViz2"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Check:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Is the topic publishing?\r\nros2 topic list | grep scan\r\n\r\n# What's the message rate?\r\nros2 topic hz /humanoid/scan\r\n\r\n# Echo to see data\r\nros2 topic echo /humanoid/scan\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Ensure ",(0,s.jsx)(e.code,{children:"<update_rate>"})," > 0 in sensor configuration"]}),"\n",(0,s.jsx)(e.li,{children:"Check Fixed Frame in RViz2 matches TF tree"}),"\n",(0,s.jsxs)(e.li,{children:["Verify sensor ",(0,s.jsx)(e.code,{children:"<visualize>true</visualize>"})]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"problem-depth-camera-shows-black-image",children:"Problem: Depth Camera Shows Black Image"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Check:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Clipping range: ",(0,s.jsx)(e.code,{children:"<near>"})," and ",(0,s.jsx)(e.code,{children:"<far>"})," appropriate"]}),"\n",(0,s.jsx)(e.li,{children:"Object in camera view (not pointing at empty space)"}),"\n",(0,s.jsx)(e.li,{children:"Lighting in Gazebo world (depth cameras need light)"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Solution:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Add lighting to world --\x3e\r\n<light type="directional" name="sun">\r\n  <pose>0 0 10 0 0 0</pose>\r\n  <diffuse>1 1 1 1</diffuse>\r\n</light>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"problem-imu-shows-only-gravity",children:"Problem: IMU Shows Only Gravity"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"This is normal!"})," When stationary, IMU linear acceleration should be ~9.81 m/s\xb2 on Z-axis (gravity)."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"To see changes:"})," Apply forces or move the robot."]}),"\n",(0,s.jsx)(e.h3,{id:"problem-gazebo-runs-slowly-with-sensors",children:"Problem: Gazebo Runs Slowly with Sensors"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Reduce camera resolution: ",(0,s.jsx)(e.code,{children:"<width>320</width> <height>240</height>"})]}),"\n",(0,s.jsxs)(e.li,{children:["Lower LiDAR samples: ",(0,s.jsx)(e.code,{children:"<samples>180</samples>"})]}),"\n",(0,s.jsxs)(e.li,{children:["Decrease update rates: ",(0,s.jsx)(e.code,{children:"<update_rate>5</update_rate>"})," (for cameras)"]}),"\n",(0,s.jsx)(e.li,{children:"Disable unnecessary visualizations"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"ROS 2 Sensor Documentation:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/",children:"sensor_msgs Reference"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://github.com/ros2/demos/tree/rolling/image_tools",children:"image_tools Package"})}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Gazebo Sensor Plugins:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://github.com/ros-simulation/gazebo_ros_pkgs/tree/ros2/gazebo_plugins",children:"Gazebo ROS 2 Sensor Plugins"})}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Sensor Fusion:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"http://docs.ros.org/en/noetic/api/robot_localization/html/index.html",children:"robot_localization Wiki"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://www.kalmanfilter.net/",children:"Kalman Filter Tutorial"})}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Papers:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'"Robot Sensor Calibration" (Tutorials on extrinsic calibration)'}),"\n",(0,s.jsx)(e.li,{children:'"IMU Preintegration for Visual-Inertial Odometry" (Advanced IMU usage)'}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Next Chapter"}),": ",(0,s.jsx)(e.a,{href:"/physical-ai-textbook/docs/module2-simulation/ch3-unity-visualization",children:"2.3 Unity Visualization"})," - Create stunning 3D visualizations and human-robot interaction simulations with Unity and ROS 2."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Previous Chapter"}),": ",(0,s.jsx)(e.a,{href:"/physical-ai-textbook/docs/module2-simulation/ch1-gazebo-essentials",children:"2.1 Gazebo Essentials"})]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Module Navigation"}),": ",(0,s.jsx)(e.a,{href:"/physical-ai-textbook/docs/intro",children:"Course Home"})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Estimated Completion Time"}),": 2 hours\r\n",(0,s.jsx)(e.strong,{children:"Prerequisites"}),": Module 1 (ROS 2 Basics, URDF), Chapter 2.1 (Gazebo Essentials)\r\n",(0,s.jsx)(e.strong,{children:"Difficulty"}),": Intermediate"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>l,x:()=>o});var i=r(6540);const s={},a=i.createContext(s);function l(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:l(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);